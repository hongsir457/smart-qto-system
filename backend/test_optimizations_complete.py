#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´ä¼˜åŒ–éªŒè¯è„šæœ¬
æµ‹è¯•æ‰€æœ‰9é¡¹ä¼˜åŒ–çš„å®æ–½æ•ˆæœ
"""

import sys
import os
import time
import json
import logging
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_optimization_1_unified_ocr_cache():
    """æµ‹è¯•ä¼˜åŒ–1: ç»Ÿä¸€OCRç¼“å­˜ç­–ç•¥"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–1: ç»Ÿä¸€OCRç¼“å­˜ç­–ç•¥")
    
    try:
        from app.utils.analysis_optimizations import OCRCacheManager, AnalysisLogger
        
        # åˆ›å»ºOCRç¼“å­˜ç®¡ç†å™¨
        cache_manager = OCRCacheManager()
        
        # æµ‹è¯•ç¼“å­˜è®¾ç½®å’Œè·å–
        test_slice_key = "test_slice_0_0"
        test_ocr_data = [{"text": "KZ1", "confidence": 0.95}]
        
        # è®¾ç½®ç¼“å­˜
        cache_manager.set_ocr_result(test_slice_key, test_ocr_data, "global_cache")
        
        # è·å–ç¼“å­˜
        cached_result = cache_manager.get_ocr_result(test_slice_key)
        
        if cached_result == test_ocr_data:
            logger.info("âœ… ä¼˜åŒ–1æµ‹è¯•é€šè¿‡: OCRç¼“å­˜è®¾ç½®å’Œè·å–æ­£å¸¸")
            
            # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
            stats = cache_manager.get_cache_stats()
            AnalysisLogger.log_cache_stats(stats)
            
            return True
        else:
            logger.error("âŒ ä¼˜åŒ–1æµ‹è¯•å¤±è´¥: OCRç¼“å­˜æ•°æ®ä¸åŒ¹é…")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–1æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_2_analyzer_instance_reuse():
    """æµ‹è¯•ä¼˜åŒ–2: åˆ†æå™¨å®ä¾‹å¤ç”¨"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–2: åˆ†æå™¨å®ä¾‹å¤ç”¨")
    
    try:
        from app.utils.analysis_optimizations import AnalyzerInstanceManager
        
        # åˆ›å»ºå®ä¾‹ç®¡ç†å™¨
        manager = AnalyzerInstanceManager()
        
        # æ¨¡æ‹Ÿåˆ†æå™¨ç±»
        class MockAnalyzer:
            def __init__(self):
                self.created_at = time.time()
            
            def reset_batch_state(self):
                self.reset_count = getattr(self, 'reset_count', 0) + 1
        
        # è·å–ç¬¬ä¸€ä¸ªå®ä¾‹
        analyzer1 = manager.get_analyzer(MockAnalyzer)
        
        # è·å–ç¬¬äºŒä¸ªå®ä¾‹ï¼ˆåº”è¯¥æ˜¯åŒä¸€ä¸ªï¼‰
        analyzer2 = manager.get_analyzer(MockAnalyzer)
        
        if analyzer1 is analyzer2:
            logger.info("âœ… ä¼˜åŒ–2æµ‹è¯•é€šè¿‡: åˆ†æå™¨å®ä¾‹æˆåŠŸå¤ç”¨")
            
            # æµ‹è¯•æ‰¹æ¬¡é‡ç½®
            manager.reset_for_new_batch()
            
            # è·å–å®ä¾‹ç»Ÿè®¡
            stats = manager.get_instance_stats()
            logger.info(f"ğŸ“Š å®ä¾‹ç»Ÿè®¡: {stats}")
            
            return True
        else:
            logger.error("âŒ ä¼˜åŒ–2æµ‹è¯•å¤±è´¥: åˆ†æå™¨å®ä¾‹æœªå¤ç”¨")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–2æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_3_coordinate_transform_service():
    """æµ‹è¯•ä¼˜åŒ–3: ç»Ÿä¸€åæ ‡è½¬æ¢æœåŠ¡"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–3: ç»Ÿä¸€åæ ‡è½¬æ¢æœåŠ¡")
    
    try:
        from app.utils.analysis_optimizations import CoordinateTransformService, CoordinatePoint, AnalysisLogger
        
        # æ¨¡æ‹Ÿåˆ‡ç‰‡åæ ‡æ˜ å°„
        slice_map = {
            "slice_0_0": {"x_offset": 0, "y_offset": 0},
            "slice_0_1": {"x_offset": 1024, "y_offset": 0},
            "slice_1_0": {"x_offset": 0, "y_offset": 1024}
        }
        
        # æ¨¡æ‹ŸåŸå›¾ä¿¡æ¯
        original_info = {"width": 2048, "height": 2048}
        
        # åˆ›å»ºåæ ‡è½¬æ¢æœåŠ¡
        coord_service = CoordinateTransformService(slice_map, original_info)
        
        # æµ‹è¯•å•ä¸ªåæ ‡è½¬æ¢
        slice_coord = CoordinatePoint(x=100, y=200)
        global_coord = coord_service.transform_to_global(slice_coord, "slice_0_1")
        
        expected_x = 100 + 1024  # åˆ‡ç‰‡åç§»
        expected_y = 200 + 0
        
        if global_coord.global_x == expected_x and global_coord.global_y == expected_y:
            logger.info("âœ… ä¼˜åŒ–3æµ‹è¯•é€šè¿‡: åæ ‡è½¬æ¢æ­£ç¡®")
            
            # æµ‹è¯•æ‰¹é‡åæ ‡è½¬æ¢
            coords = [
                (CoordinatePoint(x=50, y=50), "slice_0_0"),
                (CoordinatePoint(x=100, y=100), "slice_1_0")
            ]
            
            batch_results = coord_service.batch_transform_coordinates(coords)
            AnalysisLogger.log_coordinate_transform(len(batch_results), len(coords))
            
            return True
        else:
            logger.error(f"âŒ ä¼˜åŒ–3æµ‹è¯•å¤±è´¥: åæ ‡è½¬æ¢é”™è¯¯ - æœŸæœ›({expected_x}, {expected_y}), å®é™…({global_coord.global_x}, {global_coord.global_y})")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–3æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_4_gpt_response_parser():
    """æµ‹è¯•ä¼˜åŒ–4: ç»Ÿä¸€GPTå“åº”è§£æå™¨"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–4: ç»Ÿä¸€GPTå“åº”è§£æå™¨")
    
    try:
        from app.utils.analysis_optimizations import GPTResponseParser
        
        # æµ‹è¯•æ­£å¸¸JSONå“åº”
        normal_response = '{"drawing_info": {"drawing_title": "æµ‹è¯•å›¾çº¸"}, "component_ids": ["KZ1", "L1"]}'
        parsed_normal = GPTResponseParser.extract_json_from_response(normal_response)
        
        # æµ‹è¯•å¸¦```jsonæ ‡è®°çš„å“åº”
        markdown_response = '''```json
        {"drawing_info": {"drawing_title": "æµ‹è¯•å›¾çº¸"}, "component_ids": ["KZ1", "L1"]}
        ```'''
        parsed_markdown = GPTResponseParser.extract_json_from_response(markdown_response)
        
        # æµ‹è¯•é”™è¯¯å“åº”ï¼ˆåº”è¯¥è¿”å›é™çº§ç»“æœï¼‰
        error_response = "è¿™ä¸æ˜¯JSONæ ¼å¼"
        parsed_error = GPTResponseParser.extract_json_from_response(error_response)
        
        # éªŒè¯ç»“æœ
        if (parsed_normal.get("drawing_info", {}).get("drawing_title") == "æµ‹è¯•å›¾çº¸" and
            parsed_markdown.get("drawing_info", {}).get("drawing_title") == "æµ‹è¯•å›¾çº¸" and
            parsed_error.get("drawing_info", {}).get("drawing_title") == "æœªè¯†åˆ«"):
            
            logger.info("âœ… ä¼˜åŒ–4æµ‹è¯•é€šè¿‡: GPTå“åº”è§£æå™¨å·¥ä½œæ­£å¸¸")
            
            # æµ‹è¯•ç»“æ„éªŒè¯
            required_fields = ["drawing_info", "component_ids"]
            is_valid = GPTResponseParser.validate_json_structure(parsed_normal, required_fields)
            logger.info(f"ğŸ“‹ ç»“æ„éªŒè¯ç»“æœ: {is_valid}")
            
            return True
        else:
            logger.error("âŒ ä¼˜åŒ–4æµ‹è¯•å¤±è´¥: GPTå“åº”è§£æç»“æœä¸æ­£ç¡®")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–4æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_5_standardized_logging():
    """æµ‹è¯•ä¼˜åŒ–5: æ ‡å‡†åŒ–æ—¥å¿—è®°å½•"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–5: æ ‡å‡†åŒ–æ—¥å¿—è®°å½•")
    
    try:
        from app.utils.analysis_optimizations import AnalysisLogger, AnalysisMetadata
        
        # æµ‹è¯•å„ç§æ—¥å¿—è®°å½•æ–¹æ³•
        AnalysisLogger.log_ocr_reuse("test_slice", 10, "global_cache")
        AnalysisLogger.log_batch_processing(1, 3, 8)
        AnalysisLogger.log_coordinate_transform(20, 24)
        
        # æµ‹è¯•åˆ†æå…ƒæ•°æ®æ—¥å¿—
        metadata = AnalysisMetadata(
            analysis_method="test_analysis",
            batch_id=1,
            slice_count=8,
            success=True,
            processing_time=1.5
        )
        AnalysisLogger.log_analysis_metadata(metadata)
        
        logger.info("âœ… ä¼˜åŒ–5æµ‹è¯•é€šè¿‡: æ ‡å‡†åŒ–æ—¥å¿—è®°å½•æ­£å¸¸")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–5æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_6_configurable_batch_size():
    """æµ‹è¯•ä¼˜åŒ–6: é…ç½®åŒ–æ‰¹æ¬¡å¤§å°"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–6: é…ç½®åŒ–æ‰¹æ¬¡å¤§å°")
    
    try:
        from app.core.config import AnalysisSettings
        
        # æ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨
        batch_size = AnalysisSettings.MAX_SLICES_PER_BATCH
        cache_ttl = AnalysisSettings.OCR_CACHE_TTL
        api_timeout = AnalysisSettings.VISION_API_TIMEOUT
        
        logger.info(f"ğŸ“‹ æ‰¹æ¬¡å¤§å°é…ç½®: {batch_size}")
        logger.info(f"ğŸ“‹ ç¼“å­˜TTLé…ç½®: {cache_ttl}")
        logger.info(f"ğŸ“‹ APIè¶…æ—¶é…ç½®: {api_timeout}")
        
        # æ£€æŸ¥OCRç¼“å­˜ä¼˜å…ˆçº§é…ç½®
        priority = AnalysisSettings.OCR_CACHE_PRIORITY
        logger.info(f"ğŸ“‹ OCRç¼“å­˜ä¼˜å…ˆçº§: {priority}")
        
        if batch_size > 0 and cache_ttl > 0 and api_timeout > 0:
            logger.info("âœ… ä¼˜åŒ–6æµ‹è¯•é€šè¿‡: é…ç½®åŒ–å‚æ•°æ­£å¸¸")
            return True
        else:
            logger.error("âŒ ä¼˜åŒ–6æµ‹è¯•å¤±è´¥: é…ç½®å‚æ•°å¼‚å¸¸")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–6æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_7_dataclass_management():
    """æµ‹è¯•ä¼˜åŒ–7: æ•°æ®ç±»ç»Ÿä¸€ç®¡ç†"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–7: æ•°æ®ç±»ç»Ÿä¸€ç®¡ç†")
    
    try:
        from app.utils.analysis_optimizations import AnalysisMetadata, CoordinatePoint
        from dataclasses import asdict
        
        # æµ‹è¯•AnalysisMetadata
        metadata = AnalysisMetadata(
            analysis_method="test_method",
            batch_id=1,
            slice_count=8,
            success=True,
            ocr_cache_used=True,
            processing_time=2.5
        )
        
        # è½¬æ¢ä¸ºå­—å…¸
        metadata_dict = asdict(metadata)
        
        # æµ‹è¯•CoordinatePoint
        coord = CoordinatePoint(x=100, y=200, slice_id="test_slice")
        coord_dict = asdict(coord)
        
        if (metadata_dict["analysis_method"] == "test_method" and
            coord_dict["x"] == 100 and coord_dict["y"] == 200):
            
            logger.info("âœ… ä¼˜åŒ–7æµ‹è¯•é€šè¿‡: æ•°æ®ç±»ç®¡ç†æ­£å¸¸")
            logger.info(f"ğŸ“‹ å…ƒæ•°æ®å­—å…¸: {metadata_dict}")
            logger.info(f"ğŸ“‹ åæ ‡å­—å…¸: {coord_dict}")
            
            return True
        else:
            logger.error("âŒ ä¼˜åŒ–7æµ‹è¯•å¤±è´¥: æ•°æ®ç±»è½¬æ¢å¼‚å¸¸")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–7æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_8_enhanced_grid_slice_analyzer():
    """æµ‹è¯•ä¼˜åŒ–8: å¢å¼ºç½‘æ ¼åˆ‡ç‰‡åˆ†æå™¨ä¼˜åŒ–"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–8: å¢å¼ºç½‘æ ¼åˆ‡ç‰‡åˆ†æå™¨ä¼˜åŒ–")
    
    try:
        from app.services.enhanced_grid_slice_analyzer import EnhancedGridSliceAnalyzer
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = EnhancedGridSliceAnalyzer()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–çš„OCRç¼“å­˜ç®¡ç†å™¨
        if hasattr(analyzer, 'ocr_cache'):
            logger.info("âœ… åˆ†æå™¨å·²é›†æˆOCRç¼“å­˜ç®¡ç†å™¨")
        else:
            logger.warning("âš ï¸ åˆ†æå™¨æœªé›†æˆOCRç¼“å­˜ç®¡ç†å™¨")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹æ¬¡çŠ¶æ€é‡ç½®æ–¹æ³•
        if hasattr(analyzer, 'reset_batch_state'):
            analyzer.reset_batch_state()
            logger.info("âœ… åˆ†æå™¨æ”¯æŒæ‰¹æ¬¡çŠ¶æ€é‡ç½®")
        else:
            logger.warning("âš ï¸ åˆ†æå™¨ä¸æ”¯æŒæ‰¹æ¬¡çŠ¶æ€é‡ç½®")
        
        # æ£€æŸ¥åæ ‡è½¬æ¢æœåŠ¡åˆå§‹åŒ–
        if hasattr(analyzer, '_initialize_coordinate_service'):
            logger.info("âœ… åˆ†æå™¨æ”¯æŒåæ ‡è½¬æ¢æœåŠ¡åˆå§‹åŒ–")
        else:
            logger.warning("âš ï¸ åˆ†æå™¨ä¸æ”¯æŒåæ ‡è½¬æ¢æœåŠ¡åˆå§‹åŒ–")
        
        logger.info("âœ… ä¼˜åŒ–8æµ‹è¯•é€šè¿‡: å¢å¼ºç½‘æ ¼åˆ‡ç‰‡åˆ†æå™¨ä¼˜åŒ–æ­£å¸¸")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–8æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def test_optimization_9_vision_scanner_optimization():
    """æµ‹è¯•ä¼˜åŒ–9: Visionæ‰«æå™¨ä¼˜åŒ–"""
    logger.info("ğŸ”§ æµ‹è¯•ä¼˜åŒ–9: Visionæ‰«æå™¨ä¼˜åŒ–")
    
    try:
        from app.services.vision_scanner import VisionScannerService
        
        # åˆ›å»ºVisionæ‰«æå™¨å®ä¾‹
        scanner = VisionScannerService()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†æ–¹æ³•
        if hasattr(scanner, '_process_slices_in_batches'):
            logger.info("âœ… Visionæ‰«æå™¨æœ‰æ‰¹æ¬¡å¤„ç†æ–¹æ³•")
        else:
            logger.warning("âš ï¸ Visionæ‰«æå™¨ç¼ºå°‘æ‰¹æ¬¡å¤„ç†æ–¹æ³•")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†é…ç½®åŒ–çš„æ‰¹æ¬¡å¤§å°
        from app.core.config import AnalysisSettings
        batch_size = AnalysisSettings.MAX_SLICES_PER_BATCH
        logger.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®åŒ–æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        logger.info("âœ… ä¼˜åŒ–9æµ‹è¯•é€šè¿‡: Visionæ‰«æå™¨ä¼˜åŒ–æ­£å¸¸")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–9æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def run_all_optimization_tests():
    """è¿è¡Œæ‰€æœ‰ä¼˜åŒ–æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰ä¼˜åŒ–æµ‹è¯•")
    
    test_results = {}
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ç»Ÿä¸€OCRç¼“å­˜ç­–ç•¥", test_optimization_1_unified_ocr_cache),
        ("åˆ†æå™¨å®ä¾‹å¤ç”¨", test_optimization_2_analyzer_instance_reuse),
        ("ç»Ÿä¸€åæ ‡è½¬æ¢æœåŠ¡", test_optimization_3_coordinate_transform_service),
        ("ç»Ÿä¸€GPTå“åº”è§£æå™¨", test_optimization_4_gpt_response_parser),
        ("æ ‡å‡†åŒ–æ—¥å¿—è®°å½•", test_optimization_5_standardized_logging),
        ("é…ç½®åŒ–æ‰¹æ¬¡å¤§å°", test_optimization_6_configurable_batch_size),
        ("æ•°æ®ç±»ç»Ÿä¸€ç®¡ç†", test_optimization_7_dataclass_management),
        ("å¢å¼ºç½‘æ ¼åˆ‡ç‰‡åˆ†æå™¨", test_optimization_8_enhanced_grid_slice_analyzer),
        ("Visionæ‰«æå™¨ä¼˜åŒ–", test_optimization_9_vision_scanner_optimization)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
        logger.info(f"{'='*60}")
        
        try:
            result = test_func()
            test_results[test_name] = result
            if result:
                passed_tests += 1
                logger.info(f"âœ… æµ‹è¯•é€šè¿‡: {test_name}")
            else:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {test_name}")
        except Exception as e:
            test_results[test_name] = False
            logger.error(f"âŒ æµ‹è¯•å¼‚å¸¸: {test_name} - {e}")
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“Š ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
    logger.info(f"{'='*60}")
    logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    logger.info(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    logger.info(f"æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
    
    logger.info("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"  {status} {test_name}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    report = {
        "timestamp": time.time(),
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "success_rate": passed_tests/total_tests*100,
        "detailed_results": test_results
    }
    
    with open("optimization_test_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: optimization_test_report.json")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = run_all_optimization_tests()
    sys.exit(0 if success else 1) 