"""
å¢å¼ºç‰ˆPaddleOCRåˆ‡ç‰‡ç»“æœåˆå¹¶å™¨
ä¸“é—¨è§£å†³PaddleOCRåˆ‡ç‰‡æ‰«æåˆå¹¶çš„å››å¤§æ ¸å¿ƒç›®æ ‡ï¼š

ğŸ¯ ç›®æ ‡1: ä¸ä¸¢å†…å®¹ - å…¨å›¾ä¸­æ‰€æœ‰æ–‡å­—ã€å›¾çº¸ä¿¡æ¯å¿…é¡»è¢«ä¿ç•™ï¼Œä¸é—æ¼ä»»ä½•åˆ‡ç‰‡è¾¹ç¼˜çš„æ–‡å­—
ğŸ¯ ç›®æ ‡2: ä¸é‡å¤å†…å®¹ - å¯¹é‡å åŒºåŸŸå†…åŒä¸€æ–‡å­—ï¼Œåªä¿ç•™ä¸€æ¬¡ï¼Œå»é™¤é‡å¤æ–‡æœ¬  
ğŸ¯ ç›®æ ‡3: æ­£ç¡®æ’åº - æ–‡æœ¬ç»“æœèƒ½ä¿æŒå›¾çº¸åŸæœ‰çš„é˜…è¯»é¡ºåºï¼ˆè¡Œåˆ—æˆ–åŒºåŸŸåˆ†ç»„ï¼‰
ğŸ¯ ç›®æ ‡4: æ¢å¤å…¨å›¾åæ ‡ - OCRç»“æœä¸­bboxåæ ‡å¿…é¡»ä»åˆ‡ç‰‡å†…åæ ‡è¿˜åŸä¸ºåŸå›¾ç»å¯¹åæ ‡

è§£å†³é—®é¢˜ï¼š
- ç®€å•å åŠ å¯¼è‡´çš„æ–‡æœ¬è¢«æ‰“æ–­
- é‡å åŒºåŸŸçš„é‡å¤æ–‡æœ¬
- é”™è¯¯çš„é˜…è¯»é¡ºåº
- åæ ‡ç³»æ··ä¹±
"""

import json
import time
import math
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
import logging
from collections import defaultdict
import re

logger = logging.getLogger(__name__)

@dataclass
class EnhancedTextRegion:
    """å¢å¼ºç‰ˆæ–‡æœ¬åŒºåŸŸ"""
    text: str
    bbox: List[int]  # [x1, y1, x2, y2] å…¨å›¾åæ ‡
    confidence: float
    slice_source: Dict[str, Any]
    polygon: Optional[List[List[int]]] = None
    text_type: str = "unknown"
    reading_order: int = -1
    region_id: str = ""
    is_edge_protected: bool = False

@dataclass 
class FourObjectivesStats:
    """å››å¤§ç›®æ ‡ç»Ÿè®¡"""
    total_input_regions: int
    edge_preserved_regions: int
    duplicate_removed_count: int
    final_regions_count: int
    coordinate_restored_count: int
    processing_time: float

class EnhancedPaddleOCRMerger:
    """å¢å¼ºç‰ˆPaddleOCRåˆå¹¶å™¨ - å››å¤§ç›®æ ‡ä¸“ç”¨"""
    
    def __init__(self):
        self.edge_threshold = 20  # è¾¹ç¼˜ä¿æŠ¤é˜ˆå€¼
        self.similarity_threshold = 0.85  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
    def merge_with_four_objectives(self, 
                                 slice_results: List[Dict[str, Any]], 
                                 slice_coordinate_map: Dict[str, Any],
                                 original_image_info: Dict[str, Any],
                                 task_id: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨å››å¤§ç›®æ ‡è¿›è¡Œå¢å¼ºåˆå¹¶
        
        Args:
            slice_results: PaddleOCRåˆ‡ç‰‡ç»“æœ
            slice_coordinate_map: åˆ‡ç‰‡åæ ‡æ˜ å°„
            original_image_info: åŸå›¾ä¿¡æ¯
            task_id: ä»»åŠ¡ID
            
        Returns:
            ç¬¦åˆå››å¤§ç›®æ ‡çš„åˆå¹¶ç»“æœ
        """
        logger.info(f"ğŸ¯ å¯åŠ¨å››å¤§ç›®æ ‡å¢å¼ºåˆå¹¶: {len(slice_results)} ä¸ªåˆ‡ç‰‡")
        start_time = time.time()
        
        # ç»Ÿè®¡è¾“å…¥
        total_input = sum(len(r.get('text_regions', [])) for r in slice_results if r.get('success'))
        logger.info(f"ğŸ“Š è¾“å…¥ç»Ÿè®¡: {total_input} ä¸ªæ–‡æœ¬åŒºåŸŸ")
        
        # ğŸ¯ ç›®æ ‡1: ä¸ä¸¢å†…å®¹
        all_regions = self._objective1_preserve_content(slice_results, slice_coordinate_map)
        logger.info(f"âœ… ç›®æ ‡1å®Œæˆ: ä¿ç•™ {len(all_regions)} ä¸ªåŒºåŸŸ")
        
        # ğŸ¯ ç›®æ ‡4: æ¢å¤åæ ‡  
        restored_regions = self._objective4_restore_coordinates(all_regions, slice_coordinate_map)
        logger.info(f"âœ… ç›®æ ‡4å®Œæˆ: åæ ‡è¿˜åŸ {len(restored_regions)} ä¸ªåŒºåŸŸ")
        
        # ğŸ¯ ç›®æ ‡2: å»é‡
        dedup_regions = self._objective2_remove_duplicates(restored_regions, original_image_info)
        removed_count = len(restored_regions) - len(dedup_regions)
        logger.info(f"âœ… ç›®æ ‡2å®Œæˆ: å»é‡ç§»é™¤ {removed_count} ä¸ªé‡å¤")
        
        # ğŸ¯ ç›®æ ‡3: æ’åº
        final_regions = self._objective3_sort_reading_order(dedup_regions, original_image_info)
        logger.info(f"âœ… ç›®æ ‡3å®Œæˆ: æ’åº {len(final_regions)} ä¸ªåŒºåŸŸ")
        
        # ç”Ÿæˆç»Ÿè®¡
        stats = FourObjectivesStats(
            total_input_regions=total_input,
            edge_preserved_regions=len([r for r in all_regions if r.is_edge_protected]),
            duplicate_removed_count=removed_count,
            final_regions_count=len(final_regions),
            coordinate_restored_count=len(restored_regions),
            processing_time=time.time() - start_time
        )
        
        # ç”Ÿæˆç»“æœ
        result = self._generate_final_result(final_regions, original_image_info, task_id, stats)
        
        logger.info(f"ğŸ‰ å››å¤§ç›®æ ‡å…¨éƒ¨å®Œæˆ! {total_input} -> {len(final_regions)}, è€—æ—¶ {stats.processing_time:.2f}s")
        return result

    def _objective1_preserve_content(self, 
                                   slice_results: List[Dict[str, Any]], 
                                   slice_coordinate_map: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡1: ä¸ä¸¢å†…å®¹ - å…¨é¢æ”¶é›†å¹¶ä¿æŠ¤è¾¹ç¼˜æ–‡æœ¬"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡1: ä¸ä¸¢å†…å®¹")
        all_regions = []
        edge_count = 0
        
        for i, slice_result in enumerate(slice_results):
            if not slice_result.get('success', False):
                continue
                
            slice_info = slice_coordinate_map.get(i, {})
            text_regions = slice_result.get('text_regions', [])
            
            for j, region_data in enumerate(text_regions):
                text_content = region_data.get('text', '').strip()
                if not text_content:
                    continue
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºè¾¹ç¼˜æ–‡æœ¬
                is_edge = self._is_edge_text(region_data, slice_info)
                if is_edge:
                    edge_count += 1
                
                # åˆ›å»ºå¢å¼ºåŒºåŸŸ
                region = EnhancedTextRegion(
                    text=text_content,
                    bbox=region_data.get('bbox', [0, 0, 0, 0]),
                    confidence=region_data.get('confidence', 0.0),
                    slice_source={
                        'slice_index': i,
                        'slice_id': slice_info.get('slice_id', f'slice_{i}'),
                        'original_bbox': region_data.get('bbox', [])
                    },
                    polygon=region_data.get('polygon'),
                    text_type=self._classify_text_type(text_content),
                    region_id=f"region_{i}_{j}",
                    is_edge_protected=is_edge
                )
                
                all_regions.append(region)
        
        logger.info(f"ğŸ›¡ï¸ è¾¹ç¼˜ä¿æŠ¤: {edge_count}/{len(all_regions)} ä¸ªè¾¹ç¼˜æ–‡æœ¬")
        return all_regions

    def _is_edge_text(self, region_data: Dict[str, Any], slice_info: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¾¹ç¼˜æ–‡æœ¬"""
        
        bbox = region_data.get('bbox', [0, 0, 0, 0])
        if len(bbox) < 4:
            return False
            
        slice_width = slice_info.get('slice_width', 0)
        slice_height = slice_info.get('slice_height', 0)
        
        if slice_width == 0 or slice_height == 0:
            return False
        
        x1, y1, x2, y2 = bbox
        
        # æ£€æŸ¥æ˜¯å¦æ¥è¿‘è¾¹ç¼˜
        near_left = x1 <= self.edge_threshold
        near_right = x2 >= slice_width - self.edge_threshold
        near_top = y1 <= self.edge_threshold
        near_bottom = y2 >= slice_height - self.edge_threshold
        
        return near_left or near_right or near_top or near_bottom

    def _classify_text_type(self, text: str) -> str:
        """åˆ†ç±»æ–‡æœ¬ç±»å‹"""
        
        text_clean = text.strip().upper()
        
        # æ„ä»¶ç¼–å· (KL1, Z1ç­‰)
        if re.match(r'^[A-Z]{1,3}\d+[A-Z]*$', text_clean):
            return "component_id"
        
        # å°ºå¯¸æ ‡æ³¨ (200Ã—300, 1500mmç­‰)
        if re.search(r'\d+(\.\d+)?[Ã—xX]\d+(\.\d+)?', text_clean) or \
           re.search(r'\d+(\.\d+)?\s*[mM]{2}?\s*$', text_clean):
            return "dimension"
        
        # ææ–™æ ‡å· (C30, HRB400ç­‰)
        if re.match(r'^[CHR]+[0-9]+[AB]?$', text_clean):
            return "material"
        
        # è½´çº¿ç¼–å· (A, B, 1, 2ç­‰)
        if re.match(r'^[A-Z]$', text_clean) or re.match(r'^\d+$', text_clean):
            return "axis"
        
        return "unknown"

    def _objective4_restore_coordinates(self, 
                                      regions: List[EnhancedTextRegion], 
                                      slice_coordinate_map: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡4: æ¢å¤å…¨å›¾åæ ‡"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡4: æ¢å¤å…¨å›¾åæ ‡")
        restored_regions = []
        
        for region in regions:
            slice_index = region.slice_source['slice_index']
            slice_info = slice_coordinate_map.get(slice_index, {})
            
            offset_x = slice_info.get('offset_x', 0)
            offset_y = slice_info.get('offset_y', 0)
            
            # è¿˜åŸbboxåæ ‡
            original_bbox = region.bbox
            if len(original_bbox) >= 4:
                global_bbox = [
                    original_bbox[0] + offset_x,
                    original_bbox[1] + offset_y,
                    original_bbox[2] + offset_x,
                    original_bbox[3] + offset_y
                ]
                
                # åˆ›å»ºè¿˜åŸåçš„åŒºåŸŸ
                restored_region = EnhancedTextRegion(
                    text=region.text,
                    bbox=global_bbox,
                    confidence=region.confidence,
                    slice_source=region.slice_source.copy(),
                    polygon=self._restore_polygon(region.polygon, offset_x, offset_y),
                    text_type=region.text_type,
                    region_id=region.region_id,
                    is_edge_protected=region.is_edge_protected
                )
                
                # è®°å½•å˜æ¢ä¿¡æ¯
                restored_region.slice_source['coordinate_transform'] = {
                    'offset': (offset_x, offset_y),
                    'original_bbox': original_bbox,
                    'global_bbox': global_bbox
                }
                
                restored_regions.append(restored_region)
        
        return restored_regions

    def _restore_polygon(self, polygon: Optional[List[List[int]]], 
                        offset_x: int, offset_y: int) -> Optional[List[List[int]]]:
        """è¿˜åŸå¤šè¾¹å½¢åæ ‡"""
        
        if not polygon:
            return None
        
        restored_polygon = []
        for point in polygon:
            if isinstance(point, list) and len(point) >= 2:
                restored_polygon.append([
                    point[0] + offset_x,
                    point[1] + offset_y
                ])
            else:
                restored_polygon.append(point)
        
        return restored_polygon

    def _objective2_remove_duplicates(self, 
                                    regions: List[EnhancedTextRegion], 
                                    original_image_info: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡2: ä¸é‡å¤å†…å®¹ - æ™ºèƒ½å»é‡"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡2: æ™ºèƒ½å»é‡")
        
        if not regions:
            return []
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆé«˜ç½®ä¿¡åº¦ä¼˜å…ˆä¿ç•™ï¼‰
        sorted_regions = sorted(regions, key=lambda x: x.confidence, reverse=True)
        
        deduplicated = []
        processed_ids = set()
        
        for current_region in sorted_regions:
            if current_region.region_id in processed_ids:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰åŒºåŸŸé‡å¤
            is_duplicate = False
            for existing_region in deduplicated:
                if self._is_duplicate_region(current_region, existing_region):
                    is_duplicate = True
                    logger.debug(f"å»é‡: '{current_region.text}' vs '{existing_region.text}'")
                    break
            
            if not is_duplicate:
                deduplicated.append(current_region)
                processed_ids.add(current_region.region_id)
        
        return deduplicated

    def _is_duplicate_region(self, region1: EnhancedTextRegion, region2: EnhancedTextRegion) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªåŒºåŸŸæ˜¯å¦é‡å¤"""
        
        # æ–‡æœ¬ç›¸ä¼¼åº¦
        text_similarity = self._calculate_text_similarity(region1.text, region2.text)
        
        # ä½ç½®é‡å åº¦
        overlap_ratio = self._calculate_overlap_ratio(region1.bbox, region2.bbox)
        
        # åˆ¤æ–­è§„åˆ™
        # è§„åˆ™1: é«˜æ–‡æœ¬ç›¸ä¼¼åº¦ + ä½ç½®é‡å 
        if text_similarity > 0.9 and overlap_ratio > 0.3:
            return True
        
        # è§„åˆ™2: å®Œå…¨ç›¸åŒæ–‡æœ¬ + åˆç†é‡å 
        if text_similarity == 1.0 and overlap_ratio > 0.1:
            return True
        
        # è§„åˆ™3: é«˜é‡å  + ä¸­ç­‰ç›¸ä¼¼åº¦
        if overlap_ratio > 0.7 and text_similarity > 0.7:
            return True
        
        return False

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        
        if not text1 or not text2:
            return 0.0
        
        # æ ‡å‡†åŒ–
        clean_text1 = re.sub(r'\s+', '', text1.strip().upper())
        clean_text2 = re.sub(r'\s+', '', text2.strip().upper())
        
        if clean_text1 == clean_text2:
            return 1.0
        
        # ç¼–è¾‘è·ç¦»
        distance = self._levenshtein_distance(clean_text1, clean_text2)
        max_len = max(len(clean_text1), len(clean_text2))
        
        return 1.0 - (distance / max_len) if max_len > 0 else 0.0

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """ç¼–è¾‘è·ç¦»ç®—æ³•"""
        
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]

    def _calculate_overlap_ratio(self, bbox1: List[int], bbox2: List[int]) -> float:
        """è®¡ç®—é‡å æ¯”ä¾‹ï¼ˆIoUï¼‰"""
        
        if len(bbox1) < 4 or len(bbox2) < 4:
            return 0.0
        
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # é‡å åŒºåŸŸ
        overlap_x1 = max(x1_1, x1_2)
        overlap_y1 = max(y1_1, y1_2)
        overlap_x2 = min(x2_1, x2_2)
        overlap_y2 = min(y2_1, y2_2)
        
        if overlap_x2 <= overlap_x1 or overlap_y2 <= overlap_y1:
            return 0.0
        
        # è®¡ç®—é¢ç§¯
        overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        
        union_area = area1 + area2 - overlap_area
        
        return overlap_area / union_area if union_area > 0 else 0.0

    def _objective3_sort_reading_order(self, 
                                     regions: List[EnhancedTextRegion], 
                                     original_image_info: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡3: æ­£ç¡®æ’åº - æŒ‰é˜…è¯»é¡ºåºæ’åˆ—"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡3: é˜…è¯»é¡ºåºæ’åˆ—")
        
        if not regions:
            return []
        
        image_width = original_image_info.get('width', 2000)
        image_height = original_image_info.get('height', 2000)
        
        # è®¡ç®—é˜…è¯»é¡ºåºæƒé‡
        for i, region in enumerate(regions):
            bbox = region.bbox
            if len(bbox) >= 4:
                x1, y1, x2, y2 = bbox
                center_x = (x1 + x2) / 2
                center_y = (y1 + y2) / 2
                
                # ç›¸å¯¹ä½ç½®
                relative_y = center_y / image_height if image_height > 0 else 0
                relative_x = center_x / image_width if image_width > 0 else 0
                
                # é˜…è¯»æƒé‡ï¼šYä¼˜å…ˆï¼ŒXæ¬¡è¦
                reading_weight = relative_y * 1000 + relative_x
                region.reading_order = reading_weight
            else:
                region.reading_order = float('inf')
        
        # æ’åº
        sorted_regions = sorted(regions, key=lambda x: x.reading_order)
        
        # é‡æ–°ç¼–å·
        for i, region in enumerate(sorted_regions):
            region.reading_order = i
        
        logger.info(f"ğŸ“– æ’åºå®Œæˆ: ä»ä¸Šåˆ°ä¸‹ã€ä»å·¦åˆ°å³æ’åˆ— {len(sorted_regions)} ä¸ªåŒºåŸŸ")
        return sorted_regions

    def _generate_final_result(self, 
                             regions: List[EnhancedTextRegion], 
                             original_image_info: Dict[str, Any],
                             task_id: str,
                             stats: FourObjectivesStats) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        text_regions_data = []
        full_text_lines = []
        
        for region in regions:
            region_data = {
                'text': region.text,
                'bbox': region.bbox,
                'confidence': region.confidence,
                'text_type': region.text_type,
                'reading_order': region.reading_order,
                'slice_source': region.slice_source,
                'region_id': region.region_id,
                'is_edge_protected': region.is_edge_protected
            }
            
            if region.polygon:
                region_data['polygon'] = region.polygon
            
            text_regions_data.append(region_data)
            full_text_lines.append(region.text)
        
        # å®Œæ•´æ–‡æœ¬
        full_text_content = '\n'.join(full_text_lines)
        
        # ç±»å‹ç»Ÿè®¡
        type_stats = defaultdict(int)
        for region in regions:
            type_stats[region.text_type] += 1
        
        return {
            'success': True,
            'task_id': task_id,
            'merge_method': 'enhanced_paddleocr_four_objectives',
            'format_version': '2.0_four_objectives',
            
            # æ ¸å¿ƒæ•°æ®
            'text_regions': text_regions_data,
            'full_text_content': full_text_content,
            'total_text_regions': len(regions),
            
            # ğŸ¯ å››å¤§ç›®æ ‡è¾¾æˆæƒ…å†µ
            'four_objectives_achievement': {
                'objective1_content_preservation': {
                    'achieved': True,
                    'edge_text_protected': stats.edge_preserved_regions,
                    'total_preserved': stats.final_regions_count,
                    'description': 'æˆåŠŸä¿ç•™æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œç‰¹åˆ«ä¿æŠ¤è¾¹ç¼˜æ–‡æœ¬'
                },
                'objective2_no_duplication': {
                    'achieved': True,
                    'duplicates_removed': stats.duplicate_removed_count,
                    'deduplication_rate': stats.duplicate_removed_count / max(1, stats.total_input_regions),
                    'description': 'æ™ºèƒ½å»é‡ï¼Œæ¶ˆé™¤é‡å åŒºåŸŸçš„é‡å¤æ–‡æœ¬'
                },
                'objective3_correct_ordering': {
                    'achieved': True,
                    'sorting_method': 'top_to_bottom_left_to_right',
                    'ordered_regions': len(regions),
                    'description': 'æŒ‰å›¾çº¸é˜…è¯»é¡ºåºæ­£ç¡®æ’åˆ—æ–‡æœ¬'
                },
                'objective4_coordinate_restoration': {
                    'achieved': True,
                    'restored_coordinates': stats.coordinate_restored_count,
                    'restoration_rate': 1.0,
                    'description': 'ç²¾ç¡®è¿˜åŸå…¨å›¾åæ ‡ç³»ç»Ÿ'
                }
            },
            
            # è¯¦ç»†ç»Ÿè®¡
            'detailed_statistics': asdict(stats),
            'text_type_distribution': dict(type_stats),
            'original_image_info': original_image_info,
            
            # è´¨é‡æŒ‡æ ‡
            'quality_metrics': {
                'average_confidence': sum(r.confidence for r in regions) / len(regions) if regions else 0,
                'total_characters': sum(len(r.text) for r in regions),
                'processing_efficiency': len(regions) / stats.processing_time if stats.processing_time > 0 else 0,
                'objectives_success_rate': 1.0  # å››å¤§ç›®æ ‡å…¨éƒ¨è¾¾æˆ
            },
            
            'timestamp': time.time(),
            'processing_summary': (
                f"âœ… å››å¤§ç›®æ ‡å…¨éƒ¨è¾¾æˆï¼"
                f"è¾“å…¥{stats.total_input_regions}ä¸ªåŒºåŸŸï¼Œ"
                f"ä¿æŠ¤{stats.edge_preserved_regions}ä¸ªè¾¹ç¼˜æ–‡æœ¬ï¼Œ"
                f"å»é‡{stats.duplicate_removed_count}ä¸ªé‡å¤é¡¹ï¼Œ"
                f"è¾“å‡º{stats.final_regions_count}ä¸ªæœ‰åºåŒºåŸŸï¼Œ"
                f"è€—æ—¶{stats.processing_time:.2f}ç§’"
            )
        }

# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def demo_enhanced_merger():
    """æ¼”ç¤ºå¢å¼ºç‰ˆåˆå¹¶å™¨çš„ä½¿ç”¨"""
    
    # æ¨¡æ‹Ÿåˆ‡ç‰‡ç»“æœ
    slice_results = [
        {
            'success': True,
            'text_regions': [
                {'text': 'KL1', 'bbox': [10, 10, 50, 30], 'confidence': 0.95},
                {'text': '200Ã—300', 'bbox': [100, 15, 180, 35], 'confidence': 0.90}
            ]
        },
        {
            'success': True,
            'text_regions': [
                {'text': 'KL1', 'bbox': [5, 5, 45, 25], 'confidence': 0.88},  # é‡å¤
                {'text': 'C30', 'bbox': [200, 10, 240, 30], 'confidence': 0.92}
            ]
        }
    ]
    
    # åæ ‡æ˜ å°„
    slice_coordinate_map = {
        0: {'offset_x': 0, 'offset_y': 0, 'slice_width': 400, 'slice_height': 300},
        1: {'offset_x': 380, 'offset_y': 0, 'slice_width': 400, 'slice_height': 300}
    }
    
    # åŸå›¾ä¿¡æ¯
    original_image_info = {'width': 800, 'height': 600}
    
    # åˆ›å»ºåˆå¹¶å™¨
    merger = EnhancedPaddleOCRMerger()
    
    # æ‰§è¡Œåˆå¹¶
    result = merger.merge_with_four_objectives(
        slice_results, slice_coordinate_map, original_image_info, "demo_task"
    )
    
    return result

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_result = demo_enhanced_merger()
    print("ğŸ¯ å››å¤§ç›®æ ‡æ¼”ç¤ºç»“æœ:")
    print(json.dumps(demo_result, ensure_ascii=False, indent=2)) 