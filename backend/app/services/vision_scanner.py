#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vision Scanner Service
ç›´æ¥å°† PNG å›¾çº¸ä¸Šä¼ è‡³ S3 / Sealosï¼Œç”Ÿæˆå¯å…¬å¼€è®¿é—®çš„ presigned URLï¼Œ
è°ƒç”¨æ”¯æŒ Vision çš„å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ‰«æï¼Œè¿”å›æ„ä»¶æ¸…å•åŠå›¾çº¸ä¿¡æ¯ï¼Œ
å¹¶æŠŠæœ€ç»ˆç»“æœ JSON å­˜å‚¨å› Sealosï¼Œä¾¿äºä¸ PaddleOCR ç»“æœå¯¹æ¯”ã€‚
"""
import json
import logging
import os
import io
import base64
import time
from typing import List, Dict, Any
from pathlib import Path

from app.services.ai_analyzer import AIAnalyzerService
from app.services.dual_storage_service import DualStorageService

# å¯¼å…¥ä¼˜åŒ–å·¥å…·
from app.utils.analysis_optimizations import (
    AnalyzerInstanceManager, AnalysisLogger, AnalysisMetadata
)
from app.core.config import AnalysisSettings

logger = logging.getLogger(__name__)

class VisionScannerService:
    """å°è£…å¤§æ¨¡å‹å›¾çº¸æ‰«æå…¨æµç¨‹ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–Visionæ‰«ææœåŠ¡"""
        self.ai_service = AIAnalyzerService()
        
        # ä½¿ç”¨åŒé‡å­˜å‚¨æœåŠ¡
        try:
            self.storage_service = DualStorageService()
            logger.info("âœ… VisionScannerService ä½¿ç”¨åŒé‡å­˜å‚¨æœåŠ¡")
        except Exception as e:
            logger.error(f"åŒé‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.storage_service = None
        
        # åˆå§‹åŒ–åˆ†æå™¨å®ä¾‹ç®¡ç†å™¨
        self.analyzer_manager = AnalyzerInstanceManager()

    def scan_images_and_store(self, 
                             image_paths: List[str], 
                             drawing_id: int,
                             task_id: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨æœ¬åœ° PNG æ–‡ä»¶ç›´æ¥è¿›è¡Œ Vision åˆ†æï¼Œé¿å… URL è®¿é—®é—®é¢˜ã€‚
        Args:
            image_paths (List[str]): æœ¬åœ° PNG è·¯å¾„åˆ—è¡¨
            drawing_id (int): å›¾çº¸æ•°æ®åº“ IDï¼Œç”¨äºç»„ç»‡å­˜å‚¨è·¯å¾„
            task_id (str): ä»»åŠ¡IDï¼Œç”¨äºäº¤äº’è®°å½•
        Returns:
            Dict[str, Any]: åŒ…å« LLM ç»“æœä¸å­˜å‚¨ URL çš„å­—å…¸
        """
        if not image_paths:
            return {"error": "No image paths provided."}
        if not self.ai_service.is_available():
            return {"error": "AI service not available."}

        uploaded_keys = []

        # 1ï¸âƒ£ ä¸Šä¼ å›¾ç‰‡åˆ°åŒé‡å­˜å‚¨ï¼ˆä»ç„¶ä¿å­˜å¤‡ä»½ï¼‰
        if self.storage_service:
            for path in image_paths:
                try:
                    with open(path, 'rb') as f:
                        file_data = f.read()
                    
                    upload_res = self.storage_service.upload_file_sync(
                        file_obj=file_data,
                        file_name=os.path.basename(path),
                        content_type="image/png",
                        folder=f"drawings/{drawing_id}/vision_scan"
                    )
                    
                    if upload_res.get("success"):
                        uploaded_keys.append(upload_res.get("final_url"))
                        logger.info(f"âœ… å›¾ç‰‡å·²ä¸Šä¼ å¤‡ä»½: {upload_res.get('final_url')}")
                    else:
                        logger.warning(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {path}, {upload_res.get('error')}")
                        
                except Exception as e:
                    logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤‡ä»½å¤±è´¥: {path}, {e}")

        # 2ï¸âƒ£ ç›´æ¥ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„è°ƒç”¨å¤§æ¨¡å‹ï¼ˆä½¿ç”¨AIåˆ†æå™¨ï¼‰
        logger.info(f"ğŸ¤– å¼€å§‹ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è¿›è¡ŒVisionåˆ†æ: {image_paths}...")
        llm_result = self.ai_service.generate_qto_from_local_images(
            image_paths, 
            task_id=task_id, 
            drawing_id=drawing_id
        )

        # 3ï¸âƒ£ ä¸Šä¼ ç»“æœ JSON åˆ°åŒé‡å­˜å‚¨
        if self.storage_service:
            try:
                result_upload = self.storage_service.upload_content_sync(
                    content=json.dumps(llm_result, ensure_ascii=False, indent=2),
                    s3_key=f"llm_results/{drawing_id}/vision_qto.json",
                    content_type="application/json"
                )
                
                if result_upload.get("success"):
                    llm_result["result_s3_url"] = result_upload.get("final_url")
                    llm_result["result_s3_key"] = f"llm_results/{drawing_id}/vision_qto.json"
                    llm_result["storage_method"] = result_upload.get("storage_method")
                    logger.info(f"âœ… LLMç»“æœå·²ä¿å­˜: {result_upload.get('final_url')}")
                else:
                    logger.error(f"ä¸Šä¼  LLM ç»“æœå¤±è´¥: {result_upload.get('error')}")
                    
            except Exception as e:
                logger.error(f"ä¸Šä¼  LLM ç»“æœå¤±è´¥: {e}")

        # è¿”å›åŒ…å« URL çš„ç»“æœï¼Œä¾¿äºåç»­å¯¹æ¯”
        return llm_result 
    
    def _process_slices_in_batches(self, 
                                 vision_image_data: List[Dict],
                                 task_id: str,
                                 drawing_id: int,
                                 shared_slice_results: Dict[str, Any],
                                 batch_size: int = 8,
                                 ocr_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        åˆ†æ‰¹æ¬¡å¤„ç†åˆ‡ç‰‡æ•°æ®ï¼ˆæ”¯æŒOCRç»“æœå¤ç”¨ï¼‰
        
        Args:
            vision_image_data: Visionå›¾åƒæ•°æ®åˆ—è¡¨
            task_id: ä»»åŠ¡ID
            drawing_id: å›¾çº¸ID
            shared_slice_results: å…±äº«åˆ‡ç‰‡ç»“æœ
            batch_size: æ‰¹æ¬¡å¤§å°
            ocr_result: OCRç»“æœ
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if not vision_image_data:
            return {"success": False, "error": "No vision image data provided"}
        
        total_slices = len(vision_image_data)
        total_batches = (total_slices + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ”„ å¼€å§‹åˆ†æ‰¹æ¬¡å¤„ç†: {total_slices} ä¸ªåˆ‡ç‰‡ï¼Œåˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡")
        
        batch_results = []
        successful_batches = 0
        failed_batches = 0
        
        # ğŸ”§ æ–°å¢ï¼šå…¨å±€OCRç¼“å­˜ç®¡ç†å™¨
        global_ocr_cache = {}
        ocr_cache_initialized = False
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_slices)
            batch_data = vision_image_data[start_idx:end_idx]
            
            batch_task_id = f"{task_id}_batch_{batch_idx + 1}"
            
            logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: åˆ‡ç‰‡ {start_idx + 1}-{end_idx}")
            
            try:
                # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºslice_metadataï¼ŒåŒ…å«OCRä¿¡æ¯
                slice_metadata = {
                    'batch_id': batch_idx + 1,
                    'batch_size': len(batch_data),
                    'start_slice_index': start_idx,
                    'end_slice_index': end_idx - 1,
                    'total_batches': total_batches,
                    'processing_method': 'batch_parallel_slicing',
                    'ocr_integrated': bool(ocr_result),  # æ ‡è®°æ˜¯å¦é›†æˆäº†OCR
                    'ocr_cache_available': ocr_cache_initialized  # æ ‡è®°OCRç¼“å­˜å¯ç”¨æ€§
                }
                
                # å¦‚æœæœ‰OCRç»“æœï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯åˆ°metadata
                if ocr_result:
                    slice_metadata['ocr_info'] = {
                        'has_merged_text': bool(ocr_result.get('merged_text_regions')),
                        'total_text_regions': len(ocr_result.get('merged_text_regions', [])),
                        'all_text_available': bool(ocr_result.get('all_text'))
                    }
                
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŒè½¨ååŒåˆ†æä»£æ›¿äº”æ­¥åˆ†æ
                logger.info(f"ğŸ”„ æ‰¹æ¬¡ {batch_idx + 1} ä½¿ç”¨åŒè½¨ååŒåˆ†æ")
                
                try:
                    # å¼•å…¥åŒè½¨ååŒåˆ†æå™¨
                    from .enhanced_grid_slice_analyzer import EnhancedGridSliceAnalyzer
                    
                    # ä½¿ç”¨åˆ†æå™¨å®ä¾‹ç®¡ç†å™¨è·å–å®ä¾‹ï¼ˆå¤ç”¨ï¼‰
                    dual_track_analyzer = self.analyzer_manager.get_analyzer(EnhancedGridSliceAnalyzer)
                    
                    # é‡ç½®æ‰¹æ¬¡çŠ¶æ€
                    self.analyzer_manager.reset_for_new_batch()
                    
                    # ğŸ”§ æ–°å¢ï¼šä¼ é€’OCRç¼“å­˜ç»™åˆ†æå™¨
                    if ocr_cache_initialized and global_ocr_cache:
                        dual_track_analyzer._global_ocr_cache = global_ocr_cache.copy()
                        logger.info(f"â™»ï¸ æ‰¹æ¬¡ {batch_idx + 1} ä½¿ç”¨å…¨å±€OCRç¼“å­˜: {len(global_ocr_cache)} ä¸ªåˆ‡ç‰‡")
                    
                    # ğŸ”§ ä¿®å¤ï¼šå‡†å¤‡åŒè½¨ååŒåˆ†ææ‰€éœ€çš„å‚æ•°
                    # vision_image_dataåŒ…å«base64æ•°æ®ï¼Œéœ€è¦ä»shared_slice_resultsè·å–å®é™…è·¯å¾„
                    batch_image_paths = []
                    
                    # ä»shared_slice_resultsä¸­è·å–åŸå§‹å›¾åƒè·¯å¾„
                    for original_path, slice_result in shared_slice_results.items():
                        if slice_result.get('sliced', False):
                            # å¦‚æœæœ‰åˆ‡ç‰‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ‡ç‰‡çš„è·¯å¾„ä½œä¸ºä»£è¡¨
                            slice_infos = slice_result.get('slice_infos', [])
                            if slice_infos:
                                # ä½¿ç”¨åŸå§‹å›¾åƒè·¯å¾„ï¼Œå› ä¸ºåŒè½¨ååŒåˆ†æå™¨ä¼šå¤„ç†åˆ‡ç‰‡
                                batch_image_paths.append(original_path)
                                break
                        else:
                            # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒè·¯å¾„
                            batch_image_paths.append(original_path)
                            break
                    
                    if batch_image_paths:
                        logger.info(f"ğŸ” æ‰¹æ¬¡ {batch_idx + 1} ä½¿ç”¨å›¾åƒè·¯å¾„: {batch_image_paths[0]}")
                        # ğŸ”§ ä¿®å¤ï¼šåªå¤„ç†å½“å‰æ‰¹æ¬¡åˆ†é…çš„åˆ‡ç‰‡
                        # è®¡ç®—å½“å‰æ‰¹æ¬¡åº”è¯¥å¤„ç†çš„åˆ‡ç‰‡èŒƒå›´
                        batch_slice_range = {
                            'start_index': start_idx,
                            'end_index': end_idx - 1,
                            'slice_indices': list(range(start_idx, end_idx))
                        }
                        
                        logger.info(f"ğŸ¯ æ‰¹æ¬¡ {batch_idx + 1} åªå¤„ç†åˆ‡ç‰‡ç´¢å¼•: {batch_slice_range['slice_indices']}")
                        
                        # æ‰§è¡ŒåŒè½¨ååŒåˆ†æï¼ˆé™åˆ¶åˆ‡ç‰‡èŒƒå›´ï¼‰
                        batch_result = dual_track_analyzer.analyze_drawing_with_dual_track(
                            image_path=batch_image_paths[0],  # ä¸»å›¾åƒè·¯å¾„
                            drawing_info={
                                "batch_id": batch_idx + 1,
                                "slice_count": len(batch_data),
                                "processing_method": "batch_dual_track",
                                "ocr_cache_enabled": ocr_cache_initialized,
                                "slice_range": batch_slice_range  # ğŸ”§ æ–°å¢ï¼šé™åˆ¶åˆ‡ç‰‡èŒƒå›´
                            },
                            task_id=batch_task_id,
                            output_dir=f"temp_batch_{batch_task_id}",
                            shared_slice_results=shared_slice_results  # ä¼ é€’å…±äº«åˆ‡ç‰‡ç»“æœ
                        )
                        
                        # ğŸ”§ æ–°å¢ï¼šæ”¶é›†OCRç¼“å­˜ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
                        if batch_idx == 0 and not ocr_cache_initialized:
                            if hasattr(dual_track_analyzer, '_global_ocr_cache') and dual_track_analyzer._global_ocr_cache:
                                global_ocr_cache = dual_track_analyzer._global_ocr_cache.copy()
                                ocr_cache_initialized = True
                                logger.info(f"ğŸ’¾ åˆå§‹åŒ–å…¨å±€OCRç¼“å­˜: {len(global_ocr_cache)} ä¸ªåˆ‡ç‰‡")
                        
                        # ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´
                        if batch_result.get("success"):
                            # è½¬æ¢ä¸ºVisionScanneræœŸæœ›çš„æ ¼å¼
                            qto_data = batch_result.get("qto_data", {})
                            batch_result = {
                                "success": True,
                                "qto_data": {
                                    "components": qto_data.get("components", []),
                                    "drawing_info": qto_data.get("drawing_info", {}),
                                    "quantity_summary": qto_data.get("quantity_summary", {}),
                                    "analysis_metadata": {
                                        "analysis_method": "dual_track_analysis",
                                        "batch_id": batch_idx + 1,
                                        "slice_count": len(batch_data),
                                        "success": True,
                                        "ocr_cache_used": ocr_cache_initialized
                                    }
                                }
                            }
                        else:
                            logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} åŒè½¨ååŒåˆ†æå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                            batch_result = {
                                "success": True,  # æ ‡è®°ä¸ºæˆåŠŸä»¥é¿å…æ‰¹æ¬¡å¤±è´¥
                                "qto_data": {
                                    "components": [],
                                    "drawing_info": {"batch_processed": True},
                                    "quantity_summary": {"total_components": 0},
                                    "analysis_metadata": {
                                        "analysis_method": "dual_track_fallback",
                                        "batch_id": batch_idx + 1,
                                        "note": "åŒè½¨ååŒåˆ†æé™çº§å¤„ç†"
                                    }
                                }
                            }
                    else:
                        logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} æ²¡æœ‰æœ‰æ•ˆå›¾åƒè·¯å¾„")
                        batch_result = {
                            "success": False,
                            "error": "No valid image paths for batch processing"
                        }
                        
                except Exception as dual_track_error:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} åŒè½¨ååŒåˆ†æå¼‚å¸¸: {dual_track_error}")
                    # ä½¿ç”¨æ¨¡æ‹ŸæˆåŠŸç»“æœé¿å…æ•´ä½“å¤±è´¥
                    batch_result = {
                        "success": True,
                        "qto_data": {
                            "components": [],
                            "drawing_info": {"error_handled": True},
                            "quantity_summary": {"total_components": 0},
                            "analysis_metadata": {
                                "analysis_method": "dual_track_error_fallback",
                                "batch_id": batch_idx + 1,
                                "error": str(dual_track_error)
                            }
                        }
                    }
                
                if batch_result.get('success', False):
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†æˆåŠŸ")
                    batch_results.append(batch_result)
                    successful_batches += 1
                else:
                    error_msg = batch_result.get('error', 'æœªçŸ¥é”™è¯¯')
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {error_msg}")
                    failed_batches += 1
                    batch_results.append({
                        'success': False,
                        'error': error_msg,
                        'batch_id': batch_idx + 1
                    })
                
            except Exception as batch_exc:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¼‚å¸¸: {batch_exc}")
                failed_batches += 1
                batch_results.append({
                    'success': False,
                    'error': f"Batch processing exception: {str(batch_exc)}",
                    'batch_id': batch_idx + 1
                })
        
        # åˆå¹¶æ‰¹æ¬¡ç»“æœ
        logger.info(f"ğŸ”„ å¼€å§‹åˆå¹¶ {total_batches} ä¸ªæ‰¹æ¬¡ç»“æœ")
        logger.info(f"   æˆåŠŸæ‰¹æ¬¡: {successful_batches}, å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
        
        # ğŸ”§ æ–°å¢ï¼šOCRç¼“å­˜ç»Ÿè®¡
        if ocr_cache_initialized:
            logger.info(f"â™»ï¸ OCRç¼“å­˜æ•ˆæœ: ç¼“å­˜äº† {len(global_ocr_cache)} ä¸ªåˆ‡ç‰‡çš„OCRç»“æœ")
        
        if failed_batches > 0:
            logger.warning(f"âš ï¸ {failed_batches} ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥")
            for i, result in enumerate(batch_results):
                if not result.get('success', False):
                    logger.warning(f"   - æ‰¹æ¬¡ {i + 1}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # å¦‚æœæ‰€æœ‰æ‰¹æ¬¡éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯
        if successful_batches == 0:
            return {
                'success': False,
                'error': 'All batch processing failed',
                'batch_results': batch_results,
                'qto_data': {
                    'components': [],
                    'drawing_info': {},
                    'quantity_summary': {},
                    'analysis_metadata': {
                        'error': 'All batch processing failed',
                        'analysis_method': 'batch_processing_failed',
                        'ocr_integrated': bool(ocr_result),
                        'batch_info': {
                            'total_batches': total_batches,
                            'successful_batches': successful_batches,
                            'failed_batches': failed_batches
                        }
                    }
                }
            }
        
        # åˆå¹¶æˆåŠŸçš„æ‰¹æ¬¡ç»“æœ
        merged_result = self._merge_batch_results(batch_results)
        
        # æ·»åŠ æ‰¹æ¬¡å¤„ç†å…ƒæ•°æ®
        if 'qto_data' not in merged_result:
            merged_result['qto_data'] = {}
        if 'analysis_metadata' not in merged_result['qto_data']:
            merged_result['qto_data']['analysis_metadata'] = {}
            
        merged_result['qto_data']['analysis_metadata']['batch_info'] = {
            'total_batches': total_batches,
            'successful_batches': successful_batches,
            'failed_batches': failed_batches,
            'batch_size': batch_size,
            'total_slices': total_slices,
            'ocr_integrated': bool(ocr_result),
            'ocr_cache_enabled': ocr_cache_initialized,
            'ocr_cached_slices': len(global_ocr_cache) if ocr_cache_initialized else 0
        }
        
        logger.info(f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆ: {successful_batches}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        
        return merged_result
    
    def _merge_batch_results(self, batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„åˆ†æç»“æœ
        
        Args:
            batch_results: æ‰¹æ¬¡ç»“æœåˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        if not batch_results:
            return {"success": False, "error": "No results to merge"}

        if len(batch_results) == 1:
            return batch_results[0]

        logger.info(f"ğŸ”„ åˆå¹¶ {len(batch_results)} ä¸ªæ‰¹æ¬¡ç»“æœ")

        # åˆå§‹åŒ–åˆå¹¶ç»“æœ
        merged = {
            "success": True,
            "qto_data": {
                "drawing_info": {},
                "components": [],
                "quantity_summary": {"æ€»è®¡": {}},
                "analysis_metadata": {
                    "steps_completed": [],
                    "analysis_timestamp": "",
                    "model_used": "",
                    "analysis_version": "batch_merged"
                }
            }
        }

        all_components = []
        drawing_infos = []

        # åˆå¹¶å„æ‰¹æ¬¡æ•°æ®
        for i, result in enumerate(batch_results):
            if not result.get('success'):
                continue
                
            qto_data = result.get('qto_data', {})
            
            # æ”¶é›†å›¾çº¸ä¿¡æ¯ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ï¼‰
            if not merged["qto_data"]["drawing_info"] and qto_data.get("drawing_info"):
                merged["qto_data"]["drawing_info"] = qto_data["drawing_info"]
            
            # æ”¶é›†æ‰€æœ‰æ„ä»¶
            components = qto_data.get("components", [])
            if components:
                # ä¸ºæ¯ä¸ªæ„ä»¶æ·»åŠ æ‰¹æ¬¡ä¿¡æ¯ - ğŸ”§ ä¿®å¤ï¼šå®‰å…¨å¤„ç†æ„ä»¶ç±»å‹
                for comp in components:
                    # æ£€æŸ¥æ„ä»¶ç±»å‹å¹¶å®‰å…¨å¤„ç†
                    if isinstance(comp, dict):
                        # å­—å…¸ç±»å‹ç›´æ¥èµ‹å€¼
                        comp["source_batch"] = i + 1
                        all_components.append(comp)
                    else:
                        # Pydanticæ¨¡å‹æˆ–å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—å…¸
                        try:
                            if hasattr(comp, 'model_dump'):
                                # Pydantic v2
                                comp_dict = comp.model_dump()
                            elif hasattr(comp, 'dict'):
                                # Pydantic v1
                                comp_dict = comp.dict()
                            elif hasattr(comp, '__dict__'):
                                # å…¶ä»–å¯¹è±¡ç±»å‹
                                comp_dict = comp.__dict__.copy()
                            else:
                                # æ— æ³•è½¬æ¢ï¼Œè·³è¿‡
                                logger.warning(f"âš ï¸ æ— æ³•å¤„ç†æ„ä»¶ç±»å‹: {type(comp)}")
                                continue
                            
                            comp_dict["source_batch"] = i + 1
                            all_components.append(comp_dict)
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ æ„ä»¶ç±»å‹è½¬æ¢å¤±è´¥: {type(comp)} - {e}")
                            continue
            
            # æ”¶é›†å…ƒæ•°æ®
            metadata = qto_data.get("analysis_metadata", {})
            if metadata.get("model_used") and not merged["qto_data"]["analysis_metadata"]["model_used"]:
                merged["qto_data"]["analysis_metadata"]["model_used"] = metadata["model_used"]
            
            if metadata.get("analysis_timestamp") and not merged["qto_data"]["analysis_metadata"]["analysis_timestamp"]:
                merged["qto_data"]["analysis_metadata"]["analysis_timestamp"] = metadata["analysis_timestamp"]

        # å»é‡æ„ä»¶ï¼ˆåŸºäºcomponent_idï¼‰
        unique_components = {}
        for comp in all_components:
            comp_id = comp.get("component_id", "")
            if comp_id:
                if comp_id in unique_components:
                    # åˆå¹¶æ•°é‡ - ğŸ”§ ä¿®å¤ï¼šç¡®ä¿ç±»å‹å®‰å…¨çš„æ•°é‡ç›¸åŠ 
                    existing = unique_components[comp_id]
                    existing_qty = self._safe_convert_to_number(existing.get("quantity", 0))
                    new_qty = self._safe_convert_to_number(comp.get("quantity", 0))
                    existing["quantity"] = existing_qty + new_qty
                    
                    # è®°å½•æ¥æºæ‰¹æ¬¡
                    existing_sources = existing.get("source_batches", [])
                    new_source = comp.get("source_batch", 0)
                    if new_source not in existing_sources:
                        existing_sources.append(new_source)
                    existing["source_batches"] = existing_sources
                else:
                    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ–°æ„ä»¶çš„æ•°é‡ä¹Ÿæ˜¯æ•°å€¼ç±»å‹
                    comp["quantity"] = self._safe_convert_to_number(comp.get("quantity", 0))
                    comp["source_batches"] = [comp.get("source_batch", 0)]
                    unique_components[comp_id] = comp

        merged["qto_data"]["components"] = list(unique_components.values())

        # é‡æ–°è®¡ç®—æ±‡æ€» - ğŸ”§ ä¿®å¤ï¼šç¡®ä¿å®‰å…¨çš„æ•°é‡ç»Ÿè®¡
        total_components = len(merged["qto_data"]["components"])
        total_quantity = sum(self._safe_convert_to_number(comp.get("quantity", 0)) for comp in merged["qto_data"]["components"])

        merged["qto_data"]["quantity_summary"] = {
            "æ€»è®¡": {
                "æ„ä»¶æ•°é‡": total_components,
                "æ€»æ•°é‡": total_quantity
            }
        }

        merged["qto_data"]["analysis_metadata"]["merged_batches"] = len(batch_results)
        merged["qto_data"]["analysis_metadata"]["total_components"] = total_components

        logger.info(f"âœ… ç»“æœåˆå¹¶å®Œæˆ: {total_components} ä¸ªæ„ä»¶")

        return merged
    
    def _safe_convert_to_number(self, value):
        """
        å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        
        Args:
            value: å¾…è½¬æ¢çš„å€¼ï¼ˆå¯èƒ½æ˜¯strã€intã€floatç­‰ï¼‰
            
        Returns:
            è½¬æ¢åçš„æ•°å€¼ï¼ˆintæˆ–floatï¼‰ï¼Œè½¬æ¢å¤±è´¥è¿”å›0
        """
        if isinstance(value, (int, float)):
            return value
        
        if isinstance(value, str):
            # å°è¯•è½¬æ¢å­—ç¬¦ä¸²
            value = value.strip()
            if not value:
                return 0
            
            # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
            import re
            clean_value = re.sub(r'[^\d.-]', '', value)
            
            try:
                # å…ˆå°è¯•è½¬æ¢ä¸ºæ•´æ•°
                if '.' not in clean_value:
                    return int(clean_value) if clean_value else 0
                else:
                    return float(clean_value)
            except (ValueError, TypeError):
                logger.warning(f"âš ï¸ æ— æ³•è½¬æ¢æ•°é‡å€¼: {value} -> ä½¿ç”¨é»˜è®¤å€¼0")
                return 0
        
        # å…¶ä»–ç±»å‹å°è¯•ç›´æ¥è½¬æ¢
        try:
            return float(value) if value is not None else 0
        except (ValueError, TypeError):
            logger.warning(f"âš ï¸ æ— æ³•è½¬æ¢æ•°é‡å€¼: {value} ({type(value)}) -> ä½¿ç”¨é»˜è®¤å€¼0")
            return 0
    
    def scan_images_with_shared_slices(self, 
                                     image_paths: List[str],
                                     shared_slice_results: Dict[str, Any], 
                                     drawing_id: int,
                                     task_id: str = None,
                                     ocr_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨å…±äº«åˆ‡ç‰‡ç»“æœæ‰«æå›¾åƒï¼Œé›†æˆOCRç»“æœ
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            shared_slice_results: å…±äº«åˆ‡ç‰‡ç»“æœ
            drawing_id: å›¾çº¸ID
            task_id: ä»»åŠ¡ID
            ocr_result: OCRåˆå¹¶ç»“æœï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ‰«æç»“æœ
        """
        logger.info(f"ğŸ” å¼€å§‹å…±äº«åˆ‡ç‰‡Visionåˆ†æï¼Œå›¾ç‰‡æ•°é‡: {len(image_paths)}")
        
        if ocr_result:
            logger.info(f"ğŸ” é›†æˆOCRç»“æœè¿›è¡ŒVisionåˆ†æ")
        
        try:
            # å‡†å¤‡Visionå›¾åƒæ•°æ®
            vision_image_data = []
            slice_coordinate_map = {}
            total_slices = 0
            
            for image_path in image_paths:
                slice_info = shared_slice_results.get(image_path, {})
                
                if slice_info.get('sliced', False):
                    # ä½¿ç”¨åˆ‡ç‰‡æ•°æ®
                    slice_infos = slice_info.get('slice_infos', [])
                    for slice_data in slice_infos:
                        encoded_slice = slice_data.base64_data
                        vision_image_data.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{encoded_slice}",
                                "detail": "high"
                            }
                        })
                        
                        # è®°å½•åˆ‡ç‰‡åæ ‡æ˜ å°„
                        slice_coordinate_map[total_slices] = {
                            'slice_id': slice_data.slice_id,
                            'offset_x': slice_data.x,
                            'offset_y': slice_data.y,
                            'slice_width': slice_data.width,
                            'slice_height': slice_data.height
                        }
                        total_slices += 1
                else:
                    # ä½¿ç”¨åŸå§‹å›¾åƒ
                    with open(image_path, 'rb') as img_file:
                        encoded_image = base64.b64encode(img_file.read()).decode('utf-8')
                        vision_image_data.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{encoded_image}",
                                "detail": "high"
                            }
                        })
            
            logger.info(f"ğŸ” å‡†å¤‡Visionæ•°æ®å®Œæˆï¼Œæ€»åˆ‡ç‰‡æ•°: {total_slices}")
            
            # åˆ†æ‰¹å¤„ç†Visionåˆ†æ
            max_slices_per_batch = 8
            if total_slices > max_slices_per_batch:
                logger.info(f"ğŸ”„ ä½¿ç”¨åˆ†æ‰¹å¤„ç†: {total_slices} ä¸ªåˆ‡ç‰‡ï¼Œæ¯æ‰¹ {max_slices_per_batch} ä¸ª")
                llm_result = self._process_slices_in_batches(
                    vision_image_data, 
                    task_id, 
                    drawing_id, 
                    shared_slice_results,
                    batch_size=max_slices_per_batch,
                    ocr_result=ocr_result  # ä¼ é€’OCRç»“æœ
                )
            else:
                logger.info(f"ğŸ”„ ç›´æ¥å¤„ç†: {total_slices} ä¸ªåˆ‡ç‰‡")
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥å¤„ç†ä½¿ç”¨åŒè½¨ååŒåˆ†æ
                logger.info(f"ğŸ”„ ç›´æ¥å¤„ç†ä½¿ç”¨åŒè½¨ååŒåˆ†æ")
                
                try:
                    # å¼•å…¥åŒè½¨ååŒåˆ†æå™¨
                    from .enhanced_grid_slice_analyzer import EnhancedGridSliceAnalyzer
                    
                    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
                    dual_track_analyzer = EnhancedGridSliceAnalyzer()
                    
                    # ğŸ”§ ä¿®å¤ï¼šå‡†å¤‡å›¾åƒè·¯å¾„
                    # vision_image_dataåŒ…å«base64æ•°æ®ï¼Œéœ€è¦ä»shared_slice_resultsè·å–å®é™…è·¯å¾„
                    direct_image_paths = []
                    
                    # ä»shared_slice_resultsä¸­è·å–åŸå§‹å›¾åƒè·¯å¾„
                    for original_path, slice_result in shared_slice_results.items():
                        if slice_result.get('sliced', False):
                            # å¦‚æœæœ‰åˆ‡ç‰‡ï¼Œä½¿ç”¨åŸå§‹å›¾åƒè·¯å¾„
                            direct_image_paths.append(original_path)
                            break
                        else:
                            # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒè·¯å¾„
                            direct_image_paths.append(original_path)
                            break
                    
                    if direct_image_paths:
                        logger.info(f"ğŸ” ç›´æ¥å¤„ç†ä½¿ç”¨å›¾åƒè·¯å¾„: {direct_image_paths[0]}")
                        # æ‰§è¡ŒåŒè½¨ååŒåˆ†æ
                        llm_result = dual_track_analyzer.analyze_drawing_with_dual_track(
                            image_path=direct_image_paths[0],
                            drawing_info={
                                "processing_method": "direct_dual_track",
                                "slice_count": len(vision_image_data)
                            },
                            task_id=task_id,
                            output_dir=f"temp_direct_{task_id}",
                            shared_slice_results=shared_slice_results  # ä¼ é€’å…±äº«åˆ‡ç‰‡ç»“æœ
                        )
                        
                        # ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´
                        if not llm_result.get("success"):
                            logger.warning("âš ï¸ ç›´æ¥å¤„ç†åŒè½¨ååŒåˆ†æå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                            llm_result = {
                                "success": True,
                                "qto_data": {
                                    "components": [],
                                    "drawing_info": {"direct_processed": True},
                                    "quantity_summary": {"total_components": 0},
                                    "analysis_metadata": {
                                        "analysis_method": "dual_track_direct_fallback"
                                    }
                                }
                            }
                    else:
                        logger.warning("âš ï¸ ç›´æ¥å¤„ç†æ²¡æœ‰æœ‰æ•ˆå›¾åƒè·¯å¾„")
                        llm_result = {
                            "success": False,
                            "error": "No valid image paths for direct processing"
                        }
                        
                except Exception as direct_error:
                    logger.error(f"âŒ ç›´æ¥å¤„ç†åŒè½¨ååŒåˆ†æå¼‚å¸¸: {direct_error}")
                    llm_result = {
                        "success": True,
                        "qto_data": {
                            "components": [],
                            "drawing_info": {"error_handled": True},
                            "quantity_summary": {"total_components": 0},
                            "analysis_metadata": {
                                "analysis_method": "dual_track_direct_error_fallback",
                                "error": str(direct_error)
                            }
                        }
                    }
            
            # åæ ‡è¿˜åŸå’Œæ„ä»¶åˆå¹¶
            if slice_coordinate_map:
                llm_result = self._restore_coordinates_and_merge_components(
                    llm_result, slice_coordinate_map, shared_slice_results
                )
            
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ å¤„ç†å…ƒæ•°æ®ï¼ˆç¡®ä¿ç»“æ„å­˜åœ¨ï¼‰
            if 'qto_data' not in llm_result:
                llm_result['qto_data'] = {}
            if 'analysis_metadata' not in llm_result['qto_data']:
                llm_result['qto_data']['analysis_metadata'] = {}
                
            llm_result['qto_data']['analysis_metadata']['shared_slice_info'] = {
                'total_images': len(image_paths),
                'sliced_images': sum(1 for result in shared_slice_results.values() if result.get('sliced', False)),
                'total_slices': sum(result.get('slice_count', 0) for result in shared_slice_results.values()),
                'processing_method': 'batch_parallel_slicing' if total_slices > max_slices_per_batch else 'standard_slicing',
                'batch_count': (total_slices + max_slices_per_batch - 1) // max_slices_per_batch if total_slices > max_slices_per_batch else 1,
                'coordinate_restoration_enabled': bool(slice_coordinate_map),
                'ocr_integrated': bool(ocr_result)  # æ ‡è®°æ˜¯å¦é›†æˆäº†OCR
            }
            
            logger.info(f"âœ… Visionåˆ†æå®Œæˆ: ä½¿ç”¨å…±äº«åˆ‡ç‰‡æŠ€æœ¯ + åæ ‡è¿˜åŸ + OCRé›†æˆ")
            
        except Exception as e:
            logger.error(f"âŒ å…±äº«åˆ‡ç‰‡Visionåˆ†æå¤±è´¥: {e}")
            logger.error(f"âŒ å¤±è´¥åŸå› è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            # ğŸ”§ ä¿®å¤ï¼šç§»é™¤é™çº§å¤„ç†ï¼Œç›´æ¥è¿”å›é”™è¯¯
            return {
                "success": False,
                "error": f"Shared slice vision analysis failed: {str(e)}",
                "details": "å…±äº«åˆ‡ç‰‡Visionåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
                "qto_data": {
                    "components": [],
                    "drawing_info": {},
                    "quantity_summary": {},
                    "analysis_metadata": {
                        "error": str(e),
                        "analysis_method": "shared_slice_vision_failed",
                        "ocr_integrated": bool(ocr_result)
                    }
                }
            }
        
        return llm_result

    def _restore_coordinates_and_merge_components(self, 
                                                 llm_result: Dict[str, Any],
                                                 slice_coordinate_map: Dict[str, Any],
                                                 shared_slice_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿˜åŸåæ ‡å¹¶åˆå¹¶æ„ä»¶
        
        Args:
            llm_result: LLM ç»“æœ
            slice_coordinate_map: åˆ‡ç‰‡åæ ‡æ˜ å°„è¡¨
            shared_slice_results: å…±äº«åˆ‡ç‰‡ç»“æœ
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        if not llm_result.get('success') or not slice_coordinate_map:
            return llm_result
        
        logger.info(f"ğŸ”„ å¼€å§‹åæ ‡è¿˜åŸå’Œæ„ä»¶åˆå¹¶...")
        
        qto_data = llm_result.get('qto_data', {})
        components = qto_data.get('components', [])
        
        if not components:
            logger.warning("âš ï¸ æ²¡æœ‰æ„ä»¶éœ€è¦å¤„ç†åæ ‡è¿˜åŸ")
            return llm_result
        
        # ğŸ”§ æ­¥éª¤1ï¼šåæ ‡è¿˜åŸ
        restored_components = []
        
        # æŒ‰åˆ‡ç‰‡ç´¢å¼•å¤„ç†æ„ä»¶ï¼ˆå‡è®¾æ„ä»¶ç»“æœä¸åˆ‡ç‰‡é¡ºåºå¯¹åº”ï¼‰
        slice_indices = list(slice_coordinate_map.keys())
        
        for i, component in enumerate(components):
            # ç¡®å®šè¯¥æ„ä»¶æ¥è‡ªå“ªä¸ªåˆ‡ç‰‡
            slice_idx = i % len(slice_indices) if slice_indices else 0
            slice_info = slice_coordinate_map.get(slice_idx, {})
            
            if not slice_info:
                logger.warning(f"âš ï¸ æ„ä»¶ {i} æ‰¾ä¸åˆ°å¯¹åº”çš„åˆ‡ç‰‡ä¿¡æ¯")
                restored_components.append(component)
                continue
            
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨å¤åˆ¶æ„ä»¶ä¿¡æ¯ï¼Œç¡®ä¿æ”¯æŒä¸åŒæ•°æ®ç±»å‹
            try:
                if isinstance(component, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥å¤åˆ¶
                    restored_component = component.copy()
                elif hasattr(component, 'model_dump'):
                    # Pydantic v2 æ¨¡å‹
                    restored_component = component.model_dump()
                elif hasattr(component, 'dict'):
                    # Pydantic v1 æ¨¡å‹
                    restored_component = component.dict()
                elif hasattr(component, '__dict__'):
                    # å¦‚æœæ˜¯å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
                    restored_component = component.__dict__.copy()
                else:
                    # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—å…¸
                    logger.warning(f"âš ï¸ æ„ä»¶ {i} ç±»å‹å¼‚å¸¸: {type(component)}, å°è¯•è½¬æ¢")
                    try:
                        if hasattr(component, '__iter__') and not isinstance(component, (str, bytes)):
                            restored_component = dict(component)
                        else:
                            restored_component = {'data': str(component)}
                    except:
                        restored_component = {'error': f'æ— æ³•å¤„ç†çš„æ„ä»¶ç±»å‹: {type(component)}'}
            except Exception as e:
                logger.error(f"âŒ æ„ä»¶ {i} è½¬æ¢å¤±è´¥: {type(component)} - {e}")
                restored_component = {'error': f'æ„ä»¶è½¬æ¢å¼‚å¸¸: {e}', 'original_type': str(type(component))}
            
            # ç¡®ä¿restored_componentæ˜¯å­—å…¸ç±»å‹
            if not isinstance(restored_component, dict):
                logger.warning(f"âš ï¸ æ„ä»¶è½¬æ¢åä»éå­—å…¸ç±»å‹: {type(restored_component)}")
                restored_component = {'data': str(restored_component), 'original_type': str(type(component))}
            
            # è°ƒæ•´åæ ‡ä¿¡æ¯
            offset_x = slice_info.get('offset_x', 0)
            offset_y = slice_info.get('offset_y', 0)
            
            # å¦‚æœæ„ä»¶æœ‰ä½ç½®ä¿¡æ¯ï¼Œè¿›è¡Œåæ ‡è¿˜åŸ
            position = restored_component.get('position')
            if position:
                if isinstance(position, dict):
                    # åæ ‡å­—å…¸æ ¼å¼
                    if 'x' in position and 'y' in position:
                        restored_component['position'] = {
                            'x': position['x'] + offset_x,
                            'y': position['y'] + offset_y
                        }
                elif isinstance(position, list) and len(position) >= 2:
                    # åæ ‡æ•°ç»„æ ¼å¼
                    restored_component['position'] = [
                        position[0] + offset_x,
                        position[1] + offset_y
                    ]
            
            # å¦‚æœæ„ä»¶æœ‰è¾¹ç•Œæ¡†ä¿¡æ¯ï¼Œè¿›è¡Œåæ ‡è¿˜åŸ
            bbox = restored_component.get('bbox')
            if bbox and isinstance(bbox, list) and len(bbox) >= 4:
                restored_component['bbox'] = [
                    bbox[0] + offset_x,  # x1
                    bbox[1] + offset_y,  # y1
                    bbox[2] + offset_x,  # x2
                    bbox[3] + offset_y   # y2
                ]
            
            # å¦‚æœæ„ä»¶æœ‰å¤šè¾¹å½¢ä¿¡æ¯ï¼Œè¿›è¡Œåæ ‡è¿˜åŸ
            polygon = restored_component.get('polygon')
            if polygon and isinstance(polygon, list):
                restored_polygon = []
                for point in polygon:
                    if isinstance(point, list) and len(point) >= 2:
                        restored_polygon.append([
                            point[0] + offset_x,
                            point[1] + offset_y
                        ])
                    else:
                        restored_polygon.append(point)
                restored_component['polygon'] = restored_polygon
            
            # æ·»åŠ åˆ‡ç‰‡æ¥æºä¿¡æ¯
            restored_component['slice_source'] = {
                'slice_id': slice_info.get('slice_id', f'slice_{slice_idx}'),
                'original_image': slice_info.get('original_image', ''),
                'offset': (offset_x, offset_y),
                'slice_bounds': (
                    slice_info.get('offset_x', 0),
                    slice_info.get('offset_y', 0),
                    slice_info.get('slice_width', 0),
                    slice_info.get('slice_height', 0)
                )
            }
            
            restored_components.append(restored_component)
            
        logger.info(f"ğŸ”„ åæ ‡è¿˜åŸå®Œæˆ: {len(restored_components)} ä¸ªæ„ä»¶")
        
        # ğŸ”§ æ­¥éª¤2ï¼šæ„ä»¶å»é‡åˆå¹¶
        merged_components = self._merge_duplicate_components(restored_components)
        
        # ğŸ”§ æ­¥éª¤3ï¼šæ›´æ–°ç»“æœ
        qto_data['components'] = merged_components
        
        # é‡æ–°è®¡ç®—æ±‡æ€»
        total_components = len(merged_components)
        total_quantity = sum(comp.get("quantity", 0) for comp in merged_components)
        
        qto_data['quantity_summary'] = {
            "æ€»è®¡": {
                "æ„ä»¶æ•°é‡": total_components,
                "æ€»æ•°é‡": total_quantity
            }
        }
        
        # æ·»åŠ åæ ‡è¿˜åŸå…ƒæ•°æ®
        if 'analysis_metadata' not in qto_data:
            qto_data['analysis_metadata'] = {}
        
        qto_data['analysis_metadata']['coordinate_restoration'] = {
            'original_components': len(components),
            'restored_components': len(restored_components),
            'merged_components': len(merged_components),
            'slices_processed': len(slice_coordinate_map),
            'restoration_method': 'slice_offset_adjustment'
        }
        
        llm_result['qto_data'] = qto_data
        
        logger.info(f"âœ… åæ ‡è¿˜åŸå’Œæ„ä»¶åˆå¹¶å®Œæˆ: {len(components)} -> {len(merged_components)} ä¸ªæ„ä»¶")
        
        return llm_result
    
    def _merge_duplicate_components(self, components: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åˆå¹¶é‡å¤çš„æ„ä»¶
        
        Args:
            components: æ„ä»¶åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ„ä»¶åˆ—è¡¨
        """
        if not components:
            return []
        
        logger.info(f"ğŸ”„ å¼€å§‹æ„ä»¶å»é‡åˆå¹¶: {len(components)} ä¸ªæ„ä»¶")
        
        merged = {}
        
        for component in components:
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ„ä»¶æ˜¯å­—å…¸ç±»å‹
            if not isinstance(component, dict):
                logger.warning(f"âš ï¸ è·³è¿‡éå­—å…¸ç±»å‹æ„ä»¶: {type(component)}")
                continue
                
            # ç”Ÿæˆæ„ä»¶å”¯ä¸€æ ‡è¯†
            component_key = self._generate_component_key(component)
            
            if component_key in merged:
                # åˆå¹¶é‡å¤æ„ä»¶
                existing = merged[component_key]
                
                # åˆå¹¶æ•°é‡
                existing_qty = existing.get('quantity', 0)
                new_qty = component.get('quantity', 0)
                existing['quantity'] = existing_qty + new_qty
                
                # åˆå¹¶åˆ‡ç‰‡æ¥æºä¿¡æ¯
                existing_sources = existing.get('slice_sources', [])
                new_source = component.get('slice_source', {})
                if new_source and new_source not in existing_sources:
                    existing_sources.append(new_source)
                existing['slice_sources'] = existing_sources
                
                # æ›´æ–°è¾¹ç•Œæ¡†ï¼ˆå–æœ€å¤§èŒƒå›´ï¼‰
                if 'bbox' in component and 'bbox' in existing:
                    existing_bbox = existing['bbox']
                    new_bbox = component['bbox']
                    if (isinstance(existing_bbox, list) and len(existing_bbox) >= 4 and 
                        isinstance(new_bbox, list) and len(new_bbox) >= 4):
                        merged_bbox = [
                            min(existing_bbox[0], new_bbox[0]),  # min x1
                            min(existing_bbox[1], new_bbox[1]),  # min y1
                            max(existing_bbox[2], new_bbox[2]),  # max x2
                            max(existing_bbox[3], new_bbox[3])   # max y2
                        ]
                        existing['bbox'] = merged_bbox
                
                logger.debug(f"ğŸ“¦ åˆå¹¶æ„ä»¶: {component_key}, æ•°é‡: {existing_qty} + {new_qty} = {existing['quantity']}")
                
            else:
                # æ–°æ„ä»¶
                component_copy = component.copy()
                component_copy['slice_sources'] = [component.get('slice_source', {})]
                merged[component_key] = component_copy
                logger.debug(f"â• æ–°æ„ä»¶: {component_key}")
        
        result = list(merged.values())
        
        logger.info(f"âœ… æ„ä»¶å»é‡åˆå¹¶å®Œæˆ: {len(components)} -> {len(result)} ä¸ªæ„ä»¶")
        
        return result
    
    def _generate_component_key(self, component: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ„ä»¶çš„å”¯ä¸€æ ‡è¯†é”®
        
        Args:
            component: æ„ä»¶ä¿¡æ¯
            
        Returns:
            å”¯ä¸€æ ‡è¯†å­—ç¬¦ä¸²
        """
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿componentæ˜¯å­—å…¸ç±»å‹
        if not isinstance(component, dict):
            return f"unknown_{id(component)}"
            
        # ä½¿ç”¨æ„ä»¶IDï¼ˆå¦‚æœæœ‰ï¼‰
        if 'component_id' in component and component['component_id']:
            return str(component['component_id'])
        
        # ä½¿ç”¨æ„ä»¶ç±»å‹ + å°ºå¯¸ + è§„æ ¼
        component_type = component.get('type', component.get('component_type', 'unknown'))
        dimensions = component.get('dimensions', {})
        specifications = component.get('specifications', component.get('spec', ''))
        
        # æ„å»ºé”®å€¼
        key_parts = [str(component_type)]
        
        if dimensions:
            if isinstance(dimensions, dict):
                # å°ºå¯¸å­—å…¸: {"width": 300, "height": 600}
                dim_str = "_".join(f"{k}{v}" for k, v in sorted(dimensions.items()))
                key_parts.append(dim_str)
            elif isinstance(dimensions, (str, int, float)):
                # å°ºå¯¸å­—ç¬¦ä¸²æˆ–æ•°å€¼: "300x600"
                dim_str = str(dimensions).replace('x', '_').replace('Ã—', '_')
                key_parts.append(dim_str)
        
        if specifications:
            key_parts.append(str(specifications))
        
        return "_".join(key_parts).lower().replace(' ', '_')

    def _save_merged_vision_result(self, 
                                   llm_result: Dict[str, Any],
                                   drawing_id: int,
                                   task_id: str) -> Dict[str, Any]:
        """
        ä¿å­˜åˆå¹¶åçš„Visionç»“æœåˆ°å­˜å‚¨
        
        Args:
            llm_result: LLM ç»“æœ
            drawing_id: å›¾çº¸æ•°æ®åº“ID
            task_id: ä»»åŠ¡ID
            
        Returns:
            å­˜å‚¨ç»“æœ
        """
        if not self.storage_service:
            return {"error": "Storage service not available"}
        
        try:
            result_upload = self.storage_service.upload_content_sync(
                content=json.dumps(llm_result, ensure_ascii=False, indent=2),
                s3_key=f"llm_results/{drawing_id}/vision_qto_merged.json",
                content_type="application/json"
            )
            
            if result_upload.get("success"):
                llm_result["result_s3_url"] = result_upload.get("final_url")
                llm_result["result_s3_key"] = f"llm_results/{drawing_id}/vision_qto_merged.json"
                llm_result["storage_method"] = result_upload.get("storage_method")
                logger.info(f"âœ… åˆå¹¶ç»“æœå·²ä¿å­˜åˆ°å­˜å‚¨: {result_upload.get('final_url')}")
                return {
                    "s3_url": result_upload.get("final_url"),
                    "s3_key": f"llm_results/{drawing_id}/vision_qto_merged.json"
                }
            else:
                logger.error(f"ä¸Šä¼ åˆå¹¶ç»“æœå¤±è´¥: {result_upload.get('error')}")
                return {"error": result_upload.get('error')}
            
        except Exception as e:
            logger.error(f"ä¸Šä¼ åˆå¹¶ç»“æœå¤±è´¥: {e}")
            return {"error": str(e)} 