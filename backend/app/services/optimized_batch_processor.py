#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†å™¨
é›†æˆOCRç¼“å­˜ç®¡ç†ã€åˆ†æå™¨å®ä¾‹å¤ç”¨ã€ç»Ÿä¸€æ—¥å¿—è®°å½•ç­‰ä¼˜åŒ–åŠŸèƒ½
"""

import logging
import time
from typing import List, Dict, Any, Optional
from dataclasses import asdict

from app.utils.analysis_optimizations import (
    AnalyzerInstanceManager, AnalysisLogger, AnalysisMetadata, 
    ocr_cache_manager, GPTResponseParser
)
from app.core.config import AnalysisSettings

logger = logging.getLogger(__name__)

class OptimizedBatchProcessor:
    """ä¼˜åŒ–çš„æ‰¹æ¬¡å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†å™¨"""
        self.analyzer_manager = AnalyzerInstanceManager()
        self.ocr_cache = ocr_cache_manager
        self.global_ocr_cache = {}
        self.ocr_cache_initialized = False
    
    def process_slices_in_batches_optimized(self, 
                                          vision_image_data: List[Dict],
                                          task_id: str,
                                          drawing_id: int,
                                          shared_slice_results: Dict[str, Any],
                                          batch_size: int = None,
                                          ocr_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„åˆ†æ‰¹æ¬¡å¤„ç†åˆ‡ç‰‡æ•°æ®
        
        Args:
            vision_image_data: Visionå›¾åƒæ•°æ®åˆ—è¡¨
            task_id: ä»»åŠ¡ID
            drawing_id: å›¾çº¸ID
            shared_slice_results: å…±äº«åˆ‡ç‰‡ç»“æœ
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            ocr_result: OCRç»“æœ
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if not vision_image_data:
            return {"success": False, "error": "No vision image data provided"}
        
        # ä½¿ç”¨é…ç½®åŒ–çš„æ‰¹æ¬¡å¤§å°
        if batch_size is None:
            batch_size = AnalysisSettings.MAX_SLICES_PER_BATCH
        
        total_slices = len(vision_image_data)
        total_batches = (total_slices + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ”„ å¼€å§‹ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†: {total_slices} ä¸ªåˆ‡ç‰‡ï¼Œåˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡")
        
        batch_results = []
        successful_batches = 0
        failed_batches = 0
        start_time = time.time()
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_slices)
            batch_data = vision_image_data[start_idx:end_idx]
            
            batch_task_id = f"{task_id}_batch_{batch_idx + 1}"
            
            # ä½¿ç”¨æ ‡å‡†åŒ–æ—¥å¿—è®°å½•
            AnalysisLogger.log_batch_processing(batch_idx + 1, total_batches, len(batch_data))
            
            # åˆ›å»ºæ‰¹æ¬¡å…ƒæ•°æ®
            batch_metadata = AnalysisMetadata(
                analysis_method="optimized_batch_processing",
                batch_id=batch_idx + 1,
                slice_count=len(batch_data),
                success=False,
                ocr_cache_used=self.ocr_cache_initialized
            )
            
            try:
                # æ·»åŠ æ‰¹æ¬¡é—´å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
                if batch_idx > 0:
                    time.sleep(AnalysisSettings.BATCH_PROCESSING_DELAY)
                
                # å¤„ç†å•ä¸ªæ‰¹æ¬¡
                batch_result = self._process_single_batch_optimized(
                    batch_data=batch_data,
                    batch_idx=batch_idx,
                    batch_task_id=batch_task_id,
                    shared_slice_results=shared_slice_results,
                    ocr_result=ocr_result,
                    metadata=batch_metadata
                )
                
                if batch_result.get("success"):
                    batch_results.append(batch_result)
                    successful_batches += 1
                    batch_metadata.success = True
                else:
                    failed_batches += 1
                    batch_metadata.error_message = batch_result.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {batch_result.get('error')}")
                
                # è®°å½•æ‰¹æ¬¡å…ƒæ•°æ®
                batch_metadata.processing_time = time.time() - start_time
                AnalysisLogger.log_analysis_metadata(batch_metadata)
                
            except Exception as e:
                failed_batches += 1
                batch_metadata.error_message = str(e)
                batch_metadata.processing_time = time.time() - start_time
                AnalysisLogger.log_analysis_metadata(batch_metadata)
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¼‚å¸¸: {e}")
        
        # åˆå¹¶æ‰¹æ¬¡ç»“æœ
        if batch_results:
            merged_result = self._merge_batch_results_optimized(batch_results)
        else:
            merged_result = {"success": False, "error": "æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å¤±è´¥"}
        
        # æ·»åŠ æ‰¹æ¬¡å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        merged_result["batch_statistics"] = {
            "total_batches": total_batches,
            "successful_batches": successful_batches,
            "failed_batches": failed_batches,
            "batch_size": batch_size,
            "total_slices": total_slices,
            "ocr_cache_enabled": self.ocr_cache_initialized,
            "processing_time": time.time() - start_time,
            "success_rate": successful_batches / total_batches if total_batches > 0 else 0
        }
        
        # è®°å½•ç¼“å­˜ç»Ÿè®¡
        AnalysisLogger.log_cache_stats(self.ocr_cache.get_cache_stats())
        
        logger.info(f"âœ… ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†å®Œæˆ: {successful_batches}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        
        return merged_result
    
    def _process_single_batch_optimized(self, 
                                      batch_data: List[Dict],
                                      batch_idx: int,
                                      batch_task_id: str,
                                      shared_slice_results: Dict[str, Any],
                                      ocr_result: Dict[str, Any] = None,
                                      metadata: AnalysisMetadata = None) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # è·å–åˆ†æå™¨å®ä¾‹ï¼ˆå¤ç”¨ï¼‰
            from app.services.enhanced_grid_slice_analyzer import EnhancedGridSliceAnalyzer
            dual_track_analyzer = self.analyzer_manager.get_analyzer(EnhancedGridSliceAnalyzer)
            
            # é‡ç½®æ‰¹æ¬¡çŠ¶æ€
            self.analyzer_manager.reset_for_new_batch()
            
            # ä¼ é€’OCRç¼“å­˜ç»™åˆ†æå™¨
            if self.ocr_cache_initialized and self.global_ocr_cache:
                dual_track_analyzer._global_ocr_cache = self.global_ocr_cache.copy()
                logger.info(f"â™»ï¸ æ‰¹æ¬¡ {batch_idx + 1} ä½¿ç”¨å…¨å±€OCRç¼“å­˜: {len(self.global_ocr_cache)} ä¸ªåˆ‡ç‰‡")
            
            # å‡†å¤‡æ‰¹æ¬¡å›¾åƒè·¯å¾„
            batch_image_paths = []
            for original_path, slice_result in shared_slice_results.items():
                if slice_result.get('sliced', False):
                    batch_image_paths.append(original_path)
                    break  # åªéœ€è¦ä¸€ä¸ªä»£è¡¨è·¯å¾„
            
            if not batch_image_paths:
                return {"success": False, "error": "æ— æ³•è·å–æ‰¹æ¬¡å›¾åƒè·¯å¾„"}
            
            # åˆ›å»ºæ‰¹æ¬¡ç»˜å›¾ä¿¡æ¯
            drawing_info = {
                "batch_id": batch_idx + 1,
                "slice_count": len(batch_data),
                "processing_method": "optimized_batch_dual_track",
                "ocr_cache_enabled": self.ocr_cache_initialized,
                "drawing_id": shared_slice_results.get("drawing_id", 0)
            }
            
            # å¦‚æœæœ‰OCRç»“æœï¼Œæ·»åŠ ç›¸å…³ä¿¡æ¯
            if ocr_result:
                drawing_info["ocr_info"] = {
                    "has_merged_text": bool(ocr_result.get("merged_text_regions")),
                    "total_text_regions": len(ocr_result.get("merged_text_regions", [])),
                    "all_text_available": bool(ocr_result.get("all_text"))
                }
            
            # æ‰§è¡ŒåŒè½¨ååŒåˆ†æ
            batch_result = dual_track_analyzer.analyze_drawing_with_dual_track(
                image_path=batch_image_paths[0],
                drawing_info=drawing_info,
                task_id=batch_task_id,
                output_dir=f"temp_batch_{batch_task_id}",
                shared_slice_results=shared_slice_results
            )
            
            # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡å»ºç«‹OCRç¼“å­˜
            if batch_idx == 0 and not self.ocr_cache_initialized:
                if hasattr(dual_track_analyzer, '_global_ocr_cache') and dual_track_analyzer._global_ocr_cache:
                    self.global_ocr_cache = dual_track_analyzer._global_ocr_cache.copy()
                    self.ocr_cache_initialized = True
                    logger.info(f"ğŸ”„ å»ºç«‹å…¨å±€OCRç¼“å­˜: {len(self.global_ocr_cache)} ä¸ªåˆ‡ç‰‡")
            
            # ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´
            if batch_result.get("success"):
                qto_data = batch_result.get("qto_data", {})
                
                # æ·»åŠ ä¼˜åŒ–å…ƒæ•°æ®
                if "analysis_metadata" not in qto_data:
                    qto_data["analysis_metadata"] = {}
                
                qto_data["analysis_metadata"].update({
                    "batch_id": batch_idx + 1,
                    "slice_count": len(batch_data),
                    "ocr_cache_used": self.ocr_cache_initialized,
                    "analyzer_reused": True,
                    "processing_method": "optimized_dual_track_analysis"
                })
                
                return {
                    "success": True,
                    "qto_data": qto_data
                }
            else:
                return {
                    "success": False,
                    "error": batch_result.get("error", "æ‰¹æ¬¡åˆ†æå¤±è´¥")
                }
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¼‚å¸¸: {e}")
            return {"success": False, "error": str(e)}
    
    def _merge_batch_results_optimized(self, batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ‰¹æ¬¡ç»“æœåˆå¹¶"""
        try:
            all_components = []
            total_processing_time = 0.0
            
            for batch_result in batch_results:
                if batch_result.get("success") and batch_result.get("qto_data"):
                    qto_data = batch_result["qto_data"]
                    components = qto_data.get("components", [])
                    all_components.extend(components)
                    
                    # ç´¯ç§¯å¤„ç†æ—¶é—´
                    metadata = qto_data.get("analysis_metadata", {})
                    total_processing_time += metadata.get("processing_time", 0.0)
            
            # å»é‡åˆå¹¶æ„ä»¶
            merged_components = self._deduplicate_components_optimized(all_components)
            
            # ç”Ÿæˆåˆå¹¶ç»“æœ
            merged_result = {
                "success": True,
                "qto_data": {
                    "components": merged_components,
                    "drawing_info": {},
                    "quantity_summary": self._calculate_quantity_summary_optimized(merged_components),
                    "analysis_metadata": {
                        "total_components": len(merged_components),
                        "original_components": len(all_components),
                        "deduplication_rate": 1 - (len(merged_components) / len(all_components)) if all_components else 0,
                        "total_processing_time": total_processing_time,
                        "merge_method": "optimized_batch_merge"
                    }
                }
            }
            
            logger.info(f"âœ… æ‰¹æ¬¡ç»“æœåˆå¹¶å®Œæˆ: {len(all_components)} â†’ {len(merged_components)} ä¸ªæ„ä»¶")
            
            return merged_result
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ç»“æœåˆå¹¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _deduplicate_components_optimized(self, components: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–çš„æ„ä»¶å»é‡"""
        if not components:
            return []
        
        # ä½¿ç”¨ç»„åˆé”®è¿›è¡Œå»é‡
        seen_keys = set()
        unique_components = []
        
        for component in components:
            # ç”Ÿæˆå”¯ä¸€é”®
            component_id = component.get("component_id", "")
            component_type = component.get("component_type", "")
            location = component.get("location", {})
            
            # ä½ç½®ä¿¡æ¯ç”¨äºå»é‡
            x = location.get("global_x", location.get("x", 0)) if isinstance(location, dict) else 0
            y = location.get("global_y", location.get("y", 0)) if isinstance(location, dict) else 0
            
            # åˆ›å»ºå”¯ä¸€é”®ï¼ˆID + ç±»å‹ + å¤§è‡´ä½ç½®ï¼‰
            unique_key = f"{component_id}_{component_type}_{int(x//100)}_{int(y//100)}"
            
            if unique_key not in seen_keys:
                seen_keys.add(unique_key)
                unique_components.append(component)
        
        return unique_components
    
    def _calculate_quantity_summary_optimized(self, components: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„å·¥ç¨‹é‡æ±‡æ€»è®¡ç®—"""
        if not components:
            return {"total_count": 0, "component_types": {}}
        
        type_summary = {}
        total_volume = 0.0
        total_area = 0.0
        
        for component in components:
            comp_type = component.get("component_type", "æœªçŸ¥")
            
            if comp_type not in type_summary:
                type_summary[comp_type] = {
                    "count": 0,
                    "volume": 0.0,
                    "area": 0.0
                }
            
            type_summary[comp_type]["count"] += 1
            
            # æå–ä½“ç§¯å’Œé¢ç§¯ä¿¡æ¯
            geometry = component.get("geometry", {})
            if isinstance(geometry, dict):
                volume = float(geometry.get("volume", 0))
                area = float(geometry.get("area", 0))
                
                type_summary[comp_type]["volume"] += volume
                type_summary[comp_type]["area"] += area
                total_volume += volume
                total_area += area
        
        return {
            "total_count": len(components),
            "total_volume": round(total_volume, 2),
            "total_area": round(total_area, 2),
            "component_types": type_summary
        }
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "analyzer_stats": self.analyzer_manager.get_instance_stats(),
            "cache_stats": self.ocr_cache.get_cache_stats(),
            "ocr_cache_initialized": self.ocr_cache_initialized,
            "global_cache_size": len(self.global_ocr_cache)
        } 