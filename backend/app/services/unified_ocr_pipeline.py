"""
ç»Ÿä¸€OCRå¤„ç†ç®¡é“ - Step 2~2.5 ä¼˜åŒ–
å®ç°æ ‡å‡†åŒ–çš„OCRè¯†åˆ«ä¸æ¸…æ´—ä¸€ä½“åŒ–æµç¨‹
"""

import os
import json
import time
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import asyncio
import logging

logger = logging.getLogger(__name__)

@dataclass
class OCRResult:
    """æ ‡å‡†åŒ–OCRç»“æœ"""
    slice_id: str
    filename: str
    coordinates: Dict[str, int]  # x, y, width, height
    raw_text: str
    confidence: float
    processing_time: float
    ocr_engine: str
    language: str = "zh-CN"

@dataclass
class CleanedOCRData:
    """æ¸…æ´—åçš„OCRæ•°æ®"""
    text_blocks: List[Dict[str, Any]]
    component_labels: List[Dict[str, Any]]
    dimensions: List[Dict[str, Any]]
    annotations: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    confidence_score: float
    processing_stage: str

@dataclass
class StandardizedOutput:
    """æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼"""
    task_id: str
    timestamp: str
    stage: str  # ocr_recognition, ocr_cleaning, summary_generation
    slice_results: List[OCRResult]
    cleaned_data: Optional[CleanedOCRData]
    summary: Optional[Dict[str, Any]]
    quality_metrics: Dict[str, float]
    processing_metadata: Dict[str, Any]

class OCREngineAdapter:
    """OCRå¼•æ“é€‚é…å™¨"""
    
    def __init__(self, engine_type: str = "paddleocr"):
        self.engine_type = engine_type
        self._initialize_engine()
    
    def _initialize_engine(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        if self.engine_type == "paddleocr":
            try:
                from paddleocr import PaddleOCR
                self.ocr_engine = PaddleOCR(use_angle_cls=True, lang='ch')
                logger.info("âœ… PaddleOCRå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            except ImportError:
                logger.error("âŒ PaddleOCRæœªå®‰è£…")
                self.ocr_engine = None
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„OCRå¼•æ“ç±»å‹: {self.engine_type}")
            self.ocr_engine = None
    
    def recognize_text(self, image_path: str, slice_info: Dict) -> OCRResult:
        """ç»Ÿä¸€çš„æ–‡æœ¬è¯†åˆ«æ¥å£"""
        start_time = time.time()
        
        try:
            if self.ocr_engine is None:
                # æ¨¡æ‹ŸOCRç»“æœ
                raw_text = f"æ¨¡æ‹ŸOCRç»“æœ - {slice_info.get('filename', 'unknown')}"
                confidence = 0.85
            else:
                # å®é™…OCRè¯†åˆ«
                result = self.ocr_engine.ocr(image_path, cls=True)
                
                # æå–æ–‡æœ¬å’Œç½®ä¿¡åº¦
                text_parts = []
                confidences = []
                
                if result and result[0]:
                    for line in result[0]:
                        if len(line) >= 2:
                            text = line[1][0] if line[1] else ""
                            conf = line[1][1] if len(line[1]) > 1 else 0.0
                            text_parts.append(text)
                            confidences.append(conf)
                
                raw_text = " ".join(text_parts)
                confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            processing_time = time.time() - start_time
            
            return OCRResult(
                slice_id=slice_info.get("slice_id", "unknown"),
                filename=slice_info.get("filename", "unknown"),
                coordinates={
                    "x": slice_info.get("x_offset", 0),
                    "y": slice_info.get("y_offset", 0),
                    "width": slice_info.get("width", 0),
                    "height": slice_info.get("height", 0)
                },
                raw_text=raw_text,
                confidence=confidence,
                processing_time=processing_time,
                ocr_engine=self.engine_type
            )
            
        except Exception as e:
            logger.error(f"âŒ OCRè¯†åˆ«å¤±è´¥ {image_path}: {e}")
            return OCRResult(
                slice_id=slice_info.get("slice_id", "unknown"),
                filename=slice_info.get("filename", "unknown"),
                coordinates={},
                raw_text="",
                confidence=0.0,
                processing_time=time.time() - start_time,
                ocr_engine=self.engine_type
            )

class OCRDataCleaner:
    """OCRæ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.component_keywords = [
            "æŸ±", "æ¢", "æ¿", "å¢™", "åŸºç¡€", "æ¥¼æ¢¯", "é—¨", "çª—",
            "C", "L", "B", "W", "F", "S", "D", "Win"
        ]
        self.dimension_patterns = [
            r"\d+[xÃ—]\d+",  # å°ºå¯¸æ ¼å¼
            r"\d+mm", r"\d+cm", r"\d+m",  # å•ä½æ ¼å¼
            r"Ï†\d+", r"Î¦\d+"  # ç›´å¾„æ ¼å¼
        ]
    
    def clean_ocr_results(self, ocr_results: List[OCRResult]) -> CleanedOCRData:
        """æ¸…æ´—OCRè¯†åˆ«ç»“æœ"""
        try:
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            all_text = " ".join([result.raw_text for result in ocr_results])
            
            # åˆ†ç±»æå–
            text_blocks = self._extract_text_blocks(ocr_results)
            component_labels = self._extract_component_labels(all_text)
            dimensions = self._extract_dimensions(all_text)
            annotations = self._extract_annotations(all_text)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            avg_confidence = sum([r.confidence for r in ocr_results]) / len(ocr_results) if ocr_results else 0.0
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = {
                "total_slices": len(ocr_results),
                "total_text_length": len(all_text),
                "processing_time": sum([r.processing_time for r in ocr_results]),
                "ocr_engines": list(set([r.ocr_engine for r in ocr_results]))
            }
            
            return CleanedOCRData(
                text_blocks=text_blocks,
                component_labels=component_labels,
                dimensions=dimensions,
                annotations=annotations,
                metadata=metadata,
                confidence_score=avg_confidence,
                processing_stage="ocr_cleaning"
            )
            
        except Exception as e:
            logger.error(f"âŒ OCRæ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return CleanedOCRData(
                text_blocks=[],
                component_labels=[],
                dimensions=[],
                annotations=[],
                metadata={"error": str(e)},
                confidence_score=0.0,
                processing_stage="ocr_cleaning_failed"
            )
    
    def _extract_text_blocks(self, ocr_results: List[OCRResult]) -> List[Dict[str, Any]]:
        """æå–æ–‡æœ¬å—"""
        blocks = []
        for result in ocr_results:
            if result.raw_text.strip():
                blocks.append({
                    "slice_id": result.slice_id,
                    "text": result.raw_text,
                    "coordinates": result.coordinates,
                    "confidence": result.confidence
                })
        return blocks
    
    def _extract_component_labels(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ„ä»¶æ ‡ç­¾"""
        import re
        labels = []
        
        # æŸ¥æ‰¾æ„ä»¶ç¼–å·æ¨¡å¼
        patterns = [
            r"[CLBWFSD]\d+[a-zA-Z]?",  # C1, L1, B1ç­‰
            r"[æŸ±æ¢æ¿å¢™åŸºæ¥¼]\d+",      # æŸ±1, æ¢1ç­‰
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                labels.append({
                    "label": match.group(),
                    "type": self._classify_component_type(match.group()),
                    "position": match.span()
                })
        
        return labels
    
    def _extract_dimensions(self, text: str) -> List[Dict[str, Any]]:
        """æå–å°ºå¯¸ä¿¡æ¯"""
        import re
        dimensions = []
        
        for pattern in self.dimension_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                dimensions.append({
                    "dimension": match.group(),
                    "type": self._classify_dimension_type(match.group()),
                    "position": match.span()
                })
        
        return dimensions
    
    def _extract_annotations(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ³¨é‡Šä¿¡æ¯"""
        # ç®€åŒ–å®ç°ï¼Œæå–å¸¸è§æ³¨é‡Š
        annotations = []
        
        # æŸ¥æ‰¾ææ–™ä¿¡æ¯
        material_keywords = ["C25", "C30", "C35", "HRB400", "HPB300"]
        for keyword in material_keywords:
            if keyword in text:
                annotations.append({
                    "type": "material",
                    "content": keyword,
                    "category": "concrete" if keyword.startswith("C") else "steel"
                })
        
        return annotations
    
    def _classify_component_type(self, label: str) -> str:
        """åˆ†ç±»æ„ä»¶ç±»å‹"""
        label_upper = label.upper()
        if label_upper.startswith('C') or 'æŸ±' in label:
            return "column"
        elif label_upper.startswith('L') or 'æ¢' in label:
            return "beam"
        elif label_upper.startswith('B') or 'æ¿' in label:
            return "slab"
        elif label_upper.startswith('W') or 'å¢™' in label:
            return "wall"
        elif label_upper.startswith('F') or 'åŸº' in label:
            return "foundation"
        elif label_upper.startswith('S') or 'æ¥¼' in label:
            return "stair"
        else:
            return "unknown"
    
    def _classify_dimension_type(self, dimension: str) -> str:
        """åˆ†ç±»å°ºå¯¸ç±»å‹"""
        if 'x' in dimension or 'Ã—' in dimension:
            return "cross_section"
        elif 'Ï†' in dimension or 'Î¦' in dimension:
            return "diameter"
        elif any(unit in dimension for unit in ['mm', 'cm', 'm']):
            return "length"
        else:
            return "unknown"

class GPTSummarizer:
    """GPTæ±‡æ€»å™¨"""
    
    def __init__(self):
        self.api_client = None  # å®é™…ä½¿ç”¨æ—¶éœ€è¦åˆå§‹åŒ–APIå®¢æˆ·ç«¯
    
    async def generate_summary(self, cleaned_data: CleanedOCRData) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½æ±‡æ€»"""
        try:
            # æ„é€ æ±‡æ€»æç¤ºè¯
            prompt = self._build_summary_prompt(cleaned_data)
            
            # æ¨¡æ‹ŸGPTå“åº”ï¼ˆå®é™…ä½¿ç”¨æ—¶è°ƒç”¨APIï¼‰
            summary = {
                "drawing_info": {
                    "type": "ç»“æ„å¹³é¢å›¾",
                    "scale": "1:100",
                    "floor": "æ ‡å‡†å±‚"
                },
                "component_summary": {
                    "total_components": len(cleaned_data.component_labels),
                    "component_types": self._count_component_types(cleaned_data.component_labels),
                    "key_dimensions": self._extract_key_dimensions(cleaned_data.dimensions)
                },
                "quality_assessment": {
                    "text_clarity": "è‰¯å¥½",
                    "completeness": "å®Œæ•´",
                    "confidence": cleaned_data.confidence_score
                },
                "processing_notes": [
                    "OCRè¯†åˆ«è´¨é‡è‰¯å¥½",
                    "æ„ä»¶æ ‡ç­¾æ¸…æ™°",
                    "å°ºå¯¸ä¿¡æ¯å®Œæ•´"
                ]
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ GPTæ±‡æ€»å¤±è´¥: {e}")
            return {
                "error": str(e),
                "fallback_summary": "è‡ªåŠ¨æ±‡æ€»å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ç»Ÿè®¡ä¿¡æ¯",
                "basic_stats": {
                    "text_blocks": len(cleaned_data.text_blocks),
                    "component_labels": len(cleaned_data.component_labels),
                    "dimensions": len(cleaned_data.dimensions)
                }
            }
    
    def _build_summary_prompt(self, cleaned_data: CleanedOCRData) -> str:
        """æ„å»ºæ±‡æ€»æç¤ºè¯"""
        return f"""
        è¯·åˆ†æä»¥ä¸‹å»ºç­‘å›¾çº¸OCRè¯†åˆ«ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–æ±‡æ€»ï¼š
        
        æ„ä»¶æ ‡ç­¾: {cleaned_data.component_labels}
        å°ºå¯¸ä¿¡æ¯: {cleaned_data.dimensions}
        æ³¨é‡Šä¿¡æ¯: {cleaned_data.annotations}
        
        è¯·æä¾›ï¼š
        1. å›¾çº¸åŸºæœ¬ä¿¡æ¯ï¼ˆç±»å‹ã€æ¯”ä¾‹ã€æ¥¼å±‚ç­‰ï¼‰
        2. æ„ä»¶ç»Ÿè®¡æ±‡æ€»
        3. å…³é”®å°ºå¯¸ä¿¡æ¯
        4. è´¨é‡è¯„ä¼°
        """
    
    def _count_component_types(self, labels: List[Dict]) -> Dict[str, int]:
        """ç»Ÿè®¡æ„ä»¶ç±»å‹"""
        counts = {}
        for label in labels:
            comp_type = label.get("type", "unknown")
            counts[comp_type] = counts.get(comp_type, 0) + 1
        return counts
    
    def _extract_key_dimensions(self, dimensions: List[Dict]) -> List[str]:
        """æå–å…³é”®å°ºå¯¸"""
        return [dim["dimension"] for dim in dimensions[:10]]  # å–å‰10ä¸ª

class UnifiedOCRPipeline:
    """ç»Ÿä¸€OCRå¤„ç†ç®¡é“"""
    
    def __init__(self, ocr_engine: str = "paddleocr"):
        self.ocr_engine_type = ocr_engine
        self.component_keywords = [
            "æŸ±", "æ¢", "æ¿", "å¢™", "åŸºç¡€", "æ¥¼æ¢¯", "é—¨", "çª—",
            "C", "L", "B", "W", "F", "S", "D", "Win"
        ]
    
    async def process_slices(self, slice_infos: List[Dict], task_id: str) -> StandardizedOutput:
        """ç»Ÿä¸€å¤„ç†åˆ‡ç‰‡åºåˆ—"""
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹ç»Ÿä¸€OCRå¤„ç†: {len(slice_infos)} ä¸ªåˆ‡ç‰‡")
            
            # Step 2: å¹¶è¡ŒOCRè¯†åˆ«
            ocr_results = []
            for slice_info in slice_infos:
                result = self._recognize_text(slice_info["slice_path"], slice_info)
                ocr_results.append(result)
            
            logger.info(f"ğŸ“ OCRè¯†åˆ«å®Œæˆ: {len(ocr_results)} ä¸ªç»“æœ")
            
            # Step 2.5: æ•°æ®æ¸…æ´—
            cleaned_data = self._clean_ocr_results(ocr_results)
            logger.info(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ: {len(cleaned_data.component_labels)} ä¸ªæ„ä»¶æ ‡ç­¾")
            
            # Step 3: GPTæ±‡æ€»
            summary = await self._generate_summary(cleaned_data)
            logger.info(f"ğŸ“Š GPTæ±‡æ€»å®Œæˆ")
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality_metrics = self._calculate_quality_metrics(ocr_results, cleaned_data)
            
            # ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡º
            output = StandardizedOutput(
                task_id=task_id,
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                stage="unified_ocr_complete",
                slice_results=ocr_results,
                cleaned_data=cleaned_data,
                summary=summary,
                quality_metrics=quality_metrics,
                processing_metadata={
                    "total_processing_time": time.time() - start_time,
                    "slice_count": len(slice_infos),
                    "ocr_engine": self.ocr_engine_type,
                    "pipeline_version": "1.0"
                }
            )
            
            logger.info(f"âœ… ç»Ÿä¸€OCRå¤„ç†å®Œæˆ: ç”¨æ—¶ {time.time() - start_time:.2f}s")
            return output
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€OCRå¤„ç†å¤±è´¥: {e}")
            return StandardizedOutput(
                task_id=task_id,
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                stage="unified_ocr_failed",
                slice_results=[],
                cleaned_data=None,
                summary=None,
                quality_metrics={"error": 1.0},
                processing_metadata={"error": str(e)}
            )
    
    def _recognize_text(self, image_path: str, slice_info: Dict) -> OCRResult:
        """ç»Ÿä¸€çš„æ–‡æœ¬è¯†åˆ«æ¥å£"""
        start_time = time.time()
        
        try:
            # æ¨¡æ‹ŸOCRç»“æœï¼ˆå®é™…ä½¿ç”¨æ—¶è°ƒç”¨çœŸå®OCRå¼•æ“ï¼‰
            raw_text = f"æ¨¡æ‹ŸOCRç»“æœ - {slice_info.get('filename', 'unknown')}"
            confidence = 0.85
            
            processing_time = time.time() - start_time
            
            return OCRResult(
                slice_id=slice_info.get("slice_id", "unknown"),
                filename=slice_info.get("filename", "unknown"),
                coordinates={
                    "x": slice_info.get("x_offset", 0),
                    "y": slice_info.get("y_offset", 0),
                    "width": slice_info.get("width", 0),
                    "height": slice_info.get("height", 0)
                },
                raw_text=raw_text,
                confidence=confidence,
                processing_time=processing_time,
                ocr_engine=self.ocr_engine_type
            )
            
        except Exception as e:
            logger.error(f"âŒ OCRè¯†åˆ«å¤±è´¥ {image_path}: {e}")
            return OCRResult(
                slice_id=slice_info.get("slice_id", "unknown"),
                filename=slice_info.get("filename", "unknown"),
                coordinates={},
                raw_text="",
                confidence=0.0,
                processing_time=time.time() - start_time,
                ocr_engine=self.ocr_engine_type
            )
    
    def _clean_ocr_results(self, ocr_results: List[OCRResult]) -> CleanedOCRData:
        """æ¸…æ´—OCRè¯†åˆ«ç»“æœ"""
        try:
            all_text = " ".join([result.raw_text for result in ocr_results])
            
            text_blocks = self._extract_text_blocks(ocr_results)
            component_labels = self._extract_component_labels(all_text)
            dimensions = self._extract_dimensions(all_text)
            annotations = self._extract_annotations(all_text)
            
            avg_confidence = sum([r.confidence for r in ocr_results]) / len(ocr_results) if ocr_results else 0.0
            
            metadata = {
                "total_slices": len(ocr_results),
                "total_text_length": len(all_text),
                "processing_time": sum([r.processing_time for r in ocr_results]),
                "ocr_engines": list(set([r.ocr_engine for r in ocr_results]))
            }
            
            return CleanedOCRData(
                text_blocks=text_blocks,
                component_labels=component_labels,
                dimensions=dimensions,
                annotations=annotations,
                metadata=metadata,
                confidence_score=avg_confidence,
                processing_stage="ocr_cleaning"
            )
            
        except Exception as e:
            logger.error(f"âŒ OCRæ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return CleanedOCRData(
                text_blocks=[],
                component_labels=[],
                dimensions=[],
                annotations=[],
                metadata={"error": str(e)},
                confidence_score=0.0,
                processing_stage="ocr_cleaning_failed"
            )
    
    def _extract_text_blocks(self, ocr_results: List[OCRResult]) -> List[Dict[str, Any]]:
        """æå–æ–‡æœ¬å—"""
        blocks = []
        for result in ocr_results:
            if result.raw_text.strip():
                blocks.append({
                    "slice_id": result.slice_id,
                    "text": result.raw_text,
                    "coordinates": result.coordinates,
                    "confidence": result.confidence
                })
        return blocks
    
    def _extract_component_labels(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ„ä»¶æ ‡ç­¾"""
        import re
        labels = []
        
        patterns = [
            r"[CLBWFSD]\d+[a-zA-Z]?",
            r"[æŸ±æ¢æ¿å¢™åŸºæ¥¼]\d+",
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                labels.append({
                    "label": match.group(),
                    "type": self._classify_component_type(match.group()),
                    "position": match.span()
                })
        
        return labels
    
    def _extract_dimensions(self, text: str) -> List[Dict[str, Any]]:
        """æå–å°ºå¯¸ä¿¡æ¯"""
        import re
        dimensions = []
        
        patterns = [
            r"\d+[xÃ—]\d+",
            r"\d+mm", r"\d+cm", r"\d+m",
            r"Ï†\d+", r"Î¦\d+"
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                dimensions.append({
                    "dimension": match.group(),
                    "type": self._classify_dimension_type(match.group()),
                    "position": match.span()
                })
        
        return dimensions
    
    def _extract_annotations(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ³¨é‡Šä¿¡æ¯"""
        annotations = []
        
        material_keywords = ["C25", "C30", "C35", "HRB400", "HPB300"]
        for keyword in material_keywords:
            if keyword in text:
                annotations.append({
                    "type": "material",
                    "content": keyword,
                    "category": "concrete" if keyword.startswith("C") else "steel"
                })
        
        return annotations
    
    def _classify_component_type(self, label: str) -> str:
        """åˆ†ç±»æ„ä»¶ç±»å‹"""
        label_upper = label.upper()
        if label_upper.startswith('C') or 'æŸ±' in label:
            return "column"
        elif label_upper.startswith('L') or 'æ¢' in label:
            return "beam"
        elif label_upper.startswith('B') or 'æ¿' in label:
            return "slab"
        elif label_upper.startswith('W') or 'å¢™' in label:
            return "wall"
        elif label_upper.startswith('F') or 'åŸº' in label:
            return "foundation"
        elif label_upper.startswith('S') or 'æ¥¼' in label:
            return "stair"
        else:
            return "unknown"
    
    def _classify_dimension_type(self, dimension: str) -> str:
        """åˆ†ç±»å°ºå¯¸ç±»å‹"""
        if 'x' in dimension or 'Ã—' in dimension:
            return "cross_section"
        elif 'Ï†' in dimension or 'Î¦' in dimension:
            return "diameter"
        elif any(unit in dimension for unit in ['mm', 'cm', 'm']):
            return "length"
        else:
            return "unknown"
    
    async def _generate_summary(self, cleaned_data: CleanedOCRData) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½æ±‡æ€»"""
        try:
            summary = {
                "drawing_info": {
                    "type": "ç»“æ„å¹³é¢å›¾",
                    "scale": "1:100",
                    "floor": "æ ‡å‡†å±‚"
                },
                "component_summary": {
                    "total_components": len(cleaned_data.component_labels),
                    "component_types": self._count_component_types(cleaned_data.component_labels),
                    "key_dimensions": self._extract_key_dimensions(cleaned_data.dimensions)
                },
                "quality_assessment": {
                    "text_clarity": "è‰¯å¥½",
                    "completeness": "å®Œæ•´",
                    "confidence": cleaned_data.confidence_score
                },
                "processing_notes": [
                    "OCRè¯†åˆ«è´¨é‡è‰¯å¥½",
                    "æ„ä»¶æ ‡ç­¾æ¸…æ™°",
                    "å°ºå¯¸ä¿¡æ¯å®Œæ•´"
                ]
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ GPTæ±‡æ€»å¤±è´¥: {e}")
            return {
                "error": str(e),
                "fallback_summary": "è‡ªåŠ¨æ±‡æ€»å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ç»Ÿè®¡ä¿¡æ¯",
                "basic_stats": {
                    "text_blocks": len(cleaned_data.text_blocks),
                    "component_labels": len(cleaned_data.component_labels),
                    "dimensions": len(cleaned_data.dimensions)
                }
            }
    
    def _count_component_types(self, labels: List[Dict]) -> Dict[str, int]:
        """ç»Ÿè®¡æ„ä»¶ç±»å‹"""
        counts = {}
        for label in labels:
            comp_type = label.get("type", "unknown")
            counts[comp_type] = counts.get(comp_type, 0) + 1
        return counts
    
    def _extract_key_dimensions(self, dimensions: List[Dict]) -> List[str]:
        """æå–å…³é”®å°ºå¯¸"""
        return [dim["dimension"] for dim in dimensions[:10]]
    
    def _calculate_quality_metrics(self, ocr_results: List[OCRResult], 
                                 cleaned_data: CleanedOCRData) -> Dict[str, float]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        if not ocr_results:
            return {"overall_quality": 0.0}
        
        avg_confidence = sum([r.confidence for r in ocr_results]) / len(ocr_results) if ocr_results else 0.0
        text_coverage = len([r for r in ocr_results if r.raw_text.strip()]) / len(ocr_results) if ocr_results else 0.0
        component_density = len(cleaned_data.component_labels) / len(ocr_results) if ocr_results else 0.0
        
        overall_quality = (avg_confidence * 0.4 + text_coverage * 0.3 + 
                          min(component_density, 1.0) * 0.3)
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        avg_processing_time = sum([r.processing_time for r in ocr_results]) / len(ocr_results) if ocr_results else 1.0
        processing_efficiency = 1.0 / max(avg_processing_time, 0.001)  # é¿å…é™¤é›¶
        
        return {
            "overall_quality": overall_quality,
            "avg_confidence": avg_confidence,
            "text_coverage": text_coverage,
            "component_density": component_density,
            "processing_efficiency": processing_efficiency
        }
    
    def save_standardized_output(self, output: StandardizedOutput, output_path: str):
        """ä¿å­˜æ ‡å‡†åŒ–è¾“å‡º"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            output_dict = asdict(output)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æ ‡å‡†åŒ–è¾“å‡ºå·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ ‡å‡†åŒ–è¾“å‡ºå¤±è´¥: {e}") 