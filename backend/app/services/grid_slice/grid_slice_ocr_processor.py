#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘æ ¼åˆ‡ç‰‡OCRå¤„ç†å™¨
è´Ÿè´£OCRç»“æœçš„å¤„ç†ã€ç¼“å­˜å’Œå¤ç”¨é€»è¾‘
"""

import os
import json
import logging
import tempfile
import base64
import math
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from .grid_slice_analyzer_core import OCRTextItem, EnhancedSliceInfo
from app.utils.analysis_optimizations import ocr_cache_manager

logger = logging.getLogger(__name__)

class GridSliceOCRProcessor:
    """ç½‘æ ¼åˆ‡ç‰‡OCRå¤„ç†å™¨"""
    
    def __init__(self, core_analyzer):
        """åˆå§‹åŒ–OCRå¤„ç†å™¨"""
        self.core_analyzer = core_analyzer
        self.ocr_cache = ocr_cache_manager
        
        # åˆå§‹åŒ–OCRå¼•æ“
        try:
            from app.services.ocr.paddle_ocr import PaddleOCRService
            self.ocr_engine = PaddleOCRService()
        except Exception as e:
            logger.warning(f"âš ï¸ OCRå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.ocr_engine = None

    def can_reuse_shared_slices(self, shared_slice_results: Dict[str, Any], image_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤ç”¨å…±äº«åˆ‡ç‰‡ç»“æœ"""
        # åŸºç¡€éªŒè¯é€»è¾‘
        required_keys = ['slice_infos', 'original_width', 'original_height']
        for key in required_keys:
            if key not in shared_slice_results:
                logger.error(f"âŒ å…±äº«åˆ‡ç‰‡ç»“æœç¼ºå°‘å¿…éœ€å­—æ®µ: {key}")
                return False
        
        slice_infos = shared_slice_results.get('slice_infos', [])
        if not slice_infos:
            logger.error("âŒ å…±äº«åˆ‡ç‰‡ç»“æœä¸ºç©º")
            return False
        
        # éªŒè¯åˆ‡ç‰‡æ•°æ®å®Œæ•´æ€§
        for i, slice_info in enumerate(slice_infos):
            if not hasattr(slice_info, 'base64_data') or not slice_info.base64_data:
                logger.error(f"âŒ åˆ‡ç‰‡ {i} ç¼ºå°‘base64æ•°æ®")
                return False
        
        logger.info(f"âœ… å…±äº«åˆ‡ç‰‡éªŒè¯é€šè¿‡ï¼Œåˆ‡ç‰‡æ•°é‡: {len(slice_infos)}")
        return True

    def reuse_shared_slices(self, shared_slice_results: Dict[str, Any], image_path: str, drawing_info: Dict[str, Any]) -> Dict[str, Any]:
        """å¤ç”¨å…±äº«åˆ‡ç‰‡ç»“æœ"""
        try:
            slice_infos = shared_slice_results.get('slice_infos', [])
            original_width = shared_slice_results.get('original_width', 0)
            original_height = shared_slice_results.get('original_height', 0)
            
            logger.info(f"ğŸ“ å¼€å§‹å¤ç”¨å…±äº«åˆ‡ç‰‡: {len(slice_infos)}ä¸ªåˆ‡ç‰‡")
            
            # æ¸…ç©ºç°æœ‰åˆ‡ç‰‡
            self.core_analyzer.enhanced_slices = []
            
            # é˜²é‡å¤åæ ‡é›†åˆ
            seen_coordinates = set()
            
            for i, slice_data in enumerate(slice_infos):
                # åˆ›å»ºä¸´æ—¶åˆ‡ç‰‡æ–‡ä»¶
                try:
                    slice_image_data = base64.b64decode(slice_data.base64_data)
                    temp_slice_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
                    temp_slice_file.write(slice_image_data)
                    temp_slice_file.close()
                    
                    # æ™ºèƒ½è®¡ç®—è¡Œåˆ—ï¼Œé¿å…é‡å¤æ ‡è¯†
                    if original_width > 0 and original_height > 0:
                        # ä¼°ç®—ç½‘æ ¼å¤§å°ï¼ˆæ ¹æ®åˆ‡ç‰‡æ•°é‡å’Œå›¾ç‰‡å°ºå¯¸ï¼‰
                        total_slices = len(slice_infos)
                        estimated_rows = math.ceil(math.sqrt(total_slices * original_height / original_width))
                        estimated_cols = math.ceil(total_slices / estimated_rows)
                        
                        # åŸºäºä½ç½®ç²¾ç¡®è®¡ç®—è¡Œåˆ—
                        if estimated_rows > 0 and estimated_cols > 0:
                            row_height = max(1, original_height // estimated_rows)
                            col_width = max(1, original_width // estimated_cols)
                            row = min(slice_data.y // row_height, estimated_rows - 1)
                            col = min(slice_data.x // col_width, estimated_cols - 1)
                        else:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºç´¢å¼•è®¡ç®—
                            estimated_cols = max(1, math.ceil(math.sqrt(len(slice_infos))))
                            row = i // estimated_cols
                            col = i % estimated_cols
                    else:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºç´¢å¼•è®¡ç®—
                        estimated_cols = max(1, math.ceil(math.sqrt(len(slice_infos))))
                        row = i // estimated_cols
                        col = i % estimated_cols
                    
                    # é˜²é‡å¤æ£€æŸ¥
                    coordinate_key = f"{row}_{col}"
                    if coordinate_key in seen_coordinates:
                        # è°ƒæ•´åæ ‡é¿å…é‡å¤
                        adjustment = 1
                        while f"{row + adjustment}_{col}" in seen_coordinates:
                            adjustment += 1
                        row = row + adjustment
                        coordinate_key = f"{row}_{col}"
                    
                    seen_coordinates.add(coordinate_key)
                    
                    enhanced_slice_info = EnhancedSliceInfo(
                        filename=f"reused_slice_{row}_{col}.png",
                        row=row,
                        col=col,
                        x_offset=slice_data.x,
                        y_offset=slice_data.y,
                        source_page=drawing_info.get("page_number", 1),
                        width=slice_data.width,
                        height=slice_data.height,
                        slice_path=temp_slice_file.name,
                        ocr_results=[],  # ç¨åä¼šå¡«å……
                        enhanced_prompt=""
                    )
                    
                    self.core_analyzer.enhanced_slices.append(enhanced_slice_info)
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†åˆ‡ç‰‡ {i} å¤±è´¥: {e}")
                    continue
            
            # ä¿®å¤é‡å¤çš„åˆ‡ç‰‡ID
            self._fix_duplicate_slice_ids()
            
            logger.info(f"âœ… å…±äº«åˆ‡ç‰‡å¤ç”¨å®Œæˆ: {len(self.core_analyzer.enhanced_slices)}ä¸ªæœ‰æ•ˆåˆ‡ç‰‡")
            
            return {
                "success": True,
                "slice_count": len(self.core_analyzer.enhanced_slices),
                "original_image_info": {
                    "width": original_width,
                    "height": original_height,
                    "path": image_path
                },
                "slice_coordinate_map": self._build_slice_coordinate_map()
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤ç”¨å…±äº«åˆ‡ç‰‡å¤±è´¥: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def load_shared_ocr_results(self, shared_slice_results: Dict[str, Any], image_path: str) -> Dict[str, Any]:
        """ä»å…±äº«åˆ‡ç‰‡ç»“æœä¸­åŠ è½½OCRæ•°æ®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„OCRç»“æœ
            if hasattr(shared_slice_results, 'ocr_cache_key') and shared_slice_results.ocr_cache_key:
                cached_results = self.ocr_cache.get_cached_ocr_results(shared_slice_results.ocr_cache_key)
                if cached_results:
                    logger.info("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„OCRç»“æœ")
                    return self._apply_cached_ocr_results(cached_results)
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™æ‰§è¡ŒOCRå¤„ç†
            logger.info("ğŸ” å¼€å§‹OCRå¤„ç†...")
            return self._extract_ocr_from_slices_optimized()
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å…±äº«OCRç»“æœå¤±è´¥: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _extract_ocr_from_slices_optimized(self) -> Dict[str, Any]:
        """ä»åˆ‡ç‰‡ä¸­æå–OCRç»“æœï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            ocr_texts = []
            processed_count = 0
            cache_hit_count = 0
            
            logger.info(f"ğŸ” å¼€å§‹OCRå¤„ç†ï¼Œåˆ‡ç‰‡æ•°é‡: {len(self.core_analyzer.enhanced_slices)}")
            
            for slice_info in self.core_analyzer.enhanced_slices:
                # æ£€æŸ¥OCRç¼“å­˜
                if self._can_reuse_slice_ocr(slice_info):
                    cached_results = self._load_cached_ocr_results(slice_info)
                    slice_info.ocr_results = cached_results
                    cache_hit_count += 1
                else:
                    # æ‰§è¡ŒOCRè¯†åˆ«
                    slice_ocr_results = self._perform_slice_ocr(slice_info)
                    slice_info.ocr_results = slice_ocr_results
                    # ç¼“å­˜ç»“æœ
                    self._cache_slice_ocr_results(slice_info)
                
                # æ”¶é›†OCRæ–‡æœ¬
                if slice_info.ocr_results:
                    for ocr_item in slice_info.ocr_results:
                        ocr_texts.append(ocr_item.text)
                
                processed_count += 1
                
                if processed_count % 10 == 0:
                    logger.info(f"ğŸ“Š OCRè¿›åº¦: {processed_count}/{len(self.core_analyzer.enhanced_slices)}, ç¼“å­˜å‘½ä¸­: {cache_hit_count}")
            
            logger.info(f"âœ… OCRå¤„ç†å®Œæˆ: å¤„ç† {processed_count} ä¸ªåˆ‡ç‰‡, ç¼“å­˜å‘½ä¸­ {cache_hit_count} ä¸ª")
            
            # æå–å…¨å›¾OCRæ¦‚è§ˆ
            overview_result = self._extract_global_ocr_overview_optimized(ocr_texts)
            
            return {
                "success": True,
                "ocr_texts": ocr_texts,
                "slice_count": processed_count,
                "cache_hit_count": cache_hit_count,
                "global_overview": overview_result.get("global_overview", {})
            }
            
        except Exception as e:
            logger.error(f"âŒ OCRæå–å¤±è´¥: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _can_reuse_slice_ocr(self, slice_info: EnhancedSliceInfo) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤ç”¨åˆ‡ç‰‡OCRç»“æœ"""
        try:
            # åŸºäºåˆ‡ç‰‡æ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆç¼“å­˜é”®
            if not os.path.exists(slice_info.slice_path):
                return False
            
            file_stat = os.stat(slice_info.slice_path)
            cache_key = f"slice_ocr_{slice_info.filename}_{file_stat.st_size}_{int(file_stat.st_mtime)}"
            
            return self.ocr_cache.has_cached_results(cache_key)
        except Exception:
            return False

    def _load_cached_ocr_results(self, slice_info: EnhancedSliceInfo) -> List[OCRTextItem]:
        """åŠ è½½ç¼“å­˜çš„OCRç»“æœ"""
        try:
            file_stat = os.stat(slice_info.slice_path)
            cache_key = f"slice_ocr_{slice_info.filename}_{file_stat.st_size}_{int(file_stat.st_mtime)}"
            
            cached_data = self.ocr_cache.get_cached_ocr_results(cache_key)
            if cached_data:
                return self._convert_to_ocr_text_items(cached_data)
            return []
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜OCRç»“æœå¤±è´¥: {e}")
            return []

    def _perform_slice_ocr(self, slice_info: EnhancedSliceInfo) -> List[OCRTextItem]:
        """å¯¹å•ä¸ªåˆ‡ç‰‡æ‰§è¡ŒOCRè¯†åˆ«"""
        try:
            if not self.ocr_engine:
                logger.warning("âš ï¸ OCRå¼•æ“ä¸å¯ç”¨")
                return []
            
            # æ‰§è¡ŒOCRè¯†åˆ«
            ocr_results = self.ocr_engine.extract_text_from_image(slice_info.slice_path)
            
            # è½¬æ¢ä¸ºOCRTextItemæ ¼å¼
            ocr_items = []
            for result in ocr_results:
                try:
                    # è§£æOCRç»“æœæ ¼å¼
                    if isinstance(result, dict):
                        text = result.get('text', '')
                        confidence = result.get('confidence', 0.0)
                        position = result.get('position', [])
                    else:
                        # å¤„ç†å…¶ä»–æ ¼å¼
                        text = str(result)
                        confidence = 0.8
                        position = []
                    
                    if text.strip():
                        ocr_item = OCRTextItem(
                            text=text.strip(),
                            position=position,
                            confidence=confidence,
                            category="unknown",
                            bbox=self._calculate_bbox_from_position(position) if position else None
                        )
                        ocr_items.append(ocr_item)
                        
                except Exception as item_error:
                    logger.warning(f"âš ï¸ å¤„ç†OCRç»“æœé¡¹å¤±è´¥: {item_error}")
                    continue
            
            return ocr_items
            
        except Exception as e:
            logger.error(f"âŒ åˆ‡ç‰‡OCRè¯†åˆ«å¤±è´¥: {e}")
            return []

    def _cache_slice_ocr_results(self, slice_info: EnhancedSliceInfo):
        """ç¼“å­˜åˆ‡ç‰‡OCRç»“æœ"""
        try:
            if not slice_info.ocr_results:
                return
            
            file_stat = os.stat(slice_info.slice_path)
            cache_key = f"slice_ocr_{slice_info.filename}_{file_stat.st_size}_{int(file_stat.st_mtime)}"
            
            # å°†OCRTextItemè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_results = []
            for ocr_item in slice_info.ocr_results:
                serializable_results.append({
                    'text': ocr_item.text,
                    'position': ocr_item.position,
                    'confidence': ocr_item.confidence,
                    'category': ocr_item.category,
                    'bbox': ocr_item.bbox
                })
            
            self.ocr_cache.cache_ocr_results(cache_key, serializable_results)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜OCRç»“æœå¤±è´¥: {e}")

    def _convert_to_ocr_text_items(self, ocr_data: List[Dict]) -> List[OCRTextItem]:
        """å°†ç¼“å­˜æ•°æ®è½¬æ¢ä¸ºOCRTextItemå¯¹è±¡"""
        ocr_items = []
        for item_data in ocr_data:
            try:
                ocr_item = OCRTextItem(
                    text=item_data.get('text', ''),
                    position=item_data.get('position', []),
                    confidence=item_data.get('confidence', 0.0),
                    category=item_data.get('category', 'unknown'),
                    bbox=item_data.get('bbox')
                )
                ocr_items.append(ocr_item)
            except Exception as e:
                logger.warning(f"âš ï¸ è½¬æ¢OCRæ•°æ®é¡¹å¤±è´¥: {e}")
                continue
        return ocr_items

    def _calculate_bbox_from_position(self, position: List[List[int]]) -> Dict[str, int]:
        """ä»ä½ç½®åæ ‡è®¡ç®—è¾¹ç•Œæ¡†"""
        try:
            if not position or len(position) < 4:
                return None
            
            x_coords = [point[0] for point in position]
            y_coords = [point[1] for point in position]
            
            return {
                'x': min(x_coords),
                'y': min(y_coords),
                'width': max(x_coords) - min(x_coords),
                'height': max(y_coords) - min(y_coords)
            }
        except Exception:
            return None

    def _extract_global_ocr_overview_optimized(self, ocr_texts: List[str]) -> Dict[str, Any]:
        """æå–å…¨å›¾OCRæ¦‚è§ˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # å®ç°å…¨å›¾OCRæ¦‚è§ˆæå–é€»è¾‘
            return {
                "global_overview": {
                    "total_texts": len(ocr_texts),
                    "analysis_method": "slice_based_ocr"
                }
            }
        except Exception as e:
            logger.error(f"âŒ å…¨å›¾OCRæ¦‚è§ˆæå–å¤±è´¥: {e}")
            return {"global_overview": {}}

    def _apply_cached_ocr_results(self, cached_results: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨ç¼“å­˜çš„OCRç»“æœ"""
        try:
            # åº”ç”¨ç¼“å­˜çš„OCRç»“æœåˆ°åˆ‡ç‰‡
            for slice_info in self.core_analyzer.enhanced_slices:
                slice_key = f"{slice_info.row}_{slice_info.col}"
                if slice_key in cached_results:
                    slice_info.ocr_results = self._convert_to_ocr_text_items(cached_results[slice_key])
            
            return {"success": True}
        except Exception as e:
            logger.error(f"âŒ åº”ç”¨ç¼“å­˜OCRç»“æœå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _build_slice_coordinate_map(self) -> Dict[str, Any]:
        """æ„å»ºåˆ‡ç‰‡åæ ‡æ˜ å°„"""
        coordinate_map = {}
        for slice_info in self.core_analyzer.enhanced_slices:
            slice_key = f"{slice_info.row}_{slice_info.col}"
            coordinate_map[slice_key] = {
                'x_offset': slice_info.x_offset,
                'y_offset': slice_info.y_offset,
                'width': slice_info.width,
                'height': slice_info.height
            }
        return coordinate_map

    def _fix_duplicate_slice_ids(self):
        """ä¿®å¤é‡å¤çš„åˆ‡ç‰‡ID"""
        seen_ids = set()
        for i, slice_info in enumerate(self.core_analyzer.enhanced_slices):
            original_id = f"{slice_info.row}_{slice_info.col}"
            if original_id in seen_ids:
                # è°ƒæ•´é‡å¤çš„ID
                adjustment = 1
                new_id = f"{slice_info.row + adjustment}_{slice_info.col}"
                while new_id in seen_ids:
                    adjustment += 1
                    new_id = f"{slice_info.row + adjustment}_{slice_info.col}"
                
                slice_info.row = slice_info.row + adjustment
                slice_info.filename = f"reused_slice_{slice_info.row}_{slice_info.col}.png"
                seen_ids.add(new_id)
            else:
                seen_ids.add(original_id)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        # æ¸…ç†ä¸´æ—¶OCRæ–‡ä»¶ç­‰
        pass 