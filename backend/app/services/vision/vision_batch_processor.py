#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Visionæ‰¹æ¬¡å¤„ç†å™¨ç»„ä»¶
è´Ÿè´£å›¾åƒåˆ‡ç‰‡çš„æ‰¹æ¬¡å¤„ç†å’Œä»»åŠ¡ç®¡ç†
"""
import logging
import asyncio
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class VisionBatchProcessor:
    """Visionæ‰¹æ¬¡å¤„ç†å™¨"""
    
    def __init__(self, ai_analyzer):
        """åˆå§‹åŒ–æ‰¹æ¬¡å¤„ç†å™¨"""
        self.ai_analyzer = ai_analyzer
        self.batch_size = 10  # é»˜è®¤æ‰¹æ¬¡å¤§å°
        self.max_concurrent = 3  # æœ€å¤§å¹¶å‘æ•°
        
    async def process_slices_in_batches(self, 
                                      slice_data_list: List[Dict[str, Any]], 
                                      drawing_id: int,
                                      batch_id: str = None) -> Dict[str, Any]:
        """
        åˆ†æ‰¹å¤„ç†å›¾åƒåˆ‡ç‰‡
        
        Args:
            slice_data_list: åˆ‡ç‰‡æ•°æ®åˆ—è¡¨
            drawing_id: å›¾çº¸ID
            batch_id: æ‰¹æ¬¡ID
            
        Returns:
            æ‰¹æ¬¡å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹æ¬¡å¤„ç† - æ€»åˆ‡ç‰‡æ•°: {len(slice_data_list)}, æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        
        try:
            # åˆ†å‰²åˆ‡ç‰‡åˆ°æ‰¹æ¬¡
            batches = self._split_into_batches(slice_data_list, self.batch_size)
            logger.info(f"ğŸ“Š å·²åˆ†å‰²ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡")
            
            # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
            all_batch_results = []
            failed_batches = []
            
            for batch_index, batch_slices in enumerate(batches):
                logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_index + 1}/{len(batches)} - åˆ‡ç‰‡æ•°: {len(batch_slices)}")
                
                try:
                    batch_result = await self._process_single_batch(
                        batch_slices, batch_index, drawing_id, batch_id
                    )
                    
                    if batch_result.get("success", False):
                        all_batch_results.append(batch_result)
                        logger.info(f"âœ… æ‰¹æ¬¡ {batch_index + 1} å¤„ç†æˆåŠŸ")
                    else:
                        failed_batches.append({
                            "batch_index": batch_index,
                            "error": batch_result.get("error", "Unknown error"),
                            "slice_count": len(batch_slices)
                        })
                        logger.error(f"âŒ æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å¤±è´¥: {batch_result.get('error')}")
                        
                except Exception as e:
                    failed_batches.append({
                        "batch_index": batch_index,
                        "error": str(e),
                        "slice_count": len(batch_slices)
                    })
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å¼‚å¸¸: {e}")
                
                # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIè¿‡è½½
                if batch_index < len(batches) - 1:
                    await asyncio.sleep(0.5)
            
            # åˆå¹¶æ‰¹æ¬¡ç»“æœ
            merged_result = self._merge_batch_results(all_batch_results, failed_batches)
            
            success_rate = len(all_batch_results) / len(batches) if batches else 0
            logger.info(f"ğŸ“ˆ æ‰¹æ¬¡å¤„ç†å®Œæˆ - æˆåŠŸç‡: {success_rate:.1%} ({len(all_batch_results)}/{len(batches)})")
            
            return merged_result
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å™¨å¼‚å¸¸: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "components": [],
                "batch_summary": {
                    "total_batches": 0,
                    "successful_batches": 0,
                    "failed_batches": 0
                }
            }

    def _split_into_batches(self, data_list: List[Dict], batch_size: int) -> List[List[Dict]]:
        """å°†æ•°æ®åˆ—è¡¨åˆ†å‰²ä¸ºæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(data_list), batch_size):
            batch = data_list[i:i + batch_size]
            batches.append(batch)
        return batches

    async def _process_single_batch(self, 
                                  batch_slices: List[Dict[str, Any]], 
                                  batch_index: int,
                                  drawing_id: int,
                                  batch_id: str = None) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡
        
        Args:
            batch_slices: æ‰¹æ¬¡ä¸­çš„åˆ‡ç‰‡åˆ—è¡¨
            batch_index: æ‰¹æ¬¡ç´¢å¼•
            drawing_id: å›¾çº¸ID
            batch_id: æ‰¹æ¬¡ID
            
        Returns:
            å•ä¸ªæ‰¹æ¬¡çš„å¤„ç†ç»“æœ
        """
        batch_start_time = time.time()
        
        try:
            # å‡†å¤‡å›¾åƒè·¯å¾„åˆ—è¡¨
            image_paths = []
            slice_metadata_list = []
            
            for slice_data in batch_slices:
                image_path = slice_data.get("image_path")
                if image_path and Path(image_path).exists():
                    image_paths.append(image_path)
                    slice_metadata_list.append({
                        "slice_id": slice_data.get("slice_id"),
                        "bounds": slice_data.get("bounds"),
                        "metadata": slice_data.get("metadata", {})
                    })
                else:
                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆåˆ‡ç‰‡: {slice_data.get('slice_id', 'unknown')}")
            
            if not image_paths:
                logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_index} ä¸­æ²¡æœ‰æœ‰æ•ˆå›¾åƒ")
                return {
                    "success": False,
                    "error": "æ‰¹æ¬¡ä¸­æ²¡æœ‰æœ‰æ•ˆå›¾åƒ",
                    "components": [],
                    "batch_info": {
                        "batch_index": batch_index,
                        "slice_count": len(batch_slices),
                        "valid_images": 0
                    }
                }
            
            # è°ƒç”¨AIåˆ†æå™¨è¿›è¡ŒVisionåˆ†æ
            context_data = {
                "drawing_id": drawing_id,
                "batch_id": batch_id,
                "batch_index": batch_index,
                "slice_metadata": slice_metadata_list
            }
            
            analysis_result = self.ai_analyzer.generate_qto_from_local_images(
                image_paths, context_data
            )
            
            # å¤„ç†åˆ†æç»“æœ
            if analysis_result.get("success", False):
                qto_data = analysis_result.get("qto_data", {})
                components = qto_data.get("components", [])
                
                # ä¸ºæ¯ä¸ªæ„ä»¶æ·»åŠ æ‰¹æ¬¡ä¿¡æ¯
                for component in components:
                    component["batch_index"] = batch_index
                    component["source_batch_id"] = batch_id
                
                batch_result = {
                    "success": True,
                    "components": components,
                    "batch_info": {
                        "batch_index": batch_index,
                        "slice_count": len(batch_slices),
                        "valid_images": len(image_paths),
                        "component_count": len(components),
                        "processing_time": time.time() - batch_start_time
                    },
                    "qto_metadata": qto_data.get("analysis_metadata", {})
                }
                
                logger.info(f"âœ… æ‰¹æ¬¡ {batch_index} åˆ†æå®Œæˆ: {len(components)} ä¸ªæ„ä»¶")
                return batch_result
                
            else:
                error_msg = analysis_result.get("error", "AIåˆ†æå¤±è´¥")
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_index} AIåˆ†æå¤±è´¥: {error_msg}")
                return {
                    "success": False,
                    "error": error_msg,
                    "components": [],
                    "batch_info": {
                        "batch_index": batch_index,
                        "slice_count": len(batch_slices),
                        "valid_images": len(image_paths)
                    }
                }
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_index} å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "components": [],
                "batch_info": {
                    "batch_index": batch_index,
                    "slice_count": len(batch_slices),
                    "processing_time": time.time() - batch_start_time
                }
            }

    def _merge_batch_results(self, 
                           successful_results: List[Dict[str, Any]], 
                           failed_batches: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆå¹¶æ‰¹æ¬¡ç»“æœ
        
        Args:
            successful_results: æˆåŠŸçš„æ‰¹æ¬¡ç»“æœåˆ—è¡¨
            failed_batches: å¤±è´¥çš„æ‰¹æ¬¡ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ€»ä½“ç»“æœ
        """
        try:
            # åˆå¹¶æ‰€æœ‰æˆåŠŸæ‰¹æ¬¡çš„æ„ä»¶
            all_components = []
            total_processing_time = 0
            batch_summaries = []
            
            for result in successful_results:
                components = result.get("components", [])
                all_components.extend(components)
                
                batch_info = result.get("batch_info", {})
                total_processing_time += batch_info.get("processing_time", 0)
                batch_summaries.append(batch_info)
            
            # å»é‡å¤„ç†ï¼ˆåŸºäºæ„ä»¶IDå’Œä½ç½®ï¼‰
            unique_components = self._remove_duplicate_components(all_components)
            
            # æ„å»ºåˆå¹¶ç»“æœ
            merged_result = {
                "success": len(successful_results) > 0,
                "components": unique_components,
                "batch_summary": {
                    "total_batches": len(successful_results) + len(failed_batches),
                    "successful_batches": len(successful_results),
                    "failed_batches": len(failed_batches),
                    "total_components": len(all_components),
                    "unique_components": len(unique_components),
                    "total_processing_time": total_processing_time
                },
                "batch_details": batch_summaries,
                "failed_batch_details": failed_batches
            }
            
            if failed_batches:
                merged_result["warnings"] = [
                    f"æœ‰ {len(failed_batches)} ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥"
                ]
            
            logger.info(f"ğŸ”„ æ‰¹æ¬¡ç»“æœåˆå¹¶å®Œæˆ: {len(unique_components)} ä¸ªå”¯ä¸€æ„ä»¶")
            return merged_result
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ç»“æœåˆå¹¶å¼‚å¸¸: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"æ‰¹æ¬¡ç»“æœåˆå¹¶å¤±è´¥: {str(e)}",
                "components": [],
                "batch_summary": {
                    "total_batches": len(successful_results) + len(failed_batches),
                    "successful_batches": 0,
                    "failed_batches": len(successful_results) + len(failed_batches)
                }
            }

    def _remove_duplicate_components(self, components: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç§»é™¤é‡å¤æ„ä»¶"""
        if not components:
            return []
        
        unique_components = []
        seen_keys = set()
        
        for component in components:
            # ç”Ÿæˆæ„ä»¶çš„å”¯ä¸€æ ‡è¯†
            component_id = component.get("component_id", "")
            location = component.get("location", "")
            dimensions = component.get("dimensions", {})
            
            # åˆ›å»ºå”¯ä¸€é”®
            unique_key = f"{component_id}_{location}_{hash(str(dimensions))}"
            
            if unique_key not in seen_keys:
                seen_keys.add(unique_key)
                unique_components.append(component)
            else:
                logger.debug(f"ğŸ”„ ç§»é™¤é‡å¤æ„ä»¶: {component_id}")
        
        if len(unique_components) < len(components):
            logger.info(f"ğŸ§¹ å»é‡å®Œæˆ: {len(components)} â†’ {len(unique_components)} ä¸ªæ„ä»¶")
        
        return unique_components

    def _safe_convert_to_number(self, value: Any, default: float = 0.0) -> float:
        """å®‰å…¨è½¬æ¢å€¼ä¸ºæ•°å­—"""
        if value is None:
            return default
        
        if isinstance(value, (int, float)):
            return float(value)
        
        if isinstance(value, str):
            try:
                # ç§»é™¤å¯èƒ½çš„å•ä½å’Œç©ºæ ¼
                clean_value = value.strip().replace('mm', '').replace('m', '').replace('kg', '')
                return float(clean_value)
            except ValueError:
                return default
        
        return default 