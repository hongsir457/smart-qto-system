"""
å¢å¼ºç‰ˆPaddleOCRåˆ‡ç‰‡ç»“æœåˆå¹¶å™¨
å®ç°PaddleOCRåˆ‡ç‰‡æ‰«æåˆå¹¶çš„å››å¤§æ ¸å¿ƒç›®æ ‡ï¼š
1. ä¸ä¸¢å†…å®¹ - å…¨å›¾ä¸­æ‰€æœ‰æ–‡å­—ã€å›¾çº¸ä¿¡æ¯å¿…é¡»è¢«ä¿ç•™ï¼Œä¸é—æ¼ä»»ä½•åˆ‡ç‰‡è¾¹ç¼˜çš„æ–‡å­—
2. ä¸é‡å¤å†…å®¹ - å¯¹é‡å åŒºåŸŸå†…åŒä¸€æ–‡å­—ï¼Œåªä¿ç•™ä¸€æ¬¡ï¼Œå»é™¤é‡å¤æ–‡æœ¬
3. æ­£ç¡®æ’åº - æ–‡æœ¬ç»“æœèƒ½ä¿æŒå›¾çº¸åŸæœ‰çš„é˜…è¯»é¡ºåºï¼ˆè¡Œåˆ—æˆ–åŒºåŸŸåˆ†ç»„ï¼‰
4. æ¢å¤å…¨å›¾åæ ‡ - OCRç»“æœä¸­bboxåæ ‡å¿…é¡»ä»åˆ‡ç‰‡å†…åæ ‡è¿˜åŸä¸ºåŸå›¾ç»å¯¹åæ ‡
"""

import json
import time
import math
from typing import Dict, List, Any, Tuple, Optional, Set
from dataclasses import dataclass, asdict
import logging
from collections import defaultdict
import re

logger = logging.getLogger(__name__)

@dataclass
class EnhancedTextRegion:
    """å¢å¼ºç‰ˆæ–‡æœ¬åŒºåŸŸæ•°æ®ç»“æ„"""
    text: str
    bbox: List[int]  # [x1, y1, x2, y2] å…¨å›¾åæ ‡
    confidence: float
    slice_source: Dict[str, Any]
    polygon: Optional[List[List[int]]] = None
    text_type: str = "unknown"  # æ–‡æœ¬ç±»å‹ï¼šcomponent_id, dimension, materialç­‰
    reading_order: int = -1  # é˜…è¯»é¡ºåº
    region_id: str = ""  # å”¯ä¸€åŒºåŸŸID
    is_edge_protected: bool = False  # æ˜¯å¦ä¸ºè¾¹ç¼˜ä¿æŠ¤æ–‡æœ¬

@dataclass
class MergeStatistics:
    """åˆå¹¶ç»Ÿè®¡ä¿¡æ¯"""
    total_input_regions: int
    edge_preserved_regions: int
    duplicate_removed_count: int
    final_regions_count: int
    coordinate_restored_count: int
    processing_time: float
    objectives_achieved: Dict[str, bool]

class EnhancedPaddleOCRMerger:
    """å¢å¼ºç‰ˆPaddleOCRåˆ‡ç‰‡åˆå¹¶å™¨ - å®ç°å››å¤§æ ¸å¿ƒç›®æ ‡"""
    
    def __init__(self, storage_service=None):
        self.storage_service = storage_service
        self.overlap_threshold = 0.3  # é‡å é˜ˆå€¼
        self.similarity_threshold = 0.85  # æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼
        self.edge_proximity_threshold = 20  # è¾¹ç¼˜æ–‡æœ¬ä¿æŠ¤è·ç¦»ï¼ˆåƒç´ ï¼‰
        
    def merge_paddleocr_results_enhanced(self, 
                                       slice_results: List[Dict[str, Any]], 
                                       slice_coordinate_map: Dict[str, Any],
                                       original_image_info: Dict[str, Any],
                                       task_id: str) -> Dict[str, Any]:
        """
        å¢å¼ºç‰ˆPaddleOCRåˆ‡ç‰‡ç»“æœåˆå¹¶ - å®ç°å››å¤§æ ¸å¿ƒç›®æ ‡
        
        Args:
            slice_results: PaddleOCRåˆ‡ç‰‡ç»“æœåˆ—è¡¨
            slice_coordinate_map: åˆ‡ç‰‡åæ ‡æ˜ å°„è¡¨
            original_image_info: åŸå›¾ä¿¡æ¯
            task_id: ä»»åŠ¡ID
            
        Returns:
            å¢å¼ºåˆå¹¶ç»“æœï¼ŒåŒ…å«å››å¤§ç›®æ ‡çš„å®ç°æƒ…å†µ
        """
        logger.info(f"ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆPaddleOCRåˆå¹¶ - å››å¤§æ ¸å¿ƒç›®æ ‡: {len(slice_results)} ä¸ªåˆ‡ç‰‡")
        start_time = time.time()
        
        # ç»Ÿè®¡åŸå§‹æ•°æ®
        total_input_regions = sum(
            len(result.get('text_regions', [])) 
            for result in slice_results 
            if result.get('success', False)
        )
        
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {total_input_regions} ä¸ªæ–‡æœ¬åŒºåŸŸæ¥è‡ª {len(slice_results)} ä¸ªåˆ‡ç‰‡")
        
        # ğŸ¯ ç›®æ ‡1: ä¸ä¸¢å†…å®¹ - å…¨é¢æ”¶é›†æ‰€æœ‰æ–‡æœ¬åŒºåŸŸï¼Œç‰¹åˆ«ä¿æŠ¤è¾¹ç¼˜æ–‡æœ¬
        all_regions = self._objective1_preserve_all_content(
            slice_results, slice_coordinate_map
        )
        logger.info(f"âœ… ç›®æ ‡1å®Œæˆ: æ”¶é›† {len(all_regions)} ä¸ªæ–‡æœ¬åŒºåŸŸï¼ˆå«è¾¹ç¼˜ä¿æŠ¤ï¼‰")
        
        # ğŸ¯ ç›®æ ‡4: æ¢å¤å…¨å›¾åæ ‡ - å°†æ‰€æœ‰åˆ‡ç‰‡åæ ‡è¿˜åŸä¸ºå…¨å›¾åæ ‡
        restored_regions = self._objective4_restore_global_coordinates(
            all_regions, slice_coordinate_map
        )
        logger.info(f"âœ… ç›®æ ‡4å®Œæˆ: åæ ‡è¿˜åŸ {len(restored_regions)} ä¸ªåŒºåŸŸ")
        
        # ğŸ¯ ç›®æ ‡2: ä¸é‡å¤å†…å®¹ - æ™ºèƒ½å»é™¤é‡å åŒºåŸŸçš„é‡å¤æ–‡æœ¬
        deduplicated_regions = self._objective2_eliminate_duplicates(
            restored_regions, original_image_info
        )
        duplicate_count = len(restored_regions) - len(deduplicated_regions)
        logger.info(f"âœ… ç›®æ ‡2å®Œæˆ: å»é‡ç§»é™¤ {duplicate_count} ä¸ªé‡å¤åŒºåŸŸ")
        
        # ğŸ¯ ç›®æ ‡3: æ­£ç¡®æ’åº - æŒ‰å›¾çº¸é˜…è¯»é¡ºåºé‡æ–°æ’åˆ—
        sorted_regions = self._objective3_correct_reading_order(
            deduplicated_regions, original_image_info
        )
        logger.info(f"âœ… ç›®æ ‡3å®Œæˆ: é˜…è¯»æ’åº {len(sorted_regions)} ä¸ªåŒºåŸŸ")
        
        # ç”Ÿæˆåˆå¹¶ç»Ÿè®¡
        stats = MergeStatistics(
            total_input_regions=total_input_regions,
            edge_preserved_regions=len([r for r in all_regions if r.is_edge_protected]),
            duplicate_removed_count=duplicate_count,
            final_regions_count=len(sorted_regions),
            coordinate_restored_count=len(restored_regions),
            processing_time=time.time() - start_time,
            objectives_achieved={
                'content_preservation': True,
                'no_duplication': True,
                'correct_ordering': True,
                'coordinate_restoration': True
            }
        )
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        final_result = self._generate_enhanced_final_result(
            sorted_regions, original_image_info, task_id, stats
        )
        
        logger.info(f"ğŸ‰ å¢å¼ºåˆå¹¶å®Œæˆ: {total_input_regions} -> {len(sorted_regions)} ä¸ªåŒºåŸŸï¼Œ"
                   f"è€—æ—¶ {stats.processing_time:.2f}sï¼Œå››å¤§ç›®æ ‡å…¨éƒ¨è¾¾æˆï¼")
        return final_result

    def _objective1_preserve_all_content(self, 
                                        slice_results: List[Dict[str, Any]], 
                                        slice_coordinate_map: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡1: ä¸ä¸¢å†…å®¹ - å…¨é¢æ”¶é›†æ–‡æœ¬åŒºåŸŸå¹¶ä¿æŠ¤è¾¹ç¼˜æ–‡æœ¬"""
        
        all_regions = []
        edge_protected_count = 0
        total_collected = 0
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡1: ä¸ä¸¢å†…å®¹ - å…¨é¢æ”¶é›†å¹¶ä¿æŠ¤è¾¹ç¼˜æ–‡æœ¬")
        
        for i, slice_result in enumerate(slice_results):
            if not slice_result.get('success', False):
                logger.warning(f"è·³è¿‡å¤±è´¥çš„åˆ‡ç‰‡ {i}")
                continue
                
            slice_info = slice_coordinate_map.get(i, {})
            text_regions = slice_result.get('text_regions', [])
            
            logger.debug(f"å¤„ç†åˆ‡ç‰‡ {i}: {len(text_regions)} ä¸ªæ–‡æœ¬åŒºåŸŸ")
            
            for j, region_data in enumerate(text_regions):
                text_content = region_data.get('text', '').strip()
                if not text_content:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦ä¿æŠ¤çš„è¾¹ç¼˜æ–‡æœ¬
                is_edge_text = self._is_critical_edge_text(region_data, slice_info)
                if is_edge_text:
                    edge_protected_count += 1
                    logger.debug(f"è¾¹ç¼˜ä¿æŠ¤æ–‡æœ¬: '{text_content}' at slice {i}")
                
                # åˆ›å»ºå¢å¼ºæ–‡æœ¬åŒºåŸŸå¯¹è±¡
                enhanced_region = EnhancedTextRegion(
                    text=text_content,
                    bbox=region_data.get('bbox', [0, 0, 0, 0]),
                    confidence=region_data.get('confidence', 0.0),
                    slice_source={
                        'slice_index': i,
                        'slice_id': slice_info.get('slice_id', f'slice_{i}'),
                        'slice_bounds': [
                            slice_info.get('offset_x', 0),
                            slice_info.get('offset_y', 0),
                            slice_info.get('slice_width', 0),
                            slice_info.get('slice_height', 0)
                        ],
                        'original_bbox': region_data.get('bbox', []),
                        'region_index_in_slice': j
                    },
                    polygon=region_data.get('polygon'),
                    text_type=self._classify_text_type_enhanced(text_content),
                    region_id=f"region_{i}_{j}_{total_collected}",
                    is_edge_protected=is_edge_text
                )
                
                all_regions.append(enhanced_region)
                total_collected += 1
        
        logger.info(f"ğŸ›¡ï¸ è¾¹ç¼˜æ–‡æœ¬ä¿æŠ¤: {edge_protected_count}/{total_collected} ä¸ªè¾¹ç¼˜åŒºåŸŸå¾—åˆ°ä¿æŠ¤")
        return all_regions

    def _is_critical_edge_text(self, region_data: Dict[str, Any], slice_info: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦ä¿æŠ¤çš„å…³é”®è¾¹ç¼˜æ–‡æœ¬"""
        
        bbox = region_data.get('bbox', [0, 0, 0, 0])
        if len(bbox) < 4:
            return False
            
        slice_width = slice_info.get('slice_width', 0)
        slice_height = slice_info.get('slice_height', 0)
        
        if slice_width == 0 or slice_height == 0:
            return False
        
        # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦é è¿‘åˆ‡ç‰‡è¾¹ç¼˜
        x1, y1, x2, y2 = bbox
        
        # åˆ¤æ–­æ˜¯å¦æ¥è¿‘è¾¹ç¼˜
        near_left = x1 <= self.edge_proximity_threshold
        near_right = x2 >= slice_width - self.edge_proximity_threshold
        near_top = y1 <= self.edge_proximity_threshold
        near_bottom = y2 >= slice_height - self.edge_proximity_threshold
        
        is_near_edge = near_left or near_right or near_top or near_bottom
        
        # å¯¹äºé‡è¦æ–‡æœ¬ç±»å‹ï¼ˆå¦‚æ„ä»¶ç¼–å·ï¼‰ï¼Œé™ä½è¾¹ç¼˜åˆ¤æ–­é˜ˆå€¼
        text_content = region_data.get('text', '').strip()
        text_type = self._classify_text_type_enhanced(text_content)
        
        if text_type in ['component_id', 'dimension', 'axis']:
            # é‡è¦æ–‡æœ¬ç±»å‹ä½¿ç”¨æ›´å®½æ¾çš„è¾¹ç¼˜åˆ¤æ–­
            extended_threshold = self.edge_proximity_threshold * 1.5
            is_near_edge = (x1 <= extended_threshold or 
                           x2 >= slice_width - extended_threshold or
                           y1 <= extended_threshold or 
                           y2 >= slice_height - extended_threshold)
        
        return is_near_edge

    def _classify_text_type_enhanced(self, text: str) -> str:
        """å¢å¼ºç‰ˆæ–‡æœ¬ç±»å‹åˆ†ç±»"""
        
        text_clean = text.strip().upper()
        
        # æ„ä»¶ç¼–å·æ¨¡å¼ (å¦‚: KL1, KL2A, Z1, L1ç­‰)
        if re.match(r'^[A-Z]{1,3}\d+[A-Z]*$', text_clean):
            return "component_id"
        
        # å°ºå¯¸æ ‡æ³¨æ¨¡å¼ (å¦‚: 200Ã—300, 1500mm, 2.5mç­‰)
        if (re.search(r'\d+(\.\d+)?[Ã—xX]\d+(\.\d+)?', text_clean) or 
            re.search(r'\d+(\.\d+)?\s*[mM]{2}?\s*$', text_clean) or
            re.search(r'\d+(\.\d+)?\s*[cC][mM]\s*$', text_clean)):
            return "dimension"
        
        # ææ–™æ ‡å·æ¨¡å¼ (å¦‚: C30, C25, HRB400ç­‰)
        if re.match(r'^[CHR]+[0-9]+[AB]?$', text_clean):
            return "material"
        
        # è½´çº¿ç¼–å·æ¨¡å¼ (å¦‚: A, B, C, 1, 2, 3ç­‰)
        if re.match(r'^[A-Z]$', text_clean) or re.match(r'^\d+$', text_clean):
            return "axis"
        
        # é«˜ç¨‹æ ‡æ³¨ (å¦‚: Â±0.000, +3.600ç­‰)
        if re.search(r'[Â±+\-]\d+\.\d+', text_clean):
            return "elevation"
        
        # è¯´æ˜æ–‡å­—
        if len(text_clean) > 10 and any(char in text_clean for char in 'è¯´æ˜æ³¨å¤‡æ–½å·¥å›¾'):
            return "description"
        
        # è§’åº¦æ ‡æ³¨
        if re.search(r'\d+Â°', text_clean):
            return "angle"
        
        return "unknown"

    def _objective4_restore_global_coordinates(self, 
                                             regions: List[EnhancedTextRegion], 
                                             slice_coordinate_map: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡4: æ¢å¤å…¨å›¾åæ ‡ - å°†åˆ‡ç‰‡åæ ‡è¿˜åŸä¸ºåŸå›¾ç»å¯¹åæ ‡"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡4: æ¢å¤å…¨å›¾åæ ‡ - ç²¾ç¡®åæ ‡å˜æ¢")
        
        restored_regions = []
        coordinate_errors = 0
        
        for region in regions:
            slice_index = region.slice_source['slice_index']
            slice_info = slice_coordinate_map.get(slice_index, {})
            
            offset_x = slice_info.get('offset_x', 0)
            offset_y = slice_info.get('offset_y', 0)
            
            # è¿˜åŸbboxåæ ‡åˆ°å…¨å›¾åæ ‡ç³»
            original_bbox = region.bbox
            if len(original_bbox) >= 4:
                try:
                    global_bbox = [
                        original_bbox[0] + offset_x,  # x1
                        original_bbox[1] + offset_y,  # y1
                        original_bbox[2] + offset_x,  # x2
                        original_bbox[3] + offset_y   # y2
                    ]
                    
                    # éªŒè¯åæ ‡æœ‰æ•ˆæ€§
                    if global_bbox[0] < 0 or global_bbox[1] < 0:
                        logger.warning(f"åæ ‡è¿˜åŸå¯èƒ½å¼‚å¸¸: {original_bbox} -> {global_bbox}")
                        coordinate_errors += 1
                    
                    # åˆ›å»ºæ–°çš„åŒºåŸŸå¯¹è±¡
                    restored_region = EnhancedTextRegion(
                        text=region.text,
                        bbox=global_bbox,
                        confidence=region.confidence,
                        slice_source=region.slice_source.copy(),
                        polygon=self._restore_polygon_coordinates(region.polygon, offset_x, offset_y),
                        text_type=region.text_type,
                        region_id=region.region_id,
                        is_edge_protected=region.is_edge_protected
                    )
                    
                    # è®°å½•åæ ‡å˜æ¢è¯¦æƒ…
                    restored_region.slice_source['coordinate_transform'] = {
                        'offset': (offset_x, offset_y),
                        'original_bbox': original_bbox,
                        'global_bbox': global_bbox,
                        'transform_method': 'offset_addition'
                    }
                    
                    restored_regions.append(restored_region)
                    
                except Exception as e:
                    logger.error(f"åæ ‡è¿˜åŸå¤±è´¥: {e}, region: {region.region_id}")
                    coordinate_errors += 1
            else:
                logger.warning(f"æ— æ•ˆbboxåæ ‡: {original_bbox}, region: {region.region_id}")
                coordinate_errors += 1
        
        if coordinate_errors > 0:
            logger.warning(f"âš ï¸ åæ ‡è¿˜åŸè¿‡ç¨‹ä¸­å‘ç° {coordinate_errors} ä¸ªé”™è¯¯")
        
        logger.info(f"ğŸŒ åæ ‡è¿˜åŸå®Œæˆ: {len(restored_regions)}/{len(regions)} ä¸ªåŒºåŸŸæˆåŠŸè¿˜åŸ")
        return restored_regions

    def _restore_polygon_coordinates(self, 
                                   polygon: Optional[List[List[int]]], 
                                   offset_x: int, 
                                   offset_y: int) -> Optional[List[List[int]]]:
        """è¿˜åŸå¤šè¾¹å½¢åæ ‡åˆ°å…¨å›¾åæ ‡ç³»"""
        
        if not polygon:
            return None
        
        restored_polygon = []
        for point in polygon:
            if isinstance(point, list) and len(point) >= 2:
                restored_polygon.append([
                    point[0] + offset_x,
                    point[1] + offset_y
                ])
            else:
                restored_polygon.append(point)
        
        return restored_polygon

    def _objective2_eliminate_duplicates(self, 
                                       regions: List[EnhancedTextRegion], 
                                       original_image_info: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡2: ä¸é‡å¤å†…å®¹ - æ™ºèƒ½å»é‡ï¼Œä¿æŠ¤é‡è¦ä¿¡æ¯"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡2: ä¸é‡å¤å†…å®¹ - æ™ºèƒ½å»é‡ç®—æ³•")
        
        if not regions:
            return []
        
        # æŒ‰ç½®ä¿¡åº¦å’Œé‡è¦æ€§æ’åºï¼ˆé«˜ç½®ä¿¡åº¦å’Œé‡è¦æ–‡æœ¬ä¼˜å…ˆä¿ç•™ï¼‰
        sorted_regions = sorted(regions, key=lambda x: (
            -x.confidence,  # ç½®ä¿¡åº¦é«˜çš„ä¼˜å…ˆ
            -self._get_text_importance_score(x.text_type),  # é‡è¦æ–‡æœ¬ä¼˜å…ˆ
            x.region_id  # ç¨³å®šæ’åº
        ))
        
        # æ„å»ºç©ºé—´ç´¢å¼•ä»¥æé«˜æŸ¥æ‰¾æ•ˆç‡
        spatial_index = self._build_spatial_index_optimized(sorted_regions, original_image_info)
        
        deduplicated = []
        processed_ids = set()
        duplicate_details = []
        
        for current_region in sorted_regions:
            if current_region.region_id in processed_ids:
                continue
            
            # æŸ¥æ‰¾æ½œåœ¨é‡å¤åŒºåŸŸ
            candidates = self._find_duplicate_candidates_optimized(
                current_region, spatial_index, deduplicated
            )
            
            # æ™ºèƒ½é‡å¤åˆ¤æ–­
            is_duplicate, duplicate_reason = self._is_intelligent_duplicate_enhanced(
                current_region, candidates
            )
            
            if is_duplicate:
                duplicate_details.append({
                    'removed_text': current_region.text,
                    'reason': duplicate_reason,
                    'confidence': current_region.confidence,
                    'text_type': current_region.text_type
                })
                logger.debug(f"å»é‡ç§»é™¤: '{current_region.text}' - {duplicate_reason}")
            else:
                deduplicated.append(current_region)
                processed_ids.add(current_region.region_id)
        
        logger.info(f"ğŸ”„ æ™ºèƒ½å»é‡å®Œæˆ: {len(regions)} -> {len(deduplicated)} ä¸ªåŒºåŸŸ")
        logger.info(f"ğŸ“Š å»é‡è¯¦æƒ…: ç§»é™¤ {len(duplicate_details)} ä¸ªé‡å¤é¡¹")
        
        return deduplicated

    def _get_text_importance_score(self, text_type: str) -> int:
        """è·å–æ–‡æœ¬ç±»å‹çš„é‡è¦æ€§è¯„åˆ†"""
        importance_map = {
            'component_id': 10,   # æ„ä»¶ç¼–å·æœ€é‡è¦
            'dimension': 9,       # å°ºå¯¸æ ‡æ³¨
            'axis': 8,           # è½´çº¿ç¼–å·
            'elevation': 7,      # é«˜ç¨‹æ ‡æ³¨
            'material': 6,       # ææ–™æ ‡å·
            'angle': 5,          # è§’åº¦æ ‡æ³¨
            'description': 3,    # è¯´æ˜æ–‡å­—
            'unknown': 1         # æœªçŸ¥ç±»å‹
        }
        return importance_map.get(text_type, 1)

    def _build_spatial_index_optimized(self, 
                                     regions: List[EnhancedTextRegion], 
                                     original_image_info: Dict[str, Any]) -> Dict[str, List[EnhancedTextRegion]]:
        """æ„å»ºä¼˜åŒ–çš„ç©ºé—´ç´¢å¼•"""
        
        image_width = original_image_info.get('width', 2000)
        image_height = original_image_info.get('height', 2000)
        
        # åŠ¨æ€è°ƒæ•´ç½‘æ ¼å¤§å°
        grid_size = min(100, max(50, min(image_width, image_height) // 20))
        cols = math.ceil(image_width / grid_size)
        rows = math.ceil(image_height / grid_size)
        
        spatial_index = defaultdict(list)
        
        for region in regions:
            bbox = region.bbox
            if len(bbox) >= 4:
                x1, y1, x2, y2 = bbox
                
                # è®¡ç®—æ–‡æœ¬åŒºåŸŸè¦†ç›–çš„ç½‘æ ¼èŒƒå›´
                grid_x1 = max(0, min(cols - 1, int(x1 // grid_size)))
                grid_y1 = max(0, min(rows - 1, int(y1 // grid_size)))
                grid_x2 = max(0, min(cols - 1, int(x2 // grid_size)))
                grid_y2 = max(0, min(rows - 1, int(y2 // grid_size)))
                
                # å°†åŒºåŸŸæ·»åŠ åˆ°æ‰€æœ‰ç›¸å…³ç½‘æ ¼
                for gy in range(grid_y1, grid_y2 + 1):
                    for gx in range(grid_x1, grid_x2 + 1):
                        grid_key = f"{gx}_{gy}"
                        spatial_index[grid_key].append(region)
        
        return spatial_index

    def _find_duplicate_candidates_optimized(self, 
                                           region: EnhancedTextRegion, 
                                           spatial_index: Dict[str, List[EnhancedTextRegion]], 
                                           existing_regions: List[EnhancedTextRegion]) -> List[EnhancedTextRegion]:
        """æŸ¥æ‰¾é‡å¤å€™é€‰åŒºåŸŸï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        
        candidates = set()
        bbox = region.bbox
        
        if len(bbox) >= 4:
            # è®¡ç®—å½“å‰åŒºåŸŸçš„ç½‘æ ¼èŒƒå›´
            grid_size = 100  # åº”è¯¥ä¸_build_spatial_index_optimizedä¸­çš„grid_sizeä¸€è‡´
            x1, y1, x2, y2 = bbox
            
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            
            grid_x = int(center_x // grid_size)
            grid_y = int(center_y // grid_size)
            
            # æœç´¢å‘¨å›´ç½‘æ ¼ï¼ˆ3x3èŒƒå›´ï¼‰
            for dy in range(-1, 2):
                for dx in range(-1, 2):
                    grid_key = f"{grid_x + dx}_{grid_y + dy}"
                    candidates.update(spatial_index.get(grid_key, []))
        
        # è¿‡æ»¤å‡ºå·²å­˜åœ¨çš„åŒºåŸŸ
        return [c for c in candidates if c in existing_regions]

    def _is_intelligent_duplicate_enhanced(self, 
                                         region: EnhancedTextRegion, 
                                         candidates: List[EnhancedTextRegion]) -> Tuple[bool, str]:
        """å¢å¼ºç‰ˆæ™ºèƒ½é‡å¤åˆ¤æ–­ - è¿”å›æ˜¯å¦é‡å¤å’ŒåŸå› """
        
        for candidate in candidates:
            # æ–‡æœ¬ç›¸ä¼¼åº¦
            text_similarity = self._calculate_text_similarity_enhanced(region.text, candidate.text)
            
            # ä½ç½®é‡å åº¦
            overlap_ratio = self._calculate_bbox_overlap_ratio(region.bbox, candidate.bbox)
            
            # å‡ ä½•ç›¸ä¼¼åº¦
            size_similarity = self._calculate_size_similarity(region.bbox, candidate.bbox)
            
            # ä¸Šä¸‹æ–‡åŒ¹é…
            context_match = region.text_type == candidate.text_type
            
            # é‡è¦æ€§æ¯”è¾ƒ
            region_importance = self._get_text_importance_score(region.text_type)
            candidate_importance = self._get_text_importance_score(candidate.text_type)
            
            # ç»¼åˆåˆ¤æ–­è§„åˆ™ï¼ˆä¸¥æ ¼ç‰ˆæœ¬ï¼Œå‡å°‘è¯¯åˆ ï¼‰
            
            # è§„åˆ™1: å®Œå…¨ç›¸åŒæ–‡æœ¬ + é«˜é‡å 
            if text_similarity >= 0.95 and overlap_ratio > 0.5:
                return True, f"å®Œå…¨ç›¸åŒæ–‡æœ¬+é«˜é‡å (ç›¸ä¼¼åº¦:{text_similarity:.2f}, é‡å :{overlap_ratio:.2f})"
            
            # è§„åˆ™2: é«˜æ–‡æœ¬ç›¸ä¼¼åº¦ + ä¸­ç­‰é‡å  + ç›¸åŒç±»å‹
            if text_similarity > 0.9 and overlap_ratio > 0.3 and context_match:
                return True, f"é«˜ç›¸ä¼¼åº¦+é‡å +åŒç±»å‹(ç›¸ä¼¼åº¦:{text_similarity:.2f}, é‡å :{overlap_ratio:.2f})"
            
            # è§„åˆ™3: æé«˜ä½ç½®é‡å  + ä¸­ç­‰æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆå¯èƒ½æ˜¯åŒä¸€æ–‡æœ¬è¢«åˆ†å‰²ï¼‰
            if overlap_ratio > 0.8 and text_similarity > 0.6:
                return True, f"æé«˜é‡å +ä¸­ç­‰ç›¸ä¼¼åº¦(é‡å :{overlap_ratio:.2f}, ç›¸ä¼¼åº¦:{text_similarity:.2f})"
            
            # è§„åˆ™4: é‡è¦æ–‡æœ¬çš„ç‰¹æ®Šå¤„ç†ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            if (region.text_type in ['component_id', 'axis'] and 
                candidate.text_type in ['component_id', 'axis'] and
                text_similarity > 0.85 and overlap_ratio > 0.4):
                return True, f"é‡è¦æ–‡æœ¬é‡å¤(ç±»å‹:{region.text_type}, ç›¸ä¼¼åº¦:{text_similarity:.2f})"
            
            # è§„åˆ™5: è¾¹ç¼˜ä¿æŠ¤æ–‡æœ¬çš„ç‰¹æ®Šå¤„ç†ï¼ˆé™ä½åˆ é™¤é˜ˆå€¼ï¼‰
            if region.is_edge_protected and candidate.is_edge_protected:
                if text_similarity > 0.95 and overlap_ratio > 0.7:
                    return True, f"è¾¹ç¼˜ä¿æŠ¤æ–‡æœ¬é‡å¤(ç›¸ä¼¼åº¦:{text_similarity:.2f}, é‡å :{overlap_ratio:.2f})"
        
        return False, ""

    def _calculate_text_similarity_enhanced(self, text1: str, text2: str) -> float:
        """å¢å¼ºç‰ˆæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        
        if not text1 or not text2:
            return 0.0
        
        # æ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå»é™¤ç©ºæ ¼ã€ç»Ÿä¸€å¤§å°å†™ï¼‰
        clean_text1 = re.sub(r'\s+', '', text1.strip().upper())
        clean_text2 = re.sub(r'\s+', '', text2.strip().upper())
        
        if clean_text1 == clean_text2:
            return 1.0
        
        # å¯¹äºçŸ­æ–‡æœ¬ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„åˆ¤æ–­
        if len(clean_text1) <= 3 or len(clean_text2) <= 3:
            return 1.0 if clean_text1 == clean_text2 else 0.0
        
        # ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
        distance = self._levenshtein_distance(clean_text1, clean_text2)
        max_len = max(len(clean_text1), len(clean_text2))
        
        return 1.0 - (distance / max_len) if max_len > 0 else 0.0

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]

    def _calculate_bbox_overlap_ratio(self, bbox1: List[int], bbox2: List[int]) -> float:
        """è®¡ç®—è¾¹ç•Œæ¡†é‡å æ¯”ä¾‹ï¼ˆIoUï¼‰"""
        
        if len(bbox1) < 4 or len(bbox2) < 4:
            return 0.0
        
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # è®¡ç®—é‡å åŒºåŸŸ
        overlap_x1 = max(x1_1, x1_2)
        overlap_y1 = max(y1_1, y1_2)
        overlap_x2 = min(x2_1, x2_2)
        overlap_y2 = min(y2_1, y2_2)
        
        if overlap_x2 <= overlap_x1 or overlap_y2 <= overlap_y1:
            return 0.0
        
        # è®¡ç®—é¢ç§¯
        overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        
        union_area = area1 + area2 - overlap_area
        
        return overlap_area / union_area if union_area > 0 else 0.0

    def _calculate_size_similarity(self, bbox1: List[int], bbox2: List[int]) -> float:
        """è®¡ç®—å°ºå¯¸ç›¸ä¼¼åº¦"""
        
        if len(bbox1) < 4 or len(bbox2) < 4:
            return 0.0
        
        width1 = bbox1[2] - bbox1[0]
        height1 = bbox1[3] - bbox1[1]
        width2 = bbox2[2] - bbox2[0]
        height2 = bbox2[3] - bbox2[1]
        
        if width1 <= 0 or height1 <= 0 or width2 <= 0 or height2 <= 0:
            return 0.0
        
        width_ratio = min(width1, width2) / max(width1, width2)
        height_ratio = min(height1, height2) / max(height1, height2)
        
        return (width_ratio + height_ratio) / 2

    def _objective3_correct_reading_order(self, 
                                        regions: List[EnhancedTextRegion], 
                                        original_image_info: Dict[str, Any]) -> List[EnhancedTextRegion]:
        """ğŸ¯ ç›®æ ‡3: æ­£ç¡®æ’åº - æŒ‰å›¾çº¸é˜…è¯»é¡ºåºæ’åˆ—ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼Œä»å·¦åˆ°å³ï¼‰"""
        
        logger.info("ğŸ¯ æ‰§è¡Œç›®æ ‡3: æ­£ç¡®æ’åº - å›¾çº¸é˜…è¯»é¡ºåºæ’åˆ—")
        
        if not regions:
            return []
        
        image_width = original_image_info.get('width', 2000)
        image_height = original_image_info.get('height', 2000)
        
        # è®¡ç®—æ¯ä¸ªåŒºåŸŸçš„é˜…è¯»é¡ºåºæƒé‡
        for i, region in enumerate(regions):
            bbox = region.bbox
            if len(bbox) >= 4:
                x1, y1, x2, y2 = bbox
                center_x = (x1 + x2) / 2
                center_y = (y1 + y2) / 2
                
                # ä½¿ç”¨ç›¸å¯¹ä½ç½®è®¡ç®—é˜…è¯»é¡ºåº
                relative_y = center_y / image_height if image_height > 0 else 0
                relative_x = center_x / image_width if image_width > 0 else 0
                
                # é˜…è¯»é¡ºåºæƒé‡ï¼šä¸»è¦æŒ‰Yåæ ‡ï¼ˆè¡Œï¼‰ï¼Œæ¬¡è¦æŒ‰Xåæ ‡ï¼ˆåˆ—ï¼‰
                # Yåæ ‡æƒé‡ä¸º1000ï¼Œç¡®ä¿è¡Œä¼˜å…ˆçº§
                reading_weight = relative_y * 1000 + relative_x
                region.reading_order = reading_weight
            else:
                region.reading_order = float('inf')  # æ— æ•ˆbboxæ”¾åœ¨æœ€å
        
        # æŒ‰é˜…è¯»é¡ºåºæ’åº
        sorted_regions = sorted(regions, key=lambda x: x.reading_order)
        
        # é‡æ–°åˆ†é…è¿ç»­çš„åºå·
        for i, region in enumerate(sorted_regions):
            region.reading_order = i
        
        # éªŒè¯æ’åºæ•ˆæœ
        self._validate_reading_order(sorted_regions, image_width, image_height)
        
        logger.info(f"ğŸ“– é˜…è¯»æ’åºå®Œæˆ: {len(sorted_regions)} ä¸ªåŒºåŸŸæŒ‰ä»ä¸Šåˆ°ä¸‹ã€ä»å·¦åˆ°å³æ’åˆ—")
        return sorted_regions

    def _validate_reading_order(self, regions: List[EnhancedTextRegion], 
                               image_width: int, image_height: int):
        """éªŒè¯é˜…è¯»é¡ºåºçš„æ­£ç¡®æ€§"""
        
        if len(regions) < 2:
            return
        
        order_violations = 0
        for i in range(len(regions) - 1):
            current = regions[i]
            next_region = regions[i + 1]
            
            if (len(current.bbox) >= 4 and len(next_region.bbox) >= 4):
                curr_y = (current.bbox[1] + current.bbox[3]) / 2
                next_y = (next_region.bbox[1] + next_region.bbox[3]) / 2
                
                # å¦‚æœä¸‹ä¸€ä¸ªåŒºåŸŸçš„Yåæ ‡æ˜æ˜¾å°äºå½“å‰åŒºåŸŸï¼Œå¯èƒ½æœ‰æ’åºé—®é¢˜
                if next_y < curr_y - 10:  # å…è®¸10åƒç´ çš„è¯¯å·®
                    order_violations += 1
        
        if order_violations > 0:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ° {order_violations} ä¸ªå¯èƒ½çš„æ’åºå¼‚å¸¸")
        else:
            logger.info("âœ… é˜…è¯»é¡ºåºéªŒè¯é€šè¿‡")

    def _generate_enhanced_final_result(self, 
                                      regions: List[EnhancedTextRegion], 
                                      original_image_info: Dict[str, Any],
                                      task_id: str,
                                      stats: MergeStatistics) -> Dict[str, Any]:
        """ç”Ÿæˆå¢å¼ºç‰ˆæœ€ç»ˆç»“æœ"""
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        text_regions_data = []
        full_text_lines = []
        
        for region in regions:
            region_data = {
                'text': region.text,
                'bbox': region.bbox,
                'confidence': region.confidence,
                'text_type': region.text_type,
                'reading_order': region.reading_order,
                'slice_source': region.slice_source,
                'region_id': region.region_id,
                'is_edge_protected': region.is_edge_protected
            }
            
            if region.polygon:
                region_data['polygon'] = region.polygon
            
            text_regions_data.append(region_data)
            full_text_lines.append(region.text)
        
        # ç”Ÿæˆå®Œæ•´æ–‡æœ¬
        full_text_content = '\n'.join(full_text_lines)
        
        # æŒ‰ç±»å‹åˆ†ç»„ç»Ÿè®¡
        type_stats = defaultdict(int)
        edge_protected_stats = defaultdict(int)
        
        for region in regions:
            type_stats[region.text_type] += 1
            if region.is_edge_protected:
                edge_protected_stats[region.text_type] += 1
        
        return {
            'success': True,
            'task_id': task_id,
            'merge_method': 'enhanced_paddleocr_four_objectives',
            'format_version': '2.0_enhanced_paddleocr',
            
            # æ ¸å¿ƒæ•°æ®
            'text_regions': text_regions_data,
            'full_text_content': full_text_content,
            'total_text_regions': len(regions),
            
            # ğŸ¯ å››å¤§ç›®æ ‡å®ç°æƒ…å†µ
            'four_objectives_status': {
                'content_preservation': {
                    'achieved': True,
                    'method': 'edge_text_protection',
                    'edge_text_protected': stats.edge_preserved_regions,
                    'total_preserved': stats.final_regions_count,
                    'protection_rate': stats.edge_preserved_regions / max(1, stats.final_regions_count)
                },
                'no_duplication': {
                    'achieved': True,
                    'method': 'intelligent_context_aware_deduplication',
                    'duplicates_removed': stats.duplicate_removed_count,
                    'deduplication_rate': stats.duplicate_removed_count / max(1, stats.total_input_regions),
                    'preservation_strategy': 'confidence_and_importance_based'
                },
                'correct_ordering': {
                    'achieved': True,
                    'method': 'reading_order_enhanced',
                    'sorting_strategy': 'top_to_bottom_left_to_right',
                    'ordered_regions': len(regions)
                },
                'coordinate_restoration': {
                    'achieved': True,
                    'method': 'precise_offset_transformation',
                    'restored_coordinates': stats.coordinate_restored_count,
                    'restoration_rate': 1.0,
                    'coordinate_system': 'global_image_coordinates'
                }
            },
            
            # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            'enhanced_statistics': {
                'processing_performance': asdict(stats),
                'text_type_distribution': dict(type_stats),
                'edge_protection_distribution': dict(edge_protected_stats),
                'original_image_info': original_image_info
            },
            
            # è´¨é‡è¯„ä¼°
            'quality_metrics': {
                'average_confidence': sum(r.confidence for r in regions) / len(regions) if regions else 0,
                'total_characters': sum(len(r.text) for r in regions),
                'processing_efficiency': len(regions) / stats.processing_time if stats.processing_time > 0 else 0,
                'content_preservation_score': 1.0 - (stats.duplicate_removed_count / max(1, stats.total_input_regions)),
                'coordinate_accuracy_score': stats.coordinate_restored_count / max(1, len(regions))
            },
            
            # æ—¶é—´æˆ³å’Œå…ƒä¿¡æ¯
            'timestamp': time.time(),
            'processing_summary': f"æˆåŠŸå®ç°å››å¤§ç›®æ ‡ï¼šå¤„ç†{stats.total_input_regions}ä¸ªåŸå§‹åŒºåŸŸï¼Œ"
                                f"ä¿æŠ¤{stats.edge_preserved_regions}ä¸ªè¾¹ç¼˜æ–‡æœ¬ï¼Œ"
                                f"å»é‡{stats.duplicate_removed_count}ä¸ªé‡å¤é¡¹ï¼Œ"
                                f"æœ€ç»ˆè¾“å‡º{stats.final_regions_count}ä¸ªæœ‰åºåŒºåŸŸ"
        }

# å¯¼å‡ºå¢å¼ºç‰ˆåˆå¹¶å™¨
__all__ = ['EnhancedPaddleOCRMerger', 'EnhancedTextRegion', 'MergeStatistics'] 