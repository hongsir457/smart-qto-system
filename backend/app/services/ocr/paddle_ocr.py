#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PaddleOCRæœåŠ¡ - Eager Initialization (é¢„å…ˆåˆå§‹åŒ–) ç‰ˆæœ¬
åœ¨æ¨¡å—åŠ è½½æ—¶ç›´æ¥åˆå§‹åŒ–å…¨å±€å”¯ä¸€çš„PaddleOCRå®ä¾‹ï¼Œä»¥è§£å†³åœ¨Celeryç­‰ç¯å¢ƒä¸­åˆå§‹åŒ–å¤±è´¥çš„é—®é¢˜ã€‚
"""
import logging
import threading
import time
from pathlib import Path
from typing import Dict, Any, List
import numpy as np
import cv2
import uuid
import json
from io import BytesIO
# import shutil  # No longer needed for debugging

# æ ¸å¿ƒä¾èµ–ï¼Œå¦‚æœç¼ºå¤±åˆ™ç³»ç»Ÿæ— æ³•æä¾›OCRåŠŸèƒ½
try:
    from paddleocr import PaddleOCR
except ImportError:
    logging.critical("âŒ FATAL: paddleocråº“æœªå®‰è£…ï¼ŒOCRåŠŸèƒ½å°†å®Œå…¨ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install paddleocr")
    PaddleOCR = None

from app.core.config import settings
from app.utils.image_processing import correct_skew, enhance_image, calculate_image_clarity

# å¯¼å…¥å›¾åƒé¢„å¤„ç†å™¨
try:
    from app.services.image_preprocessor import image_preprocessor
except ImportError:
    image_preprocessor = None

# å¯¼å…¥å­˜å‚¨æœåŠ¡
try:
    from app.services.s3_service import S3Service
    from app.services.dual_storage_service import DualStorageService
except ImportError:
    S3Service = None
    DualStorageService = None

logger = logging.getLogger(__name__)

# --- Eager Initialization of Global PaddleOCR Instance ---

def _initialize_global_paddleocr():
    """
    åœ¨æ¨¡å—åŠ è½½æ—¶æ‰§è¡Œçš„é¢„å…ˆåˆå§‹åŒ–å‡½æ•°ã€‚
    åˆ›å»ºå¹¶é¢„çƒ­ä¸€ä¸ªå…¨å±€å”¯ä¸€çš„PaddleOCRå®ä¾‹ã€‚
    """
    if not PaddleOCR:
        logger.error("ğŸš« PaddleOCRåº“æœªåŠ è½½ï¼Œæ— æ³•åˆå§‹åŒ–ã€‚")
        return None

    logger.info("ğŸš€ EAGERLY INITIALIZING PADDLEOCR (GLOBAL INSTANCE)...")
    try:
        # ä¼˜åŒ–é…ç½®ï¼Œå¹³è¡¡æ€§èƒ½ä¸èµ„æºæ¶ˆè€—
        config = {
            'use_angle_cls': True,
            'lang': 'ch',
            'use_space_char': True,
            'drop_score': 0.3,  # é™ä½ä¸¢å¼ƒé˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šä½ç½®ä¿¡åº¦ç»“æœ
            'use_gpu': False,
            'enable_mkldnn': True,
            'cpu_threads': max(1, cv2.getNumberOfCPUs() // 2),
            'show_log': False,
            'det_limit_side_len': 960,
            'max_side_len': 2400, # é…åˆæ›´é«˜DPIï¼Œæå‡æœ€å¤§è¾¹é•¿
            'det_db_box_thresh': 0.5, # é™ä½æ¡†æ£€æµ‹é˜ˆå€¼ï¼Œæ›´å®¹æ˜“æ£€æµ‹å°æ–‡æœ¬
        }
        logger.info(f"âš™ï¸ PaddleOCR Config (High Accuracy): {config}")

        logger.info("â³ Creating PaddleOCR instance...")
        start_time = time.time()
        ocr_instance = PaddleOCR(**config)
        creation_time = time.time() - start_time
        logger.info(f"âœ… PaddleOCR instance created in {creation_time:.2f}s.")

        # é¢„çƒ­ï¼Œç¡®ä¿æ¨¡å‹å®Œå…¨åŠ è½½åˆ°å†…å­˜
        logger.info("ğŸ”¥ Warming up PaddleOCR instance...")
        start_time = time.time()
        warmup_image = np.zeros((100, 200, 3), dtype='uint8')
        cv2.putText(warmup_image, "Warmup", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        ocr_instance.ocr(warmup_image, cls=True)
        warmup_time = time.time() - start_time
        logger.info(f"âœ… PaddleOCR warmup complete in {warmup_time:.2f}s.")
        
        return ocr_instance

    except Exception as e:
        logger.critical("âŒâŒâŒ FATAL: Global PaddleOCR initialization failed. OCR will be unavailable.", exc_info=True)
        return None

# --- Global Instance & Thread Lock ---
_paddle_ocr_instance = _initialize_global_paddleocr()
_ocr_lock = threading.Lock()  # ç”¨äºOCRè°ƒç”¨çš„çº¿ç¨‹å®‰å…¨é”

class PaddleOCRService:
    """
    å¯¹å…¨å±€PaddleOCRå®ä¾‹çš„å°è£…æœåŠ¡ã€‚
    æä¾›æ–‡æœ¬è¯†åˆ«åŠç›¸å…³çš„è¾…åŠ©åŠŸèƒ½ã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡ï¼Œå…³è”åˆ°å…¨å±€OCRå®ä¾‹ï¼Œå¹¶å‡†å¤‡å­˜å‚¨æœåŠ¡ã€‚"""
        self.ocr = _paddle_ocr_instance
        self.initialized = self.ocr is not None
        
        # å·²ç§»é™¤é™çº§å¤„ç†
        try:
            self.dual_storage = DualStorageService() if DualStorageService else None
            self.s3_service = S3Service() if S3Service else None
            self.storage_service = self.dual_storage or self.s3_service
        except Exception as e:
            logger.warning(f"å­˜å‚¨æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.dual_storage = None
            self.s3_service = None
            self.storage_service = None
            
        if self.initialized:
            logger.info("â™»ï¸ PaddleOCRService is using the pre-initialized global instance.")
            if self.dual_storage:
                logger.info("âœ… ä½¿ç”¨åŒé‡å­˜å‚¨æœåŠ¡")
            elif self.s3_service:
                logger.info("âš ï¸ é™çº§ä½¿ç”¨S3å­˜å‚¨æœåŠ¡")
            else:
                logger.warning("âš ï¸ æ‰€æœ‰å­˜å‚¨æœåŠ¡ä¸å¯ç”¨ï¼Œæ–‡ä»¶ä¿å­˜åŠŸèƒ½å°†è¢«ç¦ç”¨")
        else:
            logger.warning("âš ï¸ PaddleOCRService is in a non-functional state due to initialization failure.")

    def is_available(self) -> bool:
        """æ£€æŸ¥OCRå®ä¾‹æ˜¯å¦å·²æˆåŠŸåˆå§‹åŒ–å¹¶å¯ç”¨ã€‚"""
        return self.initialized

    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çš„å½“å‰çŠ¶æ€ã€‚"""
        return {
            'initialized': self.initialized,
            'ocr_instance_ready': self.initialized,
            'lazy_init_attempted': True, # In eager mode, it's always "attempted" at load time
            'is_available': self.is_available(),
            'mode': 'PaddleOCR' if self.initialized else 'Unavailable'
        }

    def recognize_text(self, image_path: str, save_to_sealos: bool = True, drawing_id: str = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒOCRæ–‡æœ¬è¯†åˆ«ï¼Œå¹¶å¯é€‰åœ°å°†ç»“æœä¿å­˜åˆ°äº‘å­˜å‚¨
        
        Returns:
            Dict containing:
            - success: bool
            - text_regions: List[Dict] with detected text regions
            - all_text: str concatenated text
            - statistics: Dict with region count and confidence
            - storage_info: Dict if saved to Sealos
        """

        try:
            image_file = Path(image_path)
            if not image_file.exists():
                raise FileNotFoundError(f"Image file not found: {image_path}")

            logger.info(f"ğŸš€ Starting OCR recognition for: {image_path}")
            
            logger.info("ğŸ“Š (Pre-check) Detecting text regions on original image...")

            try:
                # ä¿®å¤: ä½¿ç”¨æ­£ç¡®çš„PaddleOCR API
                # PaddleOCRçš„ocræ–¹æ³•è¿”å›æ ¼å¼: [[[x1,y1], [x2,y2], [x3,y3], [x4,y4]], (text, confidence)]
                ocr_result = self.ocr.ocr(str(image_file), rec=True)

                if ocr_result is None or not ocr_result:
                    logger.warning("OCR detection returned no results.")
                    return {
                        "success": False,
                        "error": "No text detected",
                        "text_regions": [],
                        "all_text": "",
                        "statistics": {"total_regions": 0, "avg_confidence": 0},
                        "raw_paddle_data": []
                    }
                
                # ocr_resulté€šå¸¸æ˜¯ä¸€ä¸ªåµŒå¥—åˆ—è¡¨: [[...], [...], ...]
                # å¤„ç†å¯èƒ½çš„å¤šé¡µç»“æœï¼ˆPDFè½¬å›¾ç‰‡ï¼‰
                if isinstance(ocr_result, list) and len(ocr_result) > 0:
                    if isinstance(ocr_result[0], list) and len(ocr_result[0]) > 0:
                        # å¦‚æœæ˜¯å¤šé¡µï¼Œå–ç¬¬ä¸€é¡µ
                        page_result = ocr_result[0]
                    else:
                        page_result = ocr_result
                else:
                    page_result = []
                
                logger.info(f"ğŸ” Found {len(page_result)} text boxes, processing recognition...")
                
                # å¤„ç†OCRç»“æœ
                processed_result = self._process_ocr_result(page_result)
                processed_result["raw_paddle_data"] = page_result
                processed_result["success"] = True
                
                # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
                if processed_result.get("statistics", {}).get("total_regions", 0) == 0:
                    logger.warning("OCR recognition completed but no text regions found.")
                    # å¯èƒ½ä¸æ˜¯é”™è¯¯ï¼Œåªæ˜¯å›¾ç‰‡ä¸­æ²¡æœ‰æ–‡å­—
                    processed_result["success"] = False
                    processed_result["error"] = "No text regions detected"
                    
                # åº”ç”¨ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æ ¡æ­£ - æ ¹æ®ç”¨æˆ·è¦æ±‚å·²ç¦ç”¨
                # processed_result = self._apply_construction_text_correction(processed_result)
                logger.info("ğŸš« æ–‡æœ¬çº é”™å·²ç¦ç”¨ï¼Œä¿æŒOCRåŸå§‹ç»“æœ")

                if save_to_sealos:
                    try:
                        storage_info = self._save_complete_raw_result_to_sealos(
                            raw_paddle_data=page_result,
                            ocr_result=processed_result,
                            image_path=image_path,
                            drawing_id=drawing_id
                        )
                        processed_result["storage_info"] = storage_info
                        if storage_info.get("saved"):
                            logger.info(f"âœ… OCRç»“æœå·²ä¿å­˜åˆ°Sealos: {storage_info.get('json_result', {}).get('s3_key', 'N/A')}")
                        else:
                            logger.warning(f"âš ï¸ OCRç»“æœä¿å­˜å¤±è´¥: {storage_info.get('error', 'Unknown error')}")
                    except Exception as storage_error:
                        logger.error(f"âŒ OCRç»“æœå­˜å‚¨å¼‚å¸¸: {storage_error}")
                        processed_result["storage_info"] = {"saved": False, "error": str(storage_error)}
                
                return processed_result

            except Exception as e:
                import traceback
                logger.error(f"âš ï¸ An error occurred during OCR recognition process: {str(e)}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                
                # è¿”å›ä¸€è‡´çš„é”™è¯¯æ ¼å¼
                return {
                    "success": False,
                    "error": str(e),
                    "text_regions": [],
                    "all_text": "",
                    "statistics": {"total_regions": 0, "avg_confidence": 0},
                    "raw_paddle_data": []
                }

        except Exception as e:
            import traceback
            logger.error(f"âš ï¸ An error occurred during OCR recognition process: {e}")
            logger.debug(traceback.format_exc())
            
            # è¿”å›ä¸€è‡´çš„é”™è¯¯æ ¼å¼
            return {
                "success": False,
                "error": str(e),
                "text_regions": [],
                "all_text": "",
                "statistics": {"total_regions": 0, "avg_confidence": 0},
                "raw_paddle_data": []
            }

    def extract_text_from_image(self, image_path: str) -> List[Dict]:
        """
        æå–å›¾åƒä¸­çš„æ–‡æœ¬ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Dict]: OCRç»“æœåˆ—è¡¨
        """
        try:
            # è°ƒç”¨recognize_textè·å–å®Œæ•´ç»“æœ
            result = self.recognize_text(image_path, save_to_sealos=False)
            
            if not result.get("success", False):
                logger.warning(f"OCRæ–‡æœ¬æå–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return []
            
            # è¿”å›text_regionsæ ¼å¼çš„æ•°æ®
            text_regions = result.get("text_regions", [])
            
            # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
            extracted_texts = []
            for region in text_regions:
                extracted_texts.append({
                    "text": region.get("text", ""),
                    "position": region.get("position", []),
                    "confidence": region.get("confidence", 0.0),
                    "bbox": region.get("bbox", {})
                })
            
            logger.debug(f"æå–æ–‡æœ¬å®Œæˆ: {len(extracted_texts)} ä¸ªæ–‡æœ¬åŒºåŸŸ")
            return extracted_texts
            
        except Exception as e:
            logger.error(f"æå–å›¾åƒæ–‡æœ¬å¤±è´¥: {e}")
            return []

    def _structure_result(self, det_boxes, rec_result):
        """
        Helper function to combine detection and recognition results.
        """
        structured_output = []
        # rec_result format: [('text', confidence), ...]
        if len(det_boxes) != len(rec_result):
            logger.warning(
                f"Mismatch between detected boxes ({len(det_boxes)}) and recognized texts ({len(rec_result)}). "
                f"Results may be misaligned."
            )

        for i, res in enumerate(rec_result):
            if i < len(det_boxes):
                box = det_boxes[i]
                text, confidence = res
                structured_output.append({
                    "box": [[int(p[0]), int(p[1])] for p in box],
                    "text": text,
                    "confidence": float(confidence)
                })
        return structured_output

    def recognize_images(self, image_paths: List[str], drawing_id: int = None) -> Dict[str, Any]:
        """
        è¯†åˆ«å›¾åƒåˆ—è¡¨ï¼Œå¹¶å°†æ‰€æœ‰ç»“æœæ‰“åŒ…ä¸Šä¼ åˆ°S3ã€‚
        è¿™æ˜¯ä¸ºCeleryåŒè½¨ä»»åŠ¡è®¾è®¡çš„æ ¸å¿ƒæ–¹æ³•ã€‚
        """
        if not self.is_available():
            logger.error("ğŸš« OCR service is unavailable. Cannot process image list.")
            return {"success": False, "error": "PaddleOCR not initialized"}

        logger.info(f"ğŸš€ğŸš€ Starting batch OCR recognition for {len(image_paths)} images. Drawing ID: {drawing_id}")
        
        all_processed_results = []
        all_raw_results = [] # å­˜å‚¨æœ€åŸå§‹çš„paddle ocrè¾“å‡º
        
        for image_path in image_paths:
            # è°ƒç”¨å•å›¾è¯†åˆ«ï¼Œä½†ä¸è®©å®ƒå•ç‹¬ä¸Šä¼ S3
            processed_result = self.recognize_text(image_path, save_to_sealos=False, drawing_id=str(drawing_id))
            all_processed_results.append(processed_result)
            
            # ä¿®å¤: ç¡®ä¿processed_resultæ˜¯å­—å…¸æ ¼å¼ï¼Œå¹¶å®‰å…¨åœ°æå–åŸå§‹æ•°æ®
            if isinstance(processed_result, dict) and processed_result.get("raw_paddle_data"):
                all_raw_results.extend(processed_result["raw_paddle_data"])
            elif not isinstance(processed_result, dict):
                logger.warning(f"Expected dict from recognize_text, got {type(processed_result)}: {processed_result}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æˆåŠŸçš„ç»“æœ
        successful_results = [r for r in all_processed_results if isinstance(r, dict) and r.get("success", False)]
        
        if not successful_results:
            # æ‰€æœ‰å›¾ç‰‡éƒ½å¤„ç†å¤±è´¥
            error_msgs = []
            for r in all_processed_results:
                if isinstance(r, dict) and r.get("error"):
                    error_msgs.append(r["error"])
            
            combined_error = "; ".join(error_msgs) if error_msgs else "All OCR processing failed"
            logger.error(f"âŒ (Batch OCR) All images failed processing: {combined_error}")
            
            return {
                "success": False,
                "error": combined_error,
                "total_images_processed": len(image_paths),
                "successful_images": 0
            }

        # ä½¿ç”¨ç°æœ‰çš„ã€åŠŸèƒ½æ›´å…¨çš„ä¿å­˜æ–¹æ³•æ¥å¤„ç†ä¸Šä¼ 
        # è¿™ä¸ªæ–¹æ³•ä¼šåŒæ—¶ä¿å­˜ .json å’Œ .txt
        try:
            save_result_info = self._save_complete_raw_result_to_sealos(
                raw_paddle_data=all_raw_results,
                ocr_result={"text_regions": [region for res in successful_results for region in res.get("text_regions", [])]}, # åˆå¹¶æ‰€æœ‰æ–‡æœ¬åŒºåŸŸ
                image_path=image_paths[0] if image_paths else "batch_process",
                drawing_id=str(drawing_id)
            )
        except Exception as e:
            logger.error(f"âŒ (Batch OCR) Failed to save results to S3: {e}")
            return {
                "success": False,
                "error": f"Failed to save results: {str(e)}",
                "total_images_processed": len(image_paths),
                "successful_images": len(successful_results)
            }

        if not save_result_info.get("saved"):
            error_msg = f"Failed to save batch result to S3: {save_result_info.get('error')}"
            logger.error(f"âŒ (Batch OCR) {error_msg}")
            return {
                "success": False, 
                "error": error_msg,
                "total_images_processed": len(image_paths),
                "successful_images": len(successful_results)
            }

        # **ä¿®å¤æ ¸å¿ƒé—®é¢˜**: é¢å¤–ä¿å­˜ä¸€ä¸ªå›ºå®šåç§°çš„åˆå¹¶æ–‡ä»¶ä¾›ä¸‹æ¸¸ä½¿ç”¨
        try:
            # æ„å»ºåˆå¹¶ç»“æœæ•°æ®
            merged_data = {
                "meta": {
                    "ocr_id": str(drawing_id),
                    "drawing_id": drawing_id,
                    "processing_time": sum(r.get('statistics', {}).get('processing_time', 0) for r in successful_results),
                    "total_images": len(image_paths),
                    "successful_images": len(successful_results),
                    "processing_method": "paddle_ocr_batch_merged"
                },
                "merged_ocr_result": {
                    "total_text_regions": sum(len(r.get('text_regions', [])) for r in successful_results),
                    "text_regions": [region for res in successful_results for region in res.get("text_regions", [])],
                    "all_text": '\n'.join([res.get('all_text', '') for res in successful_results if res.get('all_text')]),
                    "statistics": {
                        "total_regions": sum(len(r.get('text_regions', [])) for r in successful_results),
                        "avg_confidence": sum(r.get('statistics', {}).get('avg_confidence', 0) for r in successful_results) / len(successful_results) if successful_results else 0,
                        "processing_time": sum(r.get('statistics', {}).get('processing_time', 0) for r in successful_results)
                    }
                }
            }
            
            # ä¿å­˜ä¸ºå›ºå®šåç§°çš„merged_result.json
            merged_content = json.dumps(merged_data, ensure_ascii=False, indent=2)
            
            # ä¸Šä¼ åˆå¹¶ç»“æœ
            result_upload = self.storage_service.upload_content_sync(
                content=merged_content,
                s3_key=f"ocr_results/{drawing_id}/merged_result.json",
                content_type="application/json"
            )
            
            if result_upload.get("success"):
                logger.info(f"âœ… é¢å¤–ä¿å­˜åˆå¹¶ç»“æœä¸º merged_result.json")
                # æ·»åŠ åˆå¹¶ç»“æœä¿¡æ¯åˆ°è¿”å›å€¼
                save_result_info["merged_result"] = {
                    "s3_key": f"ocr_results/{drawing_id}/merged_result.json",
                    "s3_url": result_upload.get("final_url"),
                    "storage_method": result_upload.get("storage_method")
                }
            else:
                logger.warning(f"âš ï¸ ä¿å­˜åˆå¹¶ç»“æœå¤±è´¥: {result_upload.get('error')}")
                
        except Exception as merge_e:
            logger.warning(f"âš ï¸ ä¿å­˜é¢å¤–åˆå¹¶æ–‡ä»¶å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {merge_e}")

        logger.info(f"âœ… (Batch OCR) ç»“æœå·²æˆåŠŸä¸Šä¼ åˆ° S3ã€‚JSON å’Œ TXT æ–‡ä»¶å‡å·²ä¿å­˜ã€‚")

        # ä¿®å¤: å®‰å…¨åœ°è®¡ç®—æ€»åŒºåŸŸæ•°
        total_regions = 0
        for r in successful_results:
            if isinstance(r, dict) and r.get('statistics'):
                total_regions += r['statistics'].get('total_regions', 0)

        return {
            "success": True,
            "result_s3_key": save_result_info.get("json_result", {}).get("s3_key"), # ä¸»ç»“æœæ˜¯JSON
            "result_s3_url": save_result_info.get("json_result", {}).get("s3_url"),
            "txt_result_s3_key": save_result_info.get("txt_result", {}).get("s3_key"), # TXTç»“æœçš„key
            "merged_result_s3_key": save_result_info.get("merged_result", {}).get("s3_key"), # æ–°å¢ï¼šåˆå¹¶ç»“æœçš„key
            "storage_result": save_result_info.get("merged_result", {}),  # æ·»åŠ å­˜å‚¨ç»“æœä¿¡æ¯ï¼Œç”¨äºä¸‹æ¸¸ä½¿ç”¨
            "total_images_processed": len(image_paths),
            "successful_images": len(successful_results),
            "total_text_regions_found": total_regions,
        }

    # --- Image Pre-processing ---
    def _enhance_image_for_ocr(self, image_path: str) -> str:
        """
        å¯¹å›¾åƒè¿›è¡ŒOCRä¼˜åŒ–é¢„å¤„ç†ï¼ŒåŒ…æ‹¬è‡ªåŠ¨resizeå’Œè´¨é‡å¢å¼º
        """
        if not image_preprocessor:
            logger.warning("âš ï¸ å›¾åƒé¢„å¤„ç†å™¨æœªå¯ç”¨ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
            return image_path
        
        try:
            logger.info(f"ğŸ”§ å¼€å§‹OCRå›¾åƒé¢„å¤„ç†: {image_path}")
            
            # è·å–å›¾åƒä¿¡æ¯ç”¨äºæ—¥å¿—
            image_info = image_preprocessor.get_image_info(image_path)
            logger.info(f"ğŸ“Š åŸå§‹å›¾åƒä¿¡æ¯: {image_info}")
            
            # è‡ªåŠ¨è°ƒæ•´å›¾åƒå°ºå¯¸å’Œè´¨é‡
            optimized_path = image_preprocessor.auto_resize_for_ocr(image_path)
            
            if optimized_path != image_path:
                # è·å–ä¼˜åŒ–åçš„å›¾åƒä¿¡æ¯
                optimized_info = image_preprocessor.get_image_info(optimized_path)
                logger.info(f"âœ¨ ä¼˜åŒ–åå›¾åƒä¿¡æ¯: {optimized_info}")
            
            return optimized_path
            
        except Exception as e:
            logger.error(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ: {e}", exc_info=True)
            return image_path

    # --- Result Post-processing ---
    def _process_ocr_result(self, result: List[List]) -> Dict[str, Any]:
        """å°†PaddleOCRåŸå§‹è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–çš„å­—å…¸ã€‚"""
        text_regions = []
        all_text_list = []
        
        # --- FIX: Handle nested list structure from PaddleOCR ---
        # PaddleOCR can return results inside an extra list, e.g., [[...results...]]
        actual_results = result
        if result and isinstance(result, list) and len(result) == 1 and isinstance(result[0], list):
             actual_results = result[0]
        # --- END FIX ---
        
        for i, line in enumerate(actual_results):
            if line is None:
                continue
            
            try:
                points, (text, confidence) = line
                bbox = self._calculate_bbox_info(points)
                
                region_data = {
                    'id': i,
                    'text': text,
                    'confidence': confidence,
                    'bbox_xyxy': bbox,
                    'type_analysis': self._analyze_text_type(text)
                }
                text_regions.append(region_data)
                all_text_list.append(text)
            except Exception as e:
                logger.error(f"Failed to process a single OCR line: {line}. Error: {e}", exc_info=True)

        statistics = self._calculate_statistics(text_regions)
        
        return {
            "text_regions": text_regions,
            "all_text": "\n".join(all_text_list),
            "statistics": statistics
        }

    def _calculate_bbox_info(self, points: List[List[float]]) -> Dict[str, float]:
        """ä»åæ ‡ç‚¹è®¡ç®—è¾¹ç•Œæ¡†ä¿¡æ¯ã€‚"""
        x_coords = [p[0] for p in points]
        y_coords = [p[1] for p in points]
        x_min, x_max = min(x_coords), max(x_coords)
        y_min, y_max = min(y_coords), max(y_coords)
        return {'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max}

    def _analyze_text_type(self, text: str) -> Dict[str, bool]:
        """åˆ†ææ–‡æœ¬ç±»å‹ï¼ˆæ•°å­—ã€å­—æ¯ç­‰ï¼‰ã€‚"""
        return {
            'is_numeric': text.replace('.', '', 1).isdigit(),
            'is_alphanumeric': text.isalnum(),
            'contains_letters': any(c.isalpha() for c in text),
            'contains_digits': any(c.isdigit() for c in text)
        }

    def _calculate_statistics(self, text_regions: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—è¯†åˆ«ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        if not text_regions:
            return {'total_regions': 0, 'avg_confidence': 0}
        
        total_regions = len(text_regions)
        avg_confidence = sum(r['confidence'] for r in text_regions) / total_regions
        return {'total_regions': total_regions, 'avg_confidence': avg_confidence}

    def _apply_construction_text_correction(self, ocr_result: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨å»ºç­‘é¢†åŸŸçš„æ–‡æœ¬æ ¡æ­£è§„åˆ™ã€‚"""
        
        # å»ºç­‘æœ¯è¯­çº é”™å­—å…¸
        correction_dict = {
            # æ„ä»¶ç±»å‹çº é”™
            'KZ': ['XZ', 'KJ', 'K2', 'KZ1', 'Kä¹™'],  # æŸ±çš„å¸¸è§é”™è¯¯
            'GL': ['CL', 'G1', 'OL', '6L', 'GL1'],    # æ¢çš„å¸¸è§é”™è¯¯  
            'JC': ['JO', 'IC', 'JG', 'J0', 'JC1'],    # åŸºç¡€çš„å¸¸è§é”™è¯¯
            'WQ': ['MQ', 'W0', 'VQ', 'WO'],           # å¢™çš„å¸¸è§é”™è¯¯
            'LT': ['L7', 'LI', '17', 'LT1'],          # æ¥¼æ¢¯çš„å¸¸è§é”™è¯¯
            'SZ': ['52', 'S2', 'SZ1', 'SJ'],         # å‰ªåŠ›å¢™çš„å¸¸è§é”™è¯¯
            
            # é’¢ç­‹ç¬¦å·çº é”™
            'Î¦': ['Ï†', 'O', '0', '@', 'Î¸', 'Ğ¤'],     # é’¢ç­‹ç¬¦å·
            '@': ['a', 'A', '&', 'Î±', 'Ğ¾'],          # é—´è·ç¬¦å·
            'C': ['G', 'c', 'â„ƒ', 'C2'],               # æ··å‡åœŸç­‰çº§
            
            # æ•°å­—çº é”™
            '1': ['I', 'l', '|', '!'],
            '0': ['O', 'o', 'Â°'],
            '2': ['Z', 'z'],
            '5': ['S', 's'],
            '6': ['G', 'b'],
            '8': ['B'],
            '9': ['g', 'q'],
            
            # å•ä½çº é”™
            'mm': ['mm', 'MM', 'rnm', 'nn'],
            'm': ['rn', 'nn', 'M'],
            'MPa': ['MPA', 'Mpa', 'mpa', 'Mfa'],
        }
        
        # æ ¼å¼åŒ–è§„åˆ™
        format_rules = [
            # æ„ä»¶ç¼–å·æ ¼å¼åŒ–: KZ-1, GL-2 ç­‰
            (r'([KGJLWSZBC])([Z|L|C|Q|T])[-\s]*(\d+)', r'\1\2-\3'),
            # é’¢ç­‹è§„æ ¼æ ¼å¼åŒ–: Î¦12@200
            (r'[Î¦Ï†O0](\d+)[@aA&](\d+)', r'Î¦\1@\2'),
            # å°ºå¯¸æ ¼å¼åŒ–: 350x500, 350Ã—500
            (r'(\d+)[xÃ—*](\d+)', r'\1Ã—\2'),
            # æ··å‡åœŸç­‰çº§: C30, C35 ç­‰
            (r'[CcGg](\d+)', r'C\1'),
        ]
        
        corrected_count = 0
        
        for region in ocr_result.get('text_regions', []):
            original_text = region.get('text', '')
            corrected_text = original_text
            
            # åº”ç”¨è¯å…¸çº é”™ï¼ˆæ›´ç²¾ç¡®çš„åŒ¹é…ï¼‰
            for correct, errors in correction_dict.items():
                for error in errors:
                    # åªåœ¨æ•´è¯åŒ¹é…æˆ–æ„ä»¶ç¼–å·ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ›¿æ¢
                    if error == corrected_text or (error in corrected_text and len(error) >= 2):
                        corrected_text = corrected_text.replace(error, correct)
            
            # åº”ç”¨æ ¼å¼åŒ–è§„åˆ™
            import re
            for pattern, replacement in format_rules:
                corrected_text = re.sub(pattern, replacement, corrected_text)
            
            # å¦‚æœæœ‰ä¿®æ”¹ï¼Œè®°å½•åŸå§‹æ–‡æœ¬
            if corrected_text != original_text:
                region['original_text'] = original_text
                region['text'] = corrected_text
                region['corrected'] = True
                region['correction_type'] = 'construction_terminology'
                corrected_count += 1
                logger.info(f"ğŸ”§ æ–‡æœ¬çº é”™: '{original_text}' â†’ '{corrected_text}'")
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if corrected_count > 0:
            ocr_result['correction_applied'] = True
            ocr_result['corrections_count'] = corrected_count
            logger.info(f"âœ… åº”ç”¨å»ºç­‘æœ¯è¯­çº é”™: {corrected_count} å¤„ä¿®æ­£")
        
        return ocr_result
    
    def _format_raw_result_as_txt(self, raw_paddle_data: List[List], image_path: str) -> str:
        """
        å°†PaddleOCRåŸå§‹ç»“æœæ ¼å¼åŒ–ä¸ºå¯è¯»çš„TXTæ–‡æœ¬
        
        Args:
            raw_paddle_data: PaddleOCRåŸå§‹è¾“å‡º
            image_path: å›¾åƒè·¯å¾„
            
        Returns:
            str: æ ¼å¼åŒ–çš„TXTå†…å®¹
        """
        try:
            lines = []
            lines.append("="*60)
            lines.append("PaddleOCRè¯†åˆ«ç»“æœ")
            lines.append("="*60)
            lines.append(f"å›¾åƒæ–‡ä»¶: {Path(image_path).name}")
            lines.append(f"è¯†åˆ«æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            lines.append("")
            
            # å¤„ç†åµŒå¥—åˆ—è¡¨ç»“æ„
            actual_results = raw_paddle_data
            if raw_paddle_data and isinstance(raw_paddle_data, list) and len(raw_paddle_data) == 1 and isinstance(raw_paddle_data[0], list):
                actual_results = raw_paddle_data[0]
            
            if not actual_results:
                lines.append("æœªæ£€æµ‹åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹")
                lines.append("")
                lines.append("="*60)
                return "\n".join(lines)
            
            lines.append(f"æ£€æµ‹åˆ° {len(actual_results)} ä¸ªæ–‡æœ¬åŒºåŸŸ:")
            lines.append("")
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
            sorted_results = sorted(actual_results, key=lambda x: x[1][1] if len(x) > 1 and len(x[1]) > 1 and x[1][1] is not None else 0, reverse=True)
            
            for i, line in enumerate(sorted_results, 1):
                if line is None:
                    continue
                
                try:
                    points, (text, confidence) = line
                    
                    # è®¡ç®—è¾¹ç•Œæ¡†
                    x_coords = [p[0] for p in points]
                    y_coords = [p[1] for p in points]
                    x_min, x_max = min(x_coords), max(x_coords)
                    y_min, y_max = min(y_coords), max(y_coords)
                    
                    # æ ¼å¼åŒ–è¾“å‡º
                    lines.append(f"[{i:02d}] æ–‡æœ¬: {text}")
                    lines.append(f"     ç½®ä¿¡åº¦: {confidence:.3f}")
                    lines.append(f"     ä½ç½®: ({x_min:.0f}, {y_min:.0f}) - ({x_max:.0f}, {y_max:.0f})")
                    lines.append(f"     å°ºå¯¸: {x_max-x_min:.0f} x {y_max-y_min:.0f}")
                    lines.append("")
                    
                except Exception as e:
                    lines.append(f"[{i:02d}] è§£æé”™è¯¯: {str(e)}")
                    lines.append("")
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            lines.append("-"*40)
            lines.append("ç»Ÿè®¡ä¿¡æ¯:")
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            confidences = []
            texts = []
            for line in actual_results:
                if line and len(line) > 1 and len(line[1]) > 1:
                    text, confidence = line[1]
                    confidences.append(confidence)
                    texts.append(text)
            
            if confidences:
                avg_confidence = sum(confidences) / len(confidences)
                max_confidence = max(confidences)
                min_confidence = min(confidences)
                
                lines.append(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                lines.append(f"æœ€é«˜ç½®ä¿¡åº¦: {max_confidence:.3f}")
                lines.append(f"æœ€ä½ç½®ä¿¡åº¦: {min_confidence:.3f}")
                lines.append(f"æ€»å­—ç¬¦æ•°: {sum(len(text) for text in texts)}")
                lines.append(f"æ€»æ–‡æœ¬æ•°: {len(texts)}")
            
            lines.append("")
            lines.append("-"*40)
            lines.append("çº¯æ–‡æœ¬å†…å®¹:")
            lines.append("-"*40)
            
            # æ·»åŠ çº¯æ–‡æœ¬å†…å®¹ï¼ˆæŒ‰ä½ç½®ä»ä¸Šåˆ°ä¸‹ï¼Œä»å·¦åˆ°å³æ’åºï¼‰
            text_with_positions = []
            for line in actual_results:
                if line and len(line) > 1:
                    try:
                        points, (text, confidence) = line
                        y_coords = [p[1] for p in points]
                        x_coords = [p[0] for p in points]
                        avg_y = sum(y_coords) / len(y_coords)
                        avg_x = sum(x_coords) / len(x_coords)
                        text_with_positions.append((avg_y, avg_x, text))
                    except:
                        continue
            
            # æŒ‰Yåæ ‡æ’åºï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ï¼Œç„¶åæŒ‰Xåæ ‡æ’åºï¼ˆä»å·¦åˆ°å³ï¼‰
            text_with_positions.sort(key=lambda x: (x[0], x[1]))
            
            for _, _, text in text_with_positions:
                lines.append(text)
            
            lines.append("")
            lines.append("="*60)
            lines.append("è¯´æ˜:")
            lines.append("- æ–‡æœ¬æŒ‰ç½®ä¿¡åº¦ä»é«˜åˆ°ä½æ’åº")
            lines.append("- ä½ç½®åæ ‡ä¸ºåƒç´ åæ ‡ç³»")
            lines.append("- çº¯æ–‡æœ¬å†…å®¹æŒ‰å›¾åƒä½ç½®ä»ä¸Šåˆ°ä¸‹ã€ä»å·¦åˆ°å³æ’åº")
            lines.append("="*60)
            
            return "\n".join(lines)
            
        except Exception as e:
            logger.error(f"âŒ æ ¼å¼åŒ–TXTç»“æœå¤±è´¥: {e}", exc_info=True)
            # è¿”å›ç®€åŒ–ç‰ˆæœ¬
            return f"PaddleOCRè¯†åˆ«ç»“æœ\nå›¾åƒ: {Path(image_path).name}\næ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\næ ¼å¼åŒ–å¤±è´¥: {str(e)}"
        
    # --- Fallback & Mocking ---
    def _mock_recognition_result(self, error: str = "Unknown error") -> Dict[str, Any]:
        """åœ¨OCRå¤±è´¥æ—¶è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„ã€ç»“æ„ä¸€è‡´çš„ç»“æœã€‚"""
        logger.warning(f"Using mock OCR result due to error: {error}")
        mock_data = self._mock_raw_paddle_result()
        result = self._process_ocr_result(mock_data)
        result["mock_mode"] = True
        result["engine_status"] = "æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆé”™è¯¯æ¢å¤ï¼‰"
        result["error"] = error
        result["texts"] = []
        result["success"] = False  # æ˜ç¡®æ ‡è®°ä¸ºå¤±è´¥
        return result

    def _mock_raw_paddle_result(self) -> List[List]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„PaddleOCRåŸå§‹è¾“å‡ºã€‚"""
        return [
            [[[10, 10], [100, 10], [100, 30], [10, 30]], ('KZ-1 300x600', 0.95)],
            [[[10, 40], [120, 40], [120, 60], [10, 60]], ('B=200 H=400', 0.92)]
        ]

    # --- Data Persistence ---
    def _save_complete_raw_result_to_sealos(self, raw_paddle_data: List[List], ocr_result: Dict[str, Any], image_path: str, drawing_id: str = None) -> Dict[str, Any]:
        """
        æ„å»ºåŒ…å«å®Œæ•´OCRåŸå§‹æ•°æ®çš„JSONå’ŒTXTï¼Œå¹¶ä¸Šä¼ åˆ°S3/Sealosã€‚
        
        Args:
            raw_paddle_data: PaddleOCRåŸå§‹è¾“å‡ºæ•°æ®
            ocr_result: å¤„ç†åçš„OCRç»“æœ
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            drawing_id: å›¾çº¸IDï¼Œç”¨äºåˆ†ç±»å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
        """
        storage_service = self.dual_storage
        if not storage_service:
            logger.warning("å­˜å‚¨æœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ä¿å­˜åŸå§‹OCRç»“æœ")
            return {"saved": False, "error": "Storage service not available"}

        try:
            # ğŸ”§ ä¿®å¤ï¼šç”Ÿæˆå”¯ä¸€çš„file_id
            import uuid
            import time
            from pathlib import Path
            
            # åŸºäºå›¾åƒæ–‡ä»¶åå’Œæ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€ID
            image_name = Path(image_path).stem  # å»é™¤æ‰©å±•åçš„æ–‡ä»¶å
            timestamp = int(time.time())
            file_id = f"ocr_{image_name}_{timestamp}_{str(uuid.uuid4())[:8]}"
            
            # å‡†å¤‡æ•°æ®å’Œæ–‡ä»¶å
            json_filename = f"{file_id}.json"
            txt_filename = f"{file_id}.txt"
            image_filename = f"{file_id}{Path(image_path).suffix}"
            folder_path = f"ocr_results/{drawing_id}"

            # 1. ä¿å­˜JSONç»“æœ
            json_data = json.dumps(ocr_result, ensure_ascii=False, indent=2).encode('utf-8')
            json_s3_key = f"{folder_path}/{json_filename}"
            json_upload_result = storage_service.upload_file_sync(
                file_obj=BytesIO(json_data),
                s3_key=json_s3_key,
                content_type="application/json"
            )
            
            # 2. ä¿å­˜TXTæ ¼å¼çš„åŸå§‹è¯†åˆ«ç»“æœ
            txt_content = self._format_raw_result_as_txt(raw_paddle_data, image_path)
            txt_s3_key = f"{folder_path}/{txt_filename}"
            txt_upload_result = storage_service.upload_file_sync(
                file_obj=BytesIO(txt_content.encode('utf-8')),
                s3_key=txt_s3_key,
                content_type="text/plain"
            )

            # 3. ä¿å­˜åŸå§‹å›¾ç‰‡
            with open(image_path, "rb") as f:
                image_data = f.read()
            
            image_s3_key = f"{folder_path}/{image_filename}"
            image_upload_result = storage_service.upload_file_sync(
                file_obj=BytesIO(image_data),
                s3_key=image_s3_key,
                content_type="image/png" # å‡è®¾ä¸ºpngï¼Œæˆ–ä»è·¯å¾„æ¨æ–­
            )

            final_result = {
                "saved": True,
                "json_result": {
                    "s3_key": json_s3_key,
                    "s3_url": json_upload_result.get("final_url"),
                    "bucket": json_upload_result.get("bucket")
                },
                "txt_result": {
                    "s3_key": txt_s3_key,
                    "s3_url": txt_upload_result.get("final_url"),
                    "bucket": txt_upload_result.get("bucket")
                },
                "image_result": {
                    "s3_key": image_s3_key,
                    "s3_url": image_upload_result.get("final_url"),
                    "bucket": image_upload_result.get("bucket")
                },
                "error": None
            }

            return final_result

        except Exception as e:
            logger.error(f"Failed to upload raw OCR data to Sealos: {e}")
            return {"saved": False, "error": str(e)}

    def recognize_text_from_image_obj(self, image_data: bytes, save_to_storage: bool = False, drawing_id: str = None) -> Dict[str, Any]:
        """ä»å†…å­˜ä¸­çš„å›¾åƒæ•°æ®æ‰§è¡ŒOCR"""
        # ... existing code ...
        # This method needs to be implemented
        # ... existing code ...

# Make sure this file can be imported and initialized without errors.
# logger.info("PaddleOCR Service module loaded.") 