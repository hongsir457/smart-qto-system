#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†è§‰åˆ†æå™¨ - è´Ÿè´£å›¾åƒçš„è§†è§‰åˆ†æå’Œå¤šè½®å¯¹è¯
"""
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)

class VisionAnalyzer:
    """
    è´Ÿè´£å›¾åƒçš„è§†è§‰åˆ†æå’Œå¤šè½®å¯¹è¯å¤„ç†
    """
    
    def __init__(self, client, interaction_logger, prompt_builder):
        """åˆå§‹åŒ–è§†è§‰åˆ†æå™¨"""
        self.client = client
        self.interaction_logger = interaction_logger
        self.prompt_builder = prompt_builder
        logger.info("âœ… VisionAnalyzer initialized")
    
    def prepare_images(self, image_paths: List[str]) -> List[Dict]:
        """å‡†å¤‡å›¾åƒæ•°æ®ä¾›Vision APIä½¿ç”¨"""
        import base64
        encoded_images = []
        
        for image_path in image_paths:
            try:
                with open(image_path, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    encoded_images.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"
                        }
                    })
                logger.info(f"âœ… ç¼–ç å›¾ç‰‡: {image_path}")
            except Exception as e:
                logger.error(f"âŒ å›¾ç‰‡ç¼–ç å¤±è´¥ {image_path}: {e}")
        
        return encoded_images
    
    def execute_multi_turn_analysis(self, encoded_images: List[Dict], 
                                  task_id: str = None, drawing_id: int = None) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šè½®åˆ†æ"""
        if not self.client:
            return {"error": "OpenAI client not available"}
        
        logger.info("ğŸ”„ å¼€å§‹å¤šè½®Visionåˆ†æ...")
        
        try:
            # Turn 1: åŸºç¡€åˆ†æ
            turn1_result = self._execute_vision_step(
                "Turn1_åŸºç¡€åˆ†æ",
                self.prompt_builder.build_system_prompt(),
                [{"type": "text", "text": "è¯·åˆ†æè¿™äº›å»ºç­‘å›¾çº¸å›¾åƒï¼Œè¯†åˆ«ç»“æ„æ„ä»¶å¹¶æå–åŸºæœ¬ä¿¡æ¯ã€‚"}] + encoded_images,
                task_id, drawing_id
            )
            
            if "error" in turn1_result:
                return turn1_result
            
            # Turn 2: è¯¦ç»†åˆ†æ
            context_prompt = self.prompt_builder.build_multi_turn_prompt(2, turn1_result.get("response"))
            turn2_result = self._execute_vision_step(
                "Turn2_è¯¦ç»†åˆ†æ",
                context_prompt,
                [{"type": "text", "text": "åŸºäºç¬¬ä¸€è½®åˆ†æï¼Œè¯·è¿›ä¸€æ­¥å®Œå–„æ„ä»¶ä¿¡æ¯å’Œå°ºå¯¸æ•°æ®ã€‚"}] + encoded_images,
                task_id, drawing_id
            )
            
            if "error" in turn2_result:
                return turn1_result  # è¿”å›ç¬¬ä¸€è½®ç»“æœä½œä¸ºå¤‡é€‰
            
            # åˆå¹¶ç»“æœ
            final_result = self._merge_multi_turn_results([turn1_result, turn2_result])
            logger.info("âœ… å¤šè½®åˆ†æå®Œæˆ")
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ å¤šè½®åˆ†æå¼‚å¸¸: {e}")
            return {"error": str(e)}
    
    def execute_multi_turn_analysis_with_context(self, encoded_images: List[Dict], 
                                               task_id: str = None, drawing_id: int = None) -> Dict[str, Any]:
        """æ‰§è¡Œå¸¦ä¸Šä¸‹æ–‡çš„å¤šè½®åˆ†æï¼ˆ5æ­¥æ³•ï¼‰"""
        if not self.client:
            return {"error": "OpenAI client not available"}
        
        logger.info("ğŸ”„ å¼€å§‹5æ­¥ä¸Šä¸‹æ–‡åˆ†æ...")
        analysis_results = {}
        
        try:
            # åˆå§‹åŒ–å¯¹è¯æ¶ˆæ¯
            conversation_messages = []
            
            # Step 1: æå–å›¾çº¸åŸºæœ¬ä¿¡æ¯
            step1_result = self._execute_contextual_step_1(
                conversation_messages, encoded_images, task_id, drawing_id
            )
            analysis_results["step1_drawing_info"] = step1_result
            
            # Step 2: è¯†åˆ«æ„ä»¶ç¼–å·
            step2_result = self._execute_contextual_step_2(
                conversation_messages, encoded_images, step1_result, task_id, drawing_id
            )
            analysis_results["step2_component_ids"] = step2_result
            
            # Step 3: ç»Ÿè®¡æ„ä»¶æ•°é‡
            step3_result = self._execute_contextual_step_3(
                conversation_messages, encoded_images, step2_result, task_id, drawing_id
            )
            analysis_results["step3_component_counts"] = step3_result
            
            # Step 4: æå–ä½ç½®ä¿¡æ¯
            step4_result = self._execute_contextual_step_4(
                conversation_messages, encoded_images, step3_result, task_id, drawing_id
            )
            analysis_results["step4_positions"] = step4_result
            
            # Step 5: æå–å±æ€§ä¿¡æ¯
            step5_result = self._execute_contextual_step_5(
                conversation_messages, encoded_images, step4_result, task_id, drawing_id
            )
            analysis_results["step5_attributes"] = step5_result
            
            # ç»¼åˆç”Ÿæˆæœ€ç»ˆQTOæ•°æ®
            final_qto = self._synthesize_qto_data(analysis_results)
            logger.info("âœ… 5æ­¥ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ")
            return final_qto
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡åˆ†æå¼‚å¸¸: {e}")
            return {"error": str(e)}
    
    def _execute_vision_step(self, step_name: str, system_prompt: str, user_content: List[Dict], 
                           task_id: str = None, drawing_id: int = None) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªVisionåˆ†ææ­¥éª¤"""
        try:
            logger.info(f"ğŸ“¤ æ‰§è¡ŒVisionæ­¥éª¤: {step_name}")
            
            from app.core.config import settings
            
            response = self.client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=settings.OPENAI_MAX_TOKENS,
                response_format={"type": "json_object"}
            )
            
            response_content = response.choices[0].message.content
            
            # è®°å½•äº¤äº’
            if self.interaction_logger:
                try:
                    self.interaction_logger.log_api_call(
                        session_id=f"{task_id}_{step_name}",
                        step_name=step_name,
                        request_data={
                            "model": settings.OPENAI_MODEL,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": "å›¾åƒåˆ†æè¯·æ±‚"}
                            ]
                        },
                        response_data={"content": response_content},
                        task_id=task_id,
                        drawing_id=drawing_id
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ äº¤äº’è®°å½•å¤±è´¥: {e}")
            
            # è§£æå“åº”
            try:
                parsed_response = json.loads(response_content)
                logger.info(f"âœ… {step_name} æ‰§è¡ŒæˆåŠŸ")
                return {"success": True, "response": parsed_response}
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ {step_name} JSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹")
                return {"success": True, "response": response_content, "raw": True}
            
        except Exception as e:
            logger.error(f"âŒ {step_name} æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _execute_contextual_step_1(self, conversation_messages: List[Dict], 
                                  encoded_images: List[Dict], task_id: str, drawing_id: int) -> Dict[str, Any]:
        """æ‰§è¡Œç¬¬1æ­¥ï¼šæå–å›¾çº¸åŸºæœ¬ä¿¡æ¯"""
        system_prompt = """
ä½ æ˜¯ä¸“ä¸šçš„å»ºç­‘å›¾çº¸åˆ†æå¸ˆã€‚è¯·ä»”ç»†åˆ†æå›¾çº¸å¹¶æå–ä»¥ä¸‹åŸºæœ¬ä¿¡æ¯ï¼š

1. é¡¹ç›®åç§°ï¼ˆä»æ ‡é¢˜æ ä¸­æå–ï¼Œä¸è¦ç¼–é€ ï¼‰
2. å›¾çº¸ç¼–å·
3. è®¾è®¡å•ä½  
4. å›¾çº¸æ¯”ä¾‹
5. ç»˜åˆ¶æ—¥æœŸ
6. å›¾çº¸ç±»å‹ï¼ˆç»“æ„å›¾ã€å»ºç­‘å›¾ç­‰ï¼‰

è¦æ±‚ï¼š
- ä¸¥æ ¼æŒ‰ç…§å›¾çº¸å®é™…å†…å®¹æå–
- å¦‚æœä¿¡æ¯ä¸æ¸…æ™°æˆ–æ— æ³•è¯†åˆ«ï¼Œè¯·æ ‡æ³¨"ä¿¡æ¯ä¸æ˜ç¡®"
- ç»å¯¹ä¸è¦ç¼–é€ æˆ–å‡è®¾ä»»ä½•ä¿¡æ¯
- è¿”å›æ ‡å‡†JSONæ ¼å¼
"""
        
        user_content = [
            {"type": "text", "text": "è¯·åˆ†æå›¾çº¸æ ‡é¢˜æ å’ŒåŸºæœ¬ä¿¡æ¯"}
        ] + encoded_images
        
        conversation_messages.append({"role": "system", "content": system_prompt})
        conversation_messages.append({"role": "user", "content": user_content})
        
        return self._make_contextual_api_call("Step1_å›¾çº¸ä¿¡æ¯", conversation_messages, task_id, drawing_id)
    
    def _execute_contextual_step_2(self, conversation_messages: List[Dict],
                                  encoded_images: List[Dict], previous_results: Dict,
                                  task_id: str, drawing_id: int) -> Dict[str, Any]:
        """æ‰§è¡Œç¬¬2æ­¥ï¼šè¯†åˆ«æ„ä»¶ç¼–å·"""
        context = self.prompt_builder.build_context_prompt("step2", previous_results)
        
        system_prompt = f"""
{context}

ç°åœ¨è¯·è¯†åˆ«å›¾çº¸ä¸­æ‰€æœ‰ç»“æ„æ„ä»¶çš„ç¼–å·ï¼š

1. æ‰«ææ•´ä¸ªå›¾çº¸ï¼Œå¯»æ‰¾æ„ä»¶æ ‡æ³¨
2. è¯†åˆ«æŸ±å­ç¼–å·ï¼ˆå¦‚KZ1, KZ2ç­‰ï¼‰
3. è¯†åˆ«æ¢ç¼–å·ï¼ˆå¦‚L1, L2ç­‰ï¼‰  
4. è¯†åˆ«æ¿ç¼–å·ï¼ˆå¦‚B1, B2ç­‰ï¼‰
5. è¯†åˆ«å…¶ä»–æ„ä»¶ç¼–å·

è¦æ±‚ï¼š
- åªè®°å½•å›¾çº¸ä¸Šå®é™…å­˜åœ¨çš„ç¼–å·
- æŒ‰æ„ä»¶ç±»å‹åˆ†ç»„
- ä¸è¦ç”Ÿæˆè§„å¾‹æ€§ç¼–å·åºåˆ—
- è¿”å›JSONæ ¼å¼
"""
        
        user_content = [
            {"type": "text", "text": "è¯·è¯†åˆ«æ‰€æœ‰æ„ä»¶ç¼–å·"}
        ] + encoded_images
        
        conversation_messages.append({"role": "user", "content": user_content})
        
        return self._make_contextual_api_call("Step2_æ„ä»¶ç¼–å·", conversation_messages, task_id, drawing_id)
    
    def _execute_contextual_step_3(self, conversation_messages: List[Dict],
                                  encoded_images: List[Dict], previous_results: Dict,
                                  task_id: str, drawing_id: int) -> Dict[str, Any]:
        """æ‰§è¡Œç¬¬3æ­¥ï¼šç»Ÿè®¡æ„ä»¶æ•°é‡"""
        context = self.prompt_builder.build_context_prompt("step3", previous_results)
        
        system_prompt = f"""
{context}

åŸºäºå·²è¯†åˆ«çš„æ„ä»¶ç¼–å·ï¼Œè¯·ç»Ÿè®¡å„ç±»æ„ä»¶çš„æ•°é‡ï¼š

1. ç»Ÿè®¡æ¯ç§æ„ä»¶ç¼–å·çš„å‡ºç°æ¬¡æ•°
2. åŒºåˆ†ä¸åŒè§„æ ¼çš„åŒç±»æ„ä»¶
3. ç”Ÿæˆæ„ä»¶æ•°é‡æ±‡æ€»è¡¨
4. éªŒè¯ç»Ÿè®¡çš„å‡†ç¡®æ€§

è¦æ±‚ï¼š
- åŸºäºå›¾çº¸å®é™…æƒ…å†µç»Ÿè®¡
- é¿å…é‡å¤è®¡æ•°
- è¿”å›JSONæ ¼å¼çš„ç»Ÿè®¡ç»“æœ
"""
        
        user_content = [
            {"type": "text", "text": "è¯·ç»Ÿè®¡æ„ä»¶æ•°é‡"}
        ] + encoded_images
        
        conversation_messages.append({"role": "user", "content": user_content})
        
        return self._make_contextual_api_call("Step3_æ„ä»¶ç»Ÿè®¡", conversation_messages, task_id, drawing_id)
    
    def _execute_contextual_step_4(self, conversation_messages: List[Dict],
                                  encoded_images: List[Dict], previous_results: Dict,
                                  task_id: str, drawing_id: int) -> Dict[str, Any]:
        """æ‰§è¡Œç¬¬4æ­¥ï¼šæå–ä½ç½®ä¿¡æ¯"""
        context = self.prompt_builder.build_context_prompt("step4", previous_results)
        
        system_prompt = f"""
{context}

è¯·æå–æ„ä»¶çš„ä½ç½®ä¿¡æ¯ï¼š

1. ç¡®å®šæ„ä»¶åœ¨å›¾çº¸ä¸­çš„åæ ‡ä½ç½®
2. è¯†åˆ«æ„ä»¶çš„å¸ƒç½®æ¨¡å¼
3. åˆ†ææ„ä»¶ä¹‹é—´çš„å…³ç³»
4. è®°å½•æ¥¼å±‚æˆ–åŒºåŸŸä¿¡æ¯

è¦æ±‚ï¼š
- åŸºäºå›¾çº¸å®é™…å¸ƒç½®
- æä¾›ç›¸å¯¹ä½ç½®å…³ç³»
- è¿”å›JSONæ ¼å¼
"""
        
        user_content = [
            {"type": "text", "text": "è¯·æå–æ„ä»¶ä½ç½®ä¿¡æ¯"}
        ] + encoded_images
        
        conversation_messages.append({"role": "user", "content": user_content})
        
        return self._make_contextual_api_call("Step4_ä½ç½®ä¿¡æ¯", conversation_messages, task_id, drawing_id)
    
    def _execute_contextual_step_5(self, conversation_messages: List[Dict],
                                  encoded_images: List[Dict], previous_results: Dict,
                                  task_id: str, drawing_id: int) -> Dict[str, Any]:
        """æ‰§è¡Œç¬¬5æ­¥ï¼šæå–å±æ€§ä¿¡æ¯"""
        context = self.prompt_builder.build_context_prompt("step5", previous_results)
        
        system_prompt = f"""
{context}

è¯·æå–æ„ä»¶çš„è¯¦ç»†å±æ€§ï¼š

1. æ„ä»¶å°ºå¯¸ï¼ˆé•¿ã€å®½ã€é«˜ï¼‰
2. æˆªé¢è§„æ ¼
3. ææ–™å¼ºåº¦ç­‰çº§
4. å…¶ä»–æŠ€æœ¯å‚æ•°

è¦æ±‚ï¼š
- ä»å›¾çº¸æ ‡æ³¨ä¸­æå–å®é™…æ•°å€¼
- å¦‚æ— æ˜ç¡®æ ‡æ³¨åˆ™æ ‡è®°"å¾…ç¡®è®¤"
- è¿”å›JSONæ ¼å¼
"""
        
        user_content = [
            {"type": "text", "text": "è¯·æå–æ„ä»¶å±æ€§ä¿¡æ¯"}
        ] + encoded_images
        
        conversation_messages.append({"role": "user", "content": user_content})
        
        return self._make_contextual_api_call("Step5_å±æ€§ä¿¡æ¯", conversation_messages, task_id, drawing_id)
    
    def _make_contextual_api_call(self, step_name: str, conversation_messages: List[Dict],
                                 task_id: str, drawing_id: int) -> Dict[str, Any]:
        """è¿›è¡Œå¸¦ä¸Šä¸‹æ–‡çš„APIè°ƒç”¨"""
        try:
            from app.core.config import settings
            
            # åªä½¿ç”¨æœ€æ–°çš„ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯
            messages = conversation_messages[-2:] if len(conversation_messages) >= 2 else conversation_messages
            
            response = self.client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=settings.OPENAI_MAX_TOKENS,
                response_format={"type": "json_object"}
            )
            
            response_content = response.choices[0].message.content
            
            # è®°å½•äº¤äº’
            if self.interaction_logger:
                try:
                    self.interaction_logger.log_api_call(
                        session_id=f"{task_id}_{step_name}",
                        step_name=step_name,
                        request_data={"messages": messages},
                        response_data={"content": response_content},
                        task_id=task_id,
                        drawing_id=drawing_id
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ äº¤äº’è®°å½•å¤±è´¥: {e}")
            
            # è§£æå“åº”
            try:
                parsed_response = json.loads(response_content)
                return {"success": True, "response": parsed_response}
            except json.JSONDecodeError:
                return {"success": True, "response": response_content, "raw": True}
            
        except Exception as e:
            logger.error(f"âŒ {step_name} APIè°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _merge_multi_turn_results(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆå¹¶å¤šè½®åˆ†æç»“æœ"""
        merged = {"success": True, "turns": len(results)}
        
        # æå–æ¯è½®çš„å“åº”æ•°æ®
        responses = []
        for i, result in enumerate(results):
            if result.get("success") and "response" in result:
                responses.append(result["response"])
        
        if responses:
            # ä½¿ç”¨æœ€åä¸€è½®çš„ç»“æœä½œä¸ºä¸»è¦ç»“æœ
            merged["qto_data"] = responses[-1]
            merged["all_responses"] = responses
        else:
            merged["error"] = "æ‰€æœ‰è½®æ¬¡éƒ½å¤±è´¥"
        
        return merged
    
    def _synthesize_qto_data(self, analysis_results: Dict) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æç»“æœç”ŸæˆQTOæ•°æ®"""
        try:
            # æå–å„æ­¥éª¤çš„æ•°æ®
            drawing_info = analysis_results.get("step1_drawing_info", {}).get("response", {})
            component_ids = analysis_results.get("step2_component_ids", {}).get("response", {})
            component_counts = analysis_results.get("step3_component_counts", {}).get("response", {})
            positions = analysis_results.get("step4_positions", {}).get("response", {})
            attributes = analysis_results.get("step5_attributes", {}).get("response", {})
            
            # æ„å»ºç»¼åˆQTOæ•°æ®
            qto_data = {
                "drawing_info": drawing_info,
                "components": self._build_component_list(component_ids, component_counts, positions, attributes),
                "summary": self._generate_quantity_summary_from_analysis(analysis_results),
                "analysis_steps": {
                    "step1": "å›¾çº¸ä¿¡æ¯æå–",
                    "step2": "æ„ä»¶ç¼–å·è¯†åˆ«", 
                    "step3": "æ„ä»¶æ•°é‡ç»Ÿè®¡",
                    "step4": "ä½ç½®ä¿¡æ¯æå–",
                    "step5": "å±æ€§ä¿¡æ¯æå–"
                }
            }
            
            return {"success": True, "qto_data": qto_data}
            
        except Exception as e:
            logger.error(f"âŒ QTOæ•°æ®ç»¼åˆå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _build_component_list(self, component_ids: Dict, component_counts: Dict, 
                            positions: Dict, attributes: Dict) -> List[Dict]:
        """æ„å»ºæ„ä»¶æ¸…å•"""
        components = []
        
        # éå†è¯†åˆ«çš„æ„ä»¶ç¼–å·
        for comp_type, ids in component_ids.items():
            if isinstance(ids, list):
                for comp_id in ids:
                    component = {
                        "component_id": comp_id,
                        "component_type": self._determine_component_type(comp_id),
                        "count": component_counts.get(comp_id, 1),
                        "position": positions.get(comp_id, {}),
                        "dimensions": attributes.get(comp_id, {}),
                        "source": "5æ­¥åˆ†ææ³•"
                    }
                    components.append(component)
        
        return components
    
    def _determine_component_type(self, component_id: str) -> str:
        """æ ¹æ®æ„ä»¶ç¼–å·ç¡®å®šæ„ä»¶ç±»å‹"""
        if component_id.startswith("KZ"):
            return "æ¡†æ¶æŸ±"
        elif component_id.startswith("L"):
            return "æ¢"
        elif component_id.startswith("B"):
            return "æ¿"
        elif component_id.startswith("Q"):
            return "å¢™"
        else:
            return "å…¶ä»–æ„ä»¶"
    
    def _generate_quantity_summary_from_analysis(self, analysis_results: Dict) -> Dict[str, Any]:
        """ä»åˆ†æç»“æœç”Ÿæˆå·¥ç¨‹é‡æ±‡æ€»"""
        summary = {
            "total_components": 0,
            "component_types": {},
            "analysis_quality": "good"
        }
        
        try:
            component_counts = analysis_results.get("step3_component_counts", {}).get("response", {})
            
            for comp_type, count in component_counts.items():
                if isinstance(count, (int, float)):
                    summary["total_components"] += count
                    comp_category = self._determine_component_type(comp_type)
                    summary["component_types"][comp_category] = summary["component_types"].get(comp_category, 0) + count
        
        except Exception as e:
            logger.warning(f"âš ï¸ æ±‡æ€»ç”Ÿæˆå¼‚å¸¸: {e}")
            summary["analysis_quality"] = "limited"
        
        return summary 