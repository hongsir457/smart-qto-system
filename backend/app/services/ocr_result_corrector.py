#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OCRç»“æœæ™ºèƒ½çº æ­£æœåŠ¡
ç”¨äºåœ¨PaddleOCRåˆå¹¶åå¯¹ç»“æœè¿›è¡Œæ™ºèƒ½çº æ­£ã€æ–‡æœ¬å¯¹é½ã€åŒ¹é…ä¼˜åŒ–ç­‰å¤„ç†
åŸºäºå»ºç­‘å·¥ç¨‹è¡Œä¸šå­—å…¸ã€åæ ‡ä½ç½®ç­‰ä¿¡æ¯è¿›è¡ŒGPTçº æ­£
"""

import json
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import re
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class CorrectedOCRResult:
    """çº æ­£åçš„OCRç»“æœ"""
    task_id: str
    original_result_key: str
    corrected_result_key: str
    
    # çº æ­£å‰åå¯¹æ¯”
    original_stats: Dict[str, Any]
    corrected_stats: Dict[str, Any]
    
    # çº æ­£åçš„å†…å®¹
    drawing_basic_info: Dict[str, Any]          # å›¾çº¸åŸºæœ¬ä¿¡æ¯
    component_list: List[Dict[str, Any]]        # æ„ä»¶æ¸…å•åŠåŸºæœ¬å±æ€§
    global_notes: List[Dict[str, Any]]          # å…¨å±€è¯´æ˜å¤‡æ³¨æ–‡æœ¬
    text_regions_corrected: List[Dict[str, Any]] # çº æ­£åçš„æ–‡æœ¬åŒºåŸŸ
    
    # çº æ­£å¤„ç†ä¿¡æ¯
    correction_summary: Dict[str, Any]
    processing_metadata: Dict[str, Any]
    timestamp: float

class OCRResultCorrector:
    """OCRç»“æœæ™ºèƒ½çº æ­£å™¨"""
    
    def __init__(self, ai_analyzer=None, storage_service=None):
        self.ai_analyzer = ai_analyzer
        self.storage_service = storage_service
        
        # å»ºç­‘å·¥ç¨‹è¡Œä¸šè¯å…¸
        self.construction_dictionary = self._load_construction_dictionary()
        
        # å¸¸è§OCRé”™è¯¯æ¨¡å¼
        self.ocr_error_patterns = self._load_ocr_error_patterns()
    
    def _load_construction_dictionary(self) -> Dict[str, List[str]]:
        """åŠ è½½å»ºç­‘å·¥ç¨‹è¡Œä¸šè¯å…¸"""
        return {
            "component_types": [
                "æ¢", "æŸ±", "æ¿", "å¢™", "åŸºç¡€", "æ¥¼æ¢¯", "å±‹é¢", "é—¨", "çª—",
                "KL", "KZ", "KB", "KQ", "JC", "LT", "WM", "M", "C",
                "æ¡†æ¶æ¢", "æ¡†æ¶æŸ±", "æ¡†æ¶æ¿", "å‰ªåŠ›å¢™", "æ¡å½¢åŸºç¡€", "ç‹¬ç«‹åŸºç¡€"
            ],
            "materials": [
                "C20", "C25", "C30", "C35", "C40", "C50",
                "HRB400", "HRB500", "HPB300",
                "MU10", "MU15", "MU20", "M5", "M7.5", "M10"
            ],
            "dimensions": [
                "Ã—", "x", "*", "mm", "m", "é•¿", "å®½", "é«˜", "åš",
                "ç›´å¾„", "åŠå¾„", "æ·±åº¦", "è·¨åº¦"
            ],
            "axis_lines": [
                "è½´", "è½´çº¿", "å®šä½è½´çº¿", "A", "B", "C", "D", "E", "F", "G",
                "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"
            ],
            "drawing_info": [
                "å›¾çº¸ç¼–å·", "å›¾å·", "æ¯”ä¾‹", "è®¾è®¡", "å®¡æ ¸", "é¡¹ç›®", "å·¥ç¨‹",
                "ç»“æ„å›¾", "å»ºç­‘å›¾", "å¹³é¢å›¾", "ç«‹é¢å›¾", "å‰–é¢å›¾", "è¯¦å›¾"
            ],
            "specifications": [
                "è§„æ ¼", "å‹å·", "ç­‰çº§", "å¼ºåº¦", "é…ç­‹", "é’¢ç­‹", "é¢„åŸ‹ä»¶",
                "Ï†", "Î¦", "@", "é—´è·", "ä¿æŠ¤å±‚", "æ­æ¥é•¿åº¦"
            ]
        }
    
    def _load_ocr_error_patterns(self) -> List[Dict[str, str]]:
        """åŠ è½½å¸¸è§OCRé”™è¯¯æ¨¡å¼"""
        return [
            # æ•°å­—è¯¯è¯†åˆ«
            {"pattern": r"0", "corrections": ["O", "o", "Â°"]},
            {"pattern": r"1", "corrections": ["l", "I", "|"]},
            {"pattern": r"5", "corrections": ["S", "s"]},
            {"pattern": r"8", "corrections": ["B"]},
            
            # å­—æ¯è¯¯è¯†åˆ«
            {"pattern": r"O", "corrections": ["0", "Q"]},
            {"pattern": r"I", "corrections": ["1", "l", "|"]},
            {"pattern": r"S", "corrections": ["5"]},
            
            # ä¸­æ–‡è¯¯è¯†åˆ«
            {"pattern": r"æ¢", "corrections": ["ç²±", "æ¨‘"]},
            {"pattern": r"æŸ±", "corrections": ["ä½", "æ³¨"]},
            {"pattern": r"æ¿", "corrections": ["æ‰³", "ç‰ˆ"]},
            {"pattern": r"å¢™", "corrections": ["å¼º", "ç‰†"]},
            
            # ç¬¦å·è¯¯è¯†åˆ«
            {"pattern": r"Ã—", "corrections": ["x", "*", "X"]},
            {"pattern": r"Ï†", "corrections": ["Î¦", "Ñ„", "Â¢"]},
            {"pattern": r"@", "corrections": ["ï¼ ", "a"]},
        ]
    
    async def correct_ocr_result(self, 
                               merged_ocr_key: str, 
                               drawing_id: int, 
                               task_id: str,
                               original_image_info: Dict[str, Any] = None) -> CorrectedOCRResult:
        """
        å¯¹åˆå¹¶çš„OCRç»“æœè¿›è¡Œæ™ºèƒ½çº æ­£
        
        Args:
            merged_ocr_key: åˆå¹¶OCRç»“æœçš„å­˜å‚¨é”®
            drawing_id: å›¾çº¸ID
            task_id: ä»»åŠ¡ID
            original_image_info: åŸå§‹å›¾åƒä¿¡æ¯
            
        Returns:
            çº æ­£åçš„OCRç»“æœ
        """
        logger.info(f"ğŸ”§ å¼€å§‹OCRç»“æœæ™ºèƒ½åˆ†æ: {merged_ocr_key}")
        start_time = time.time()
        
        try:
            # 1. ä¸‹è½½åŸå§‹OCRç»“æœ
            original_result = await self._download_ocr_result(merged_ocr_key)
            if not original_result:
                raise ValueError(f"æ— æ³•ä¸‹è½½OCRç»“æœ: {merged_ocr_key}")
            
            # 2. ä»…è¿›è¡ŒåŸºæœ¬é¢„å¤„ç†ï¼ˆæ¸…ç†æ ¼å¼ï¼Œä¸è¿›è¡Œçº æ­£ï¼‰
            preprocessed_result = self._preprocess_ocr_text_simple(original_result)
            
            # 3. ç›´æ¥ä½¿ç”¨GPTè¿›è¡Œæ™ºèƒ½åˆ†æå’Œç»“æ„åŒ–æå–ï¼ˆè·³è¿‡è¯å…¸çº é”™ï¼‰
            logger.info("ğŸš€ è·³è¿‡è¯å…¸çº é”™ï¼Œç›´æ¥ä½¿ç”¨GPTæ™ºèƒ½åˆ†æ...")
            gpt_analyzed = await self._apply_gpt_analysis_only(
                preprocessed_result, task_id, original_image_info
            )
            
            # 4. è·å–è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼ˆä¼˜å…ˆnatural_language_summaryï¼Œå¦åˆ™é™çº§ä¸ºgpt_response_textå­—æ®µæˆ–ç©ºå­—ç¬¦ä¸²ï¼‰
            natural_language_text = gpt_analyzed.get("natural_language_summary")
            if not natural_language_text:
                # å…¼å®¹æ—§æµç¨‹ï¼Œå°è¯•è·å–gpt_response_text
                natural_language_text = gpt_analyzed.get("gpt_response_text", "")
            if not natural_language_text:
                logger.warning("âš ï¸ æœªè·å–åˆ°è‡ªç„¶è¯­è¨€åˆ†ææ–‡æœ¬ï¼Œä¿å­˜ç©ºå†…å®¹")
                natural_language_text = ""
            
            # 5. ä¿å­˜è‡ªç„¶è¯­è¨€æ–‡æœ¬åˆ° analyzed_result.txt
            analyzed_key = f"ocr_results/{drawing_id}/analyzed_result.txt"
            storage_result = None
            if self.storage_service:
                try:
                    storage_result = await self.storage_service.upload_content_async(
                        content=natural_language_text,
                        key=analyzed_key,
                        content_type="text/plain"
                    )
                    logger.info(f"âœ… åˆ†æç»“æœè‡ªç„¶è¯­è¨€æ–‡æœ¬å·²ä¿å­˜: {analyzed_key}")
                except Exception as save_exc:
                    logger.error(f"âŒ åˆ†æç»“æœè‡ªç„¶è¯­è¨€æ–‡æœ¬ä¿å­˜å¤±è´¥: {save_exc}")
                    storage_result = {"success": False, "error": str(save_exc)}
            else:
                logger.error("âŒ å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜åˆ†æç»“æœ")
                storage_result = {"success": False, "error": "å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–"}
            
            # 6. æ„å»ºè¿”å›ç»“æœï¼ˆå…¶ä½™ç»Ÿè®¡å’Œç»“æ„ä½“ä¿æŒä¸å˜ï¼‰
            corrected_ocr_result = CorrectedOCRResult(
                task_id=task_id,
                original_result_key=merged_ocr_key,
                corrected_result_key=analyzed_key,
                original_stats=self._extract_stats(original_result),
                corrected_stats={},
                drawing_basic_info={},
                component_list=[],
                global_notes=[],
                text_regions_corrected=[],
                correction_summary={},
                processing_metadata={
                    "processing_time": time.time() - start_time,
                    "analysis_method": "gpt_only_natural_language_txt",
                    "ai_model_used": self.ai_analyzer.__class__.__name__ if self.ai_analyzer else None,
                    "storage_info": storage_result
                },
                timestamp=time.time()
            )
            
            logger.info(f"âœ… OCRç»“æœè‡ªç„¶è¯­è¨€åˆ†ææ–‡æœ¬ä¿å­˜å®Œæˆ: è€—æ—¶ {time.time() - start_time:.2f}s")
            
            return corrected_ocr_result
            
        except Exception as e:
            logger.error(f"âŒ OCRç»“æœæ™ºèƒ½åˆ†æå¤±è´¥: {e}")
            raise

    def _preprocess_ocr_text_simple(self, ocr_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç®€å•é¢„å¤„ç†OCRæ–‡æœ¬ï¼ˆä¸è¿›è¡Œçº æ­£ï¼‰"""
        logger.info("ğŸ”§ å¼€å§‹ç®€å•OCRæ–‡æœ¬é¢„å¤„ç†ï¼ˆä¿æŒåŸæ–‡ä¸å˜ï¼‰...")
        
        # å¤åˆ¶åŸå§‹ç»“æœ
        preprocessed = json.loads(json.dumps(ocr_result))
        
        # è·å–æ–‡æœ¬åŒºåŸŸ
        text_regions = preprocessed.get("merged_result", {}).get("all_text_regions", [])
        
        cleaned_regions = []
        for region in text_regions:
            text = region.get("text", "").strip()
            if not text:
                continue
            
            # ä»…åšåŸºæœ¬æ¸…ç†ï¼Œä¸è¿›è¡Œçº æ­£
            cleaned_text = self._clean_text_basic(text)
            if cleaned_text:
                region["text"] = cleaned_text
                region["original_text"] = text
                cleaned_regions.append(region)
        
        # æ›´æ–°æ–‡æœ¬åŒºåŸŸ
        if "merged_result" in preprocessed:
            preprocessed["merged_result"]["all_text_regions"] = cleaned_regions
        
        logger.info(f"ğŸ“ ç®€å•é¢„å¤„ç†å®Œæˆ: {len(text_regions)} -> {len(cleaned_regions)} ä¸ªæ–‡æœ¬åŒºåŸŸï¼ˆä¿æŒåŸæ–‡ï¼‰")
        return preprocessed
    
    def _clean_text_basic(self, text: str) -> str:
        """åŸºæœ¬æ–‡æœ¬æ¸…ç†ï¼ˆä¸è¿›è¡Œä»»ä½•çº æ­£ï¼‰"""
        if not text:
            return ""
        
        # ä»…å»é™¤å¤šä½™ç©ºç™½ï¼Œä¿æŒæ‰€æœ‰åŸå§‹å†…å®¹
        cleaned = re.sub(r'\s+', ' ', text.strip())
        
        # ä¸è¿›è¡Œä»»ä½•çº æ­£ï¼Œä¿æŒOCRè¯†åˆ«çš„åŸå§‹ç»“æœ
        return cleaned

    async def _apply_gpt_analysis_only(self, 
                                     ocr_result: Dict[str, Any], 
                                     task_id: str,
                                     original_image_info: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨GPTè¿›è¡Œæ™ºèƒ½åˆ†æï¼šçº æ­£æ˜æ˜¾é”™è¯¯ã€å…¨å›¾è¿˜åŸã€æå–å›¾çº¸åŸºæœ¬ä¿¡æ¯ã€æ„ä»¶æ¸…å•åŠæ„ä»¶å±æ€§
        æ³¨æ„ï¼šä¸ä½¿ç”¨è¯å…¸çº é”™ï¼Œè€Œæ˜¯è®©GPTåŸºäºä¸Šä¸‹æ–‡è¿›è¡Œæ™ºèƒ½åˆ¤æ–­
        """
        logger.info("ğŸ¤– å¼€å§‹GPTæ™ºèƒ½åˆ†æï¼šçº æ­£æ˜æ˜¾é”™è¯¯ã€æå–å›¾çº¸ä¿¡æ¯å’Œæ„ä»¶æ¸…å•...")
        
        try:
            # ç¡®ä¿è·å–åˆ°æ­£ç¡®çš„OCRæ•°æ®
            if isinstance(ocr_result, dict) and "data" in ocr_result:
                # å¦‚æœocr_resultæ˜¯ä¸‹è½½ç»“æœçš„åŒ…è£…
                original_content_json = ocr_result["data"]
            else:
                # ç›´æ¥ä½¿ç”¨ocr_result
                original_content_json = ocr_result
            
            # æå–OCRæ–‡æœ¬ç”¨äºGPTåˆ†æ
            merged_result = original_content_json.get("merged_ocr_result")
            if not merged_result:
                merged_result = original_content_json.get("merged_result")
            text_regions = []
            if merged_result:
                # ä¼˜å…ˆæ”¯æŒtext_regions
                text_regions = merged_result.get("text_regions", [])
                # å…¼å®¹all_text_regions
                if not text_regions:
                    text_regions = merged_result.get("all_text_regions", [])
                # å…¼å®¹texts
                if not text_regions:
                    text_regions = merged_result.get("texts", [])
                # å…¼å®¹ocr_results
                if not text_regions:
                    text_regions = merged_result.get("ocr_results", [])
                # å…¼å®¹regions
                if not text_regions:
                    text_regions = merged_result.get("regions", [])
                # å…¼å®¹text_results
                if not text_regions:
                    text_regions = merged_result.get("text_results", [])
            
            logger.info(f"ğŸ” OCRç»“æœæ•°æ®ç»“æ„è°ƒè¯•:")
            logger.info(f"   - merged_result keys: {list(merged_result.keys()) if merged_result else 'None'}")
            logger.info(f"   - text_regions count: {len(text_regions) if text_regions else 0}")
            
            if not text_regions:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°OCRæ–‡æœ¬åŒºåŸŸï¼Œè¿”å›ç©ºç»“æœ")
                return {"success": False, "message": "æœªæ‰¾åˆ°OCRæ–‡æœ¬åŒºåŸŸ", "text_regions": []}
            
            # æ„å»ºGPTåˆ†ææç¤ºè¯ï¼ˆæ–‡æœ¬åŒºåŸŸæ’åºã€ç›¸é‚»åˆå¹¶ã€çº¯æ–‡æœ¬ï¼‰
            prompt, ocr_plain_text = self._build_gpt_analysis_prompt(text_regions, original_image_info, return_plain_text=True)

            # è¾“å‡ºå…¨å›¾æ–‡æœ¬æ¦‚è§ˆï¼ˆå‰5è¡Œå’Œå5è¡Œï¼‰
            plain_lines = ocr_plain_text.split('\n')
            preview_lines = plain_lines[:5] + (["..."] if len(plain_lines) > 10 else []) + plain_lines[-5:] if len(plain_lines) > 10 else plain_lines
            logger.info("ğŸ“‹ å…¨å›¾æ–‡æœ¬æ¦‚è§ˆï¼š\n" + '\n'.join(preview_lines))

            # ä¿å­˜æ‹¼æ¥çº¯æ–‡æœ¬åˆ°S3
            if self.storage_service:
                try:
                    drawing_id = original_content_json.get('meta', {}).get('drawing_id', 'unknown')
                    s3_key = f"ocr_results/{drawing_id}/ocr_plain_text_{task_id}.txt"
                    save_result = self.storage_service.upload_content_sync(
                        content=ocr_plain_text,
                        s3_key=s3_key,
                        content_type="text/plain"
                    )
                    if save_result.get("success"):
                        logger.info(f"âœ… æ‹¼æ¥çº¯æ–‡æœ¬å·²ä¿å­˜åˆ°S3: {save_result.get('final_url')}")
                    else:
                        logger.warning(f"âš ï¸ æ‹¼æ¥çº¯æ–‡æœ¬ä¿å­˜åˆ°S3å¤±è´¥: {save_result.get('error')}")
                except Exception as save_exc:
                    logger.warning(f"âš ï¸ æ‹¼æ¥çº¯æ–‡æœ¬ä¿å­˜å¼‚å¸¸: {save_exc}")

            # è°ƒç”¨AIåˆ†æå™¨
            if not self.ai_analyzer:
                logger.error("âŒ AIåˆ†æå™¨æœªåˆå§‹åŒ–")
                return self._create_fallback_result(text_regions)
            
            # ä½¿ç”¨AIåˆ†æå™¨è¿›è¡Œæ™ºèƒ½åˆ†æï¼ˆpromptåªåŒ…å«æ‹¼æ¥çº¯æ–‡æœ¬ï¼Œä¸å«åŸå§‹jsonæˆ–å¤§æ•°ç»„ï¼‰
            analysis_response = await self.ai_analyzer.analyze_text_async(
                prompt=prompt,
                session_id=f"ocr_analysis_{task_id}",
                context_data={"task_type": "ocr_intelligent_analysis", "drawing_id": task_id}
            )
            
            if not analysis_response.get("success"):
                logger.error(f"âŒ GPTåˆ†æå¤±è´¥: {analysis_response.get('error')}")
                return self._create_fallback_result(text_regions)
            
            # è§£æGPTåˆ†æç»“æœ
            gpt_response_text = analysis_response.get("response", "")
            analyzed_result = self._parse_gpt_analysis_response(gpt_response_text, original_content_json)
            
            # è‡ªåŠ¨æå–æœ€ç»ˆè‡ªç„¶è¯­è¨€æ‘˜è¦
            final_summary = analyzed_result.get("natural_language_summary", "")
            
            # ä¿å­˜å¤§æ¨¡å‹äº¤äº’è¿‡ç¨‹åˆ°Sealos
            if self.storage_service:
                try:
                    drawing_id = original_content_json.get('meta', {}).get('drawing_id', 'unknown')
                    interaction_key = f"ocr_results/{drawing_id}/gpt_interaction_{task_id}.json"
                    interaction_data = {
                        "prompt": prompt,
                        "raw_response": gpt_response_text,
                        "final_summary": final_summary,
                        "finish_reason": analysis_response.get("finish_reason", "unknown")
                    }
                    self.storage_service.upload_content_sync(
                        content=json.dumps(interaction_data, ensure_ascii=False, indent=2),
                        s3_key=interaction_key,
                        content_type="application/json"
                    )
                    logger.info(f"âœ… å¤§æ¨¡å‹äº¤äº’è¿‡ç¨‹å·²ä¿å­˜åˆ°S3: {interaction_key}")
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤§æ¨¡å‹äº¤äº’è¿‡ç¨‹ä¿å­˜å¤±è´¥: {e}")
            
            # ç›´æ¥è¿”å›åŒ…å«è‡ªç„¶è¯­è¨€æ‘˜è¦çš„ç»“æœï¼Œä¸å†è°ƒç”¨postprocess
            return analyzed_result
            
        except Exception as e:
            logger.error(f"âŒ GPTæ™ºèƒ½åˆ†æå¼‚å¸¸: {e}", exc_info=True)
            return self._create_fallback_result(text_regions if 'text_regions' in locals() else [])
    
    def _create_fallback_result(self, text_regions: List[Dict]) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨ç»“æœ"""
        return {
            "drawing_basic_info": {},
            "component_list": [],
            "global_notes": [],
            "analysis_summary": {"message": "GPT analysis failed, returning original OCR data."},
            "text_regions_analyzed": text_regions
        }

    def _build_gpt_analysis_prompt(self, 
                                 text_regions: List[dict], 
                                 image_info: Dict[str, Any] = None,
                                 return_plain_text: bool = False) -> str:
        """æ„å»ºGPTåˆ†ææç¤ºè¯ï¼ˆæ–‡æœ¬åŒºåŸŸæ’åºã€ç›¸é‚»åˆå¹¶ã€çº¯æ–‡æœ¬ï¼‰ï¼Œå¯è¿”å›çº¯æ–‡æœ¬å†…å®¹"""
        # 1. æŒ‰yå‡åºã€xå‡åºæ’åº
        sorted_regions = sorted(
            text_regions,
            key=lambda r: (r.get('bbox', r.get('box', [[0,0],[0,0],[0,0],[0,0]]))[0][1],
                           r.get('bbox', r.get('box', [[0,0],[0,0],[0,0],[0,0]]))[0][0])
        )
        # 2. ç›¸é‚»åˆå¹¶ï¼ˆé«˜åº¦æ¥è¿‘ã€xé—´è·å°ï¼‰
        merged_lines = []
        line_buffer = []
        last_y = None
        last_h = None
        last_x2 = None
        y_threshold = 10  # åƒç´ å®¹å·®
        x_gap_threshold = 30  # xé—´è·å®¹å·®
        for region in sorted_regions:
            text = region.get('text', '').strip()
            if not text:
                continue
            box = region.get('bbox', region.get('box', [[0,0],[0,0],[0,0],[0,0]]))
            y = box[0][1]
            x1 = box[0][0]
            x2 = box[2][0]
            h = abs(box[2][1] - box[0][1])
            if last_y is not None and abs(y - last_y) <= y_threshold and last_h is not None and abs(h - last_h) <= y_threshold:
                # åŒä¸€è¡Œï¼Œåˆ¤æ–­xé—´è·
                if last_x2 is not None and (x1 - last_x2) <= x_gap_threshold:
                    line_buffer.append(text)
                    last_x2 = x2
                    continue
                else:
                    merged_lines.append(' '.join(line_buffer))
                    line_buffer = [text]
                    last_x2 = x2
            else:
                if line_buffer:
                    merged_lines.append(' '.join(line_buffer))
                line_buffer = [text]
                last_x2 = x2
            last_y = y
            last_h = h
        if line_buffer:
            merged_lines.append(' '.join(line_buffer))
        # 3. æ‹¼æ¥ä¸ºçº¯æ–‡æœ¬
        ocr_plain_text = '\n'.join(merged_lines)
        prompt = f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å»ºç­‘å·¥ç¨‹é€ ä»·å¸ˆï¼Œç°åœ¨éœ€è¦å¯¹PaddleOCRè¯†åˆ«çš„æ–‡æœ¬ç»“æœè¿›è¡Œæ™ºèƒ½åˆ†æå’Œç»“æ„åŒ–æå–ã€‚

## é‡è¦è¯´æ˜
- æ–‡æœ¬å·²æŒ‰å›¾çº¸é¡ºåºæ’åºï¼Œå¹¶è‡ªåŠ¨åˆå¹¶ç›¸é‚»æ–‡æœ¬æ¡†ä¸ºä¸€å¥
- ä½ çš„ä»»åŠ¡æ˜¯ï¼š**å¯¹OCRè¯†åˆ«æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æ’åºã€å½’çº³ã€åˆæˆï¼Œå¹¶çº æ­£OCRè¿‡ç¨‹ä¸­çš„æ˜æ˜¾è¯†åˆ«é”™è¯¯**ï¼ˆå¦‚æ•°å­—/å­—æ¯æ··æ·†ã€å¸¸è§é”™åˆ«å­—ã€æ ¼å¼æ··ä¹±ç­‰ï¼‰ã€‚
- è¾“å‡ºå†…å®¹åº”ä¸º**ç»“æ„æ¸…æ™°ã€è¯­ä¹‰è¿è´¯çš„é«˜è´¨é‡è‡ªç„¶è¯­è¨€æ‘˜è¦**ï¼Œè€Œä¸æ˜¯OCRåŸæ–‡çš„ç®€å•æ‹¼æ¥ã€‚
- å¯¹äºä¸ç¡®å®šçš„å†…å®¹ï¼Œè¯·åˆç†æ¨æ–­æˆ–åœ¨å¤‡æ³¨ä¸­è¯´æ˜ã€‚
- ä¼˜å…ˆä¿æŒæ•°å­—ã€ææ–™æ ‡å·ã€æ„ä»¶ç¼–å·çš„åŸå§‹å½¢å¼ï¼Œé™¤éæ˜æ˜¾é”™è¯¯ã€‚

## ä»»åŠ¡è¯´æ˜
å¯¹ä»¥ä¸‹OCRè¯†åˆ«çš„æ–‡æœ¬è¿›è¡Œåˆ†æã€çº é”™ã€åˆ†ç±»æ•´ç†ï¼Œæå–å‡ºå»ºç­‘å›¾çº¸çš„å…³é”®ä¿¡æ¯ã€‚

## OCRè¯†åˆ«çš„åŸå§‹æ–‡æœ¬ï¼ˆæ¯è¡Œä¸ºä¸€æ®µï¼Œé¡ºåºä¸å›¾çº¸åˆ†å¸ƒä¸€è‡´ï¼‰
{ocr_plain_text}

## å»ºç­‘å·¥ç¨‹ä¸“ä¸šçŸ¥è¯†å‚è€ƒ
1. **æ„ä»¶ç±»å‹**: æ¢(KL)ã€æŸ±(KZ)ã€æ¿(KB)ã€å¢™(KQ)ã€åŸºç¡€(JC)ã€æ¥¼æ¢¯(LT)ç­‰
2. **ææ–™ç­‰çº§**: C20ã€C25ã€C30ã€C35ã€C40ã€C50ã€HRB400ã€HRB500ç­‰
3. **å°ºå¯¸è¡¨ç¤º**: é•¿Ã—å®½Ã—é«˜ã€Ï†ç›´å¾„ã€@é—´è·ç­‰
4. **è½´çº¿ç¼–å·**: Aã€Bã€Cã€D...è½´ï¼Œ1ã€2ã€3ã€4...è½´
5. **å›¾çº¸ä¿¡æ¯**: å›¾å·ã€æ¯”ä¾‹ã€é¡¹ç›®åç§°ã€è®¾è®¡å•ä½ç­‰

## åˆ†æåŸåˆ™
1. **æ™ºèƒ½æ’åºä¸å½’çº³**ï¼šæ ¹æ®å»ºç­‘å›¾çº¸å¸¸è¯†ï¼Œå¯¹å†…å®¹è¿›è¡Œåˆç†æ’åºã€å½’çº³ã€åˆæˆï¼Œæå‡å¯è¯»æ€§å’Œä¸“ä¸šæ€§ã€‚
2. **çº æ­£æ˜æ˜¾é”™è¯¯**ï¼šå¦‚æ•°å­—/å­—æ¯æ··æ·†ã€å¸¸è§é”™åˆ«å­—ã€æ ¼å¼æ··ä¹±ç­‰ï¼Œéœ€æ™ºèƒ½çº æ­£ã€‚
3. **ä¿æŒåŸæ–‡ä¼˜å…ˆ**: å¯¹äºæ•°å­—ã€ææ–™æ ‡å·(å¦‚C20)ã€å°ºå¯¸(å¦‚33.170)ç­‰ï¼Œé™¤éæ˜æ˜¾é”™è¯¯ï¼Œå¦åˆ™ä¿æŒä¸å˜ã€‚
4. **æ„ä»¶ç¼–å·ä¿æŠ¤**: K-JKZ1ã€GZ1ç­‰æ„ä»¶ç¼–å·é€šå¸¸æ˜¯æ­£ç¡®çš„ï¼Œè¯·ä¿æŒåŸæ ·ã€‚
5. **åˆç†æ¨æµ‹**: åªæœ‰åœ¨æ–‡æœ¬æ˜æ˜¾ä¸ç¬¦åˆå»ºç­‘è§„èŒƒæ—¶æ‰è¿›è¡Œè°ƒæ•´ã€‚
6. **ä¸Šä¸‹æ–‡åˆ†æ**: ç»“åˆå›¾çº¸æ•´ä½“å†…å®¹åˆ¤æ–­æ–‡æœ¬çš„åˆç†æ€§ã€‚

## è¾“å‡ºè¦æ±‚
è¯·æŒ‰ä»¥ä¸‹**è‡ªç„¶è¯­è¨€æ‘˜è¦**æ ¼å¼è¾“å‡ºåˆ†æåçš„ç»“æœï¼Œä¿æŒç®€æ´ä»¥ä¾¿ä½œä¸ºåç»­Visionåˆ†æçš„ä¸Šä¸‹æ–‡ï¼š

**æ ¼å¼è¦æ±‚ï¼š**
1. ç”¨ç®€æ´çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¸è¦è¾“å‡ºJSONæ ¼å¼
2. æ§åˆ¶æ€»å­—æ•°åœ¨300å­—ä»¥å†…
3. é‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ï¼šå›¾çº¸ç±»å‹ã€ä¸»è¦æ„ä»¶ã€ææ–™ç­‰çº§

**è¾“å‡ºæ¨¡æ¿ï¼š**
```
å›¾çº¸ä¿¡æ¯ï¼š[å›¾çº¸ç±»å‹] [é¡¹ç›®åç§°] [å›¾çº¸æ¯”ä¾‹]

æ„ä»¶æ¦‚è§ˆï¼š
- ä¸»è¦æ„ä»¶ç±»å‹ï¼š[æ¢ã€æŸ±ã€æ¿ç­‰]
- æ„ä»¶ç¼–å·ç¤ºä¾‹ï¼š[KL1ã€Z1ã€B1ç­‰ï¼Œåˆ—ä¸¾5-8ä¸ªä»£è¡¨æ€§ç¼–å·]
- ææ–™ç­‰çº§ï¼š[C30ã€HRB400ç­‰]
- è½´çº¿åˆ†å¸ƒï¼š[A-Dè½´ã€1-5è½´ç­‰]

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- ç»“æ„ä½“ç³»ï¼š[æ¡†æ¶ç»“æ„ã€å‰ªåŠ›å¢™ç»“æ„ç­‰]
- æ„ä»¶æ•°é‡ï¼šçº¦[X]ä¸ªä¸»è¦æ„ä»¶
- å›¾çº¸å¤æ‚åº¦ï¼š[ç®€å•/ä¸­ç­‰/å¤æ‚]

æ–½å·¥è¦ç‚¹ï¼š
- [å…³é”®çš„æ–½å·¥è¯´æ˜æˆ–æŠ€æœ¯è¦æ±‚ï¼Œ1-2æ¡]
```

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
å›¾çº¸ä¿¡æ¯ï¼šç»“æ„å¹³é¢å›¾ æŸåŠå…¬æ¥¼å·¥ç¨‹ 1:100

æ„ä»¶æ¦‚è§ˆï¼š
- ä¸»è¦æ„ä»¶ç±»å‹ï¼šæ¡†æ¶æ¢ã€æ¡†æ¶æŸ±ã€ç°æµ‡æ¿
- æ„ä»¶ç¼–å·ç¤ºä¾‹ï¼šKL1ã€KL2ã€KZ1ã€KZ2ã€B1
- ææ–™ç­‰çº§ï¼šC30æ··å‡åœŸã€HRB400é’¢ç­‹
- è½´çº¿åˆ†å¸ƒï¼šA-Fè½´ã€1-6è½´

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- ç»“æ„ä½“ç³»ï¼šé’¢ç­‹æ··å‡åœŸæ¡†æ¶ç»“æ„
- æ„ä»¶æ•°é‡ï¼šçº¦15ä¸ªä¸»è¦æ„ä»¶
- å›¾çº¸å¤æ‚åº¦ï¼šä¸­ç­‰

æ–½å·¥è¦ç‚¹ï¼š
- æ¢æŸ±èŠ‚ç‚¹éœ€åŠ å¼ºé…ç­‹
- æ¿åšåº¦120mmï¼ŒåŒå‘é…ç­‹
```

## æ³¨æ„äº‹é¡¹
1. **å¿…é¡»å¯¹OCRæ–‡æœ¬è¿›è¡Œæ™ºèƒ½æ’åºã€å½’çº³ã€åˆæˆï¼Œå¹¶çº æ­£æ˜æ˜¾è¯†åˆ«é”™è¯¯**ã€‚
2. ä¼˜å…ˆä¿æŒOCRåŸæ–‡ï¼Œç‰¹åˆ«æ˜¯æ•°å­—ã€ææ–™æ ‡å·ã€æ„ä»¶ç¼–å·ã€‚
3. æ ¹æ®å»ºç­‘å·¥ç¨‹å¸¸è¯†åˆ¤æ–­æ–‡æœ¬çš„åˆç†æ€§ã€‚
4. å¯¹äºä¸ç¡®å®šçš„å†…å®¹ï¼Œä¿æŒåŸæ–‡å¹¶åœ¨å¤‡æ³¨ä¸­è¯´æ˜ã€‚
5. ç¡®ä¿è¾“å‡ºçš„è‡ªç„¶è¯­è¨€æ‘˜è¦æ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥ç›´æ¥è§£æã€‚
6. **é¿å…è¿‡åº¦çº æ­£**ï¼Œä¿æŒOCRè¯†åˆ«ç»“æœçš„åŸå§‹æ€§å’Œå‡†ç¡®æ€§ã€‚
"""
        if return_plain_text:
            return prompt, ocr_plain_text
        return prompt
    
    def _parse_gpt_analysis_response(self, 
                                   gpt_response: str, 
                                   original_result: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æGPTåˆ†æå“åº” - æ”¯æŒè‡ªç„¶è¯­è¨€æ ¼å¼ï¼Œè‹¥ä¸ºJSONåˆ™è‡ªåŠ¨è½¬ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦"""
        logger.info("ğŸ“Š è§£æGPTåˆ†æå“åº”...")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„è‡ªç„¶è¯­è¨€æ ¼å¼
            if "å›¾çº¸ä¿¡æ¯ï¼š" in gpt_response and "æ„ä»¶æ¦‚è§ˆï¼š" in gpt_response:
                # è§£æè‡ªç„¶è¯­è¨€æ ¼å¼
                parsed_response = self._parse_natural_language_response(gpt_response)
                parsed_response["text_regions_analyzed"] = original_result.get("merged_result", {}).get("all_text_regions", [])
                parsed_response["natural_language_summary"] = gpt_response.strip()
                logger.info("âœ… è‡ªç„¶è¯­è¨€æ ¼å¼åˆ†æå“åº”è§£ææˆåŠŸ")
                return parsed_response
            # æ£€æŸ¥æ˜¯å¦ä¸ºJSONæ ¼å¼
            if gpt_response.strip().startswith("{") or gpt_response.strip().startswith("["):
                try:
                    json_content = gpt_response.strip()
                    parsed_json = json.loads(json_content)
                    # è‡ªåŠ¨è½¬ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦
                    summary_lines = []
                    if isinstance(parsed_json, dict):
                        # å›¾çº¸ä¿¡æ¯
                        drawing_info = parsed_json.get("drawing_basic_info", {})
                        summary_lines.append(f"å›¾çº¸ä¿¡æ¯ï¼š{drawing_info.get('drawing_type', '')} {drawing_info.get('project_name', '')} {drawing_info.get('scale', '')}")
                        summary_lines.append("")
                        # æ„ä»¶æ¦‚è§ˆ
                        summary_lines.append("æ„ä»¶æ¦‚è§ˆï¼š")
                        comp_types = set()
                        comp_ids = []
                        materials = set()
                        for comp in parsed_json.get("component_list", []):
                            if comp.get("component_type"): comp_types.add(comp["component_type"])
                            if comp.get("component_id"): comp_ids.append(comp["component_id"])
                            if comp.get("material"): materials.add(comp["material"])
                        summary_lines.append(f"- ä¸»è¦æ„ä»¶ç±»å‹ï¼š{ 'ã€'.join(list(comp_types)[:5]) }")
                        summary_lines.append(f"- æ„ä»¶ç¼–å·ç¤ºä¾‹ï¼š{ 'ã€'.join(comp_ids[:8]) }")
                        summary_lines.append(f"- ææ–™ç­‰çº§ï¼š{ 'ã€'.join(list(materials)[:3]) }")
                        summary_lines.append(f"- è½´çº¿åˆ†å¸ƒï¼š{drawing_info.get('axis_lines', '')}")
                        summary_lines.append("")
                        # æŠ€æœ¯ç‰¹ç‚¹
                        summary_lines.append("æŠ€æœ¯ç‰¹ç‚¹ï¼š")
                        summary_lines.append(f"- ç»“æ„ä½“ç³»ï¼š{drawing_info.get('structure_type', '')}")
                        summary_lines.append(f"- æ„ä»¶æ•°é‡ï¼šçº¦{len(parsed_json.get('component_list', []))}ä¸ªä¸»è¦æ„ä»¶")
                        summary_lines.append(f"- å›¾çº¸å¤æ‚åº¦ï¼š{drawing_info.get('complexity', '')}")
                        summary_lines.append("")
                        # æ–½å·¥è¦ç‚¹
                        summary_lines.append("æ–½å·¥è¦ç‚¹ï¼š")
                        notes = [n.get('content','') for n in parsed_json.get('global_notes', []) if n.get('content')]
                        for note in notes[:2]:
                            summary_lines.append(f"- {note}")
                        summary_text = '\n'.join(summary_lines)
                        parsed_json["natural_language_summary"] = summary_text
                        parsed_json["text_regions_analyzed"] = original_result.get("merged_result", {}).get("all_text_regions", [])
                        logger.info("âœ… JSONæ ¼å¼è‡ªåŠ¨è½¬ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦")
                        return parsed_json
                except Exception as e:
                    logger.error(f"âŒ JSONè½¬è‡ªç„¶è¯­è¨€å¤±è´¥: {e}")
            # å…¼å®¹æ—§çš„JSONä»£ç å—æ ¼å¼
            if "```json" in gpt_response:
                json_start = gpt_response.find("```json") + 7
                json_end = gpt_response.find("```", json_start)
                json_content = gpt_response[json_start:json_end].strip()
                try:
                    parsed_json = json.loads(json_content)
                    # è‡ªåŠ¨è½¬ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦
                    summary_lines = []
                    if isinstance(parsed_json, dict):
                        drawing_info = parsed_json.get("drawing_basic_info", {})
                        summary_lines.append(f"å›¾çº¸ä¿¡æ¯ï¼š{drawing_info.get('drawing_type', '')} {drawing_info.get('project_name', '')} {drawing_info.get('scale', '')}")
                        summary_lines.append("")
                        summary_lines.append("æ„ä»¶æ¦‚è§ˆï¼š")
                        comp_types = set()
                        comp_ids = []
                        materials = set()
                        for comp in parsed_json.get("component_list", []):
                            if comp.get("component_type"): comp_types.add(comp["component_type"])
                            if comp.get("component_id"): comp_ids.append(comp["component_id"])
                            if comp.get("material"): materials.add(comp["material"])
                        summary_lines.append(f"- ä¸»è¦æ„ä»¶ç±»å‹ï¼š{ 'ã€'.join(list(comp_types)[:5]) }")
                        summary_lines.append(f"- æ„ä»¶ç¼–å·ç¤ºä¾‹ï¼š{ 'ã€'.join(comp_ids[:8]) }")
                        summary_lines.append(f"- ææ–™ç­‰çº§ï¼š{ 'ã€'.join(list(materials)[:3]) }")
                        summary_lines.append(f"- è½´çº¿åˆ†å¸ƒï¼š{drawing_info.get('axis_lines', '')}")
                        summary_lines.append("")
                        summary_lines.append("æŠ€æœ¯ç‰¹ç‚¹ï¼š")
                        summary_lines.append(f"- ç»“æ„ä½“ç³»ï¼š{drawing_info.get('structure_type', '')}")
                        summary_lines.append(f"- æ„ä»¶æ•°é‡ï¼šçº¦{len(parsed_json.get('component_list', []))}ä¸ªä¸»è¦æ„ä»¶")
                        summary_lines.append(f"- å›¾çº¸å¤æ‚åº¦ï¼š{drawing_info.get('complexity', '')}")
                        summary_lines.append("")
                        summary_lines.append("æ–½å·¥è¦ç‚¹ï¼š")
                        notes = [n.get('content','') for n in parsed_json.get('global_notes', []) if n.get('content')]
                        for note in notes[:2]:
                            summary_lines.append(f"- {note}")
                        summary_text = '\n'.join(summary_lines)
                        parsed_json["natural_language_summary"] = summary_text
                        parsed_json["text_regions_analyzed"] = original_result.get("merged_result", {}).get("all_text_regions", [])
                        logger.info("âœ… JSONä»£ç å—è‡ªåŠ¨è½¬ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦")
                        return parsed_json
                except Exception as e:
                    logger.error(f"âŒ JSONä»£ç å—è½¬è‡ªç„¶è¯­è¨€å¤±è´¥: {e}")
            # å…œåº•ï¼šåŸæ ·è¿”å›
            logger.info("ğŸ”„ è¿”å›åŸå§‹ç»“æœä½œä¸ºå¤‡ç”¨")
            return {
                "drawing_basic_info": {},
                "component_list": [],
                "global_notes": [],
                "analysis_summary": {"error": "æœªèƒ½è§£æä¸ºè‡ªç„¶è¯­è¨€æˆ–JSON"},
                "text_regions_analyzed": original_result.get("merged_result", {}).get("all_text_regions", []),
                "natural_language_summary": gpt_response[:300] + "..." if len(gpt_response) > 300 else gpt_response
            }
            
        except Exception as e:
            logger.error(f"âŒ GPTåˆ†æå“åº”è§£æå¤±è´¥: {e}")
            logger.info("ğŸ”„ è¿”å›åŸå§‹ç»“æœä½œä¸ºå¤‡ç”¨")
            return {
                "drawing_basic_info": {},
                "component_list": [],
                "global_notes": [],
                "analysis_summary": {"error": f"GPTå“åº”è§£æå¤±è´¥: {e}"},
                "text_regions_analyzed": original_result.get("merged_result", {}).get("all_text_regions", []),
                "natural_language_summary": gpt_response[:300] + "..." if len(gpt_response) > 300 else gpt_response
            }

    def _parse_natural_language_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æè‡ªç„¶è¯­è¨€æ ¼å¼çš„GPTå“åº”"""
        try:
            parsed_data = {
                "drawing_basic_info": {},
                "component_list": [],
                "global_notes": [],
                "analysis_summary": {}
            }
            
            lines = response_text.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('```'):
                    continue
                
                # è¯†åˆ«å„ä¸ªéƒ¨åˆ†
                if line.startswith('å›¾çº¸ä¿¡æ¯ï¼š'):
                    info_text = line.replace('å›¾çº¸ä¿¡æ¯ï¼š', '').strip()
                    info_parts = info_text.split(' ')
                    if len(info_parts) >= 1:
                        parsed_data["drawing_basic_info"]["drawing_type"] = info_parts[0]
                    if len(info_parts) >= 2:
                        parsed_data["drawing_basic_info"]["project_name"] = info_parts[1]
                    if len(info_parts) >= 3:
                        parsed_data["drawing_basic_info"]["scale"] = info_parts[2]
                
                elif line.startswith('æ„ä»¶æ¦‚è§ˆï¼š'):
                    current_section = "components"
                
                elif line.startswith('æŠ€æœ¯ç‰¹ç‚¹ï¼š'):
                    current_section = "technical"
                
                elif line.startswith('æ–½å·¥è¦ç‚¹ï¼š'):
                    current_section = "construction"
                
                elif line.startswith('- ') and current_section:
                    content = line[2:].strip()
                    
                    if current_section == "components":
                        if content.startswith('æ„ä»¶ç¼–å·ç¤ºä¾‹ï¼š'):
                            # æå–æ„ä»¶ç¼–å·
                            ids_text = content.replace('æ„ä»¶ç¼–å·ç¤ºä¾‹ï¼š', '')
                            component_ids = [id.strip() for id in ids_text.split('ã€') if id.strip()]
                            for comp_id in component_ids[:5]:  # é™åˆ¶æ•°é‡
                                parsed_data["component_list"].append({
                                    "component_id": comp_id,
                                    "component_type": "æœªæŒ‡å®š",
                                    "material": "",
                                    "dimensions": "",
                                    "location": "",
                                    "specifications": ""
                                })
                        
                        elif content.startswith('ææ–™ç­‰çº§ï¼š'):
                            material_text = content.replace('ææ–™ç­‰çº§ï¼š', '')
                            # å°†ææ–™ä¿¡æ¯æ·»åŠ åˆ°å·²æœ‰æ„ä»¶ä¸­
                            materials = [m.strip() for m in material_text.split('ã€') if m.strip()]
                            if materials and parsed_data["component_list"]:
                                for comp in parsed_data["component_list"]:
                                    comp["material"] = materials[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªææ–™ç­‰çº§
                    
                    elif current_section in ["technical", "construction"]:
                        parsed_data["global_notes"].append({
                            "note_type": "æŠ€æœ¯è¯´æ˜" if current_section == "technical" else "æ–½å·¥è¯´æ˜",
                            "content": content,
                            "importance": "medium"
                        })
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            parsed_data["analysis_summary"] = {
                "total_components": len(parsed_data["component_list"]),
                "total_notes": len(parsed_data["global_notes"]),
                "processing_method": "natural_language_analysis",
                "confidence_level": "high"
            }
            
            return parsed_data
            
        except Exception as e:
            logger.error(f"âŒ è§£æè‡ªç„¶è¯­è¨€å“åº”å¤±è´¥: {e}")
            return {
                "drawing_basic_info": {},
                "component_list": [],
                "global_notes": [],
                "analysis_summary": {"error": f"è‡ªç„¶è¯­è¨€è§£æå¤±è´¥: {e}"}
            }

    async def _download_ocr_result(self, ocr_key: str) -> Optional[Dict[str, Any]]:
        """ä¸‹è½½OCRåˆå¹¶ç»“æœ"""
        logger.info(f"ğŸ”½ æ­£åœ¨ä¸‹è½½OCRç»“æœ: {ocr_key}")
        try:
            if not self.storage_service:
                logger.error("å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–")
                return {"success": False, "error": "å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–"}
            
            download_result = await self.storage_service.download_content_async(ocr_key)
            
            if download_result is None:
                logger.error(f"âŒ ä¸‹è½½OCRç»“æœå¤±è´¥ï¼Œæ–‡ä»¶å¯èƒ½ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {ocr_key}")
                return {"success": False, "error": f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {ocr_key}"}

            # å‡è®¾ä¸‹è½½å†…å®¹å·²ç»æ˜¯è§£æåçš„JSONå­—å…¸
            return {"success": True, "data": download_result}

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½OCRç»“æœå¼‚å¸¸: {e}", exc_info=True)
            return {"success": False, "error": f"ä¸‹è½½æ—¶å‘ç”Ÿå¼‚å¸¸: {e}"}
    
    def _postprocess_analyzed_result(self, analyzed_result: Dict[str, Any]) -> Dict[str, Any]:
        """åå¤„ç†åˆ†æç»“æœ"""
        logger.info("ğŸ”§ å¼€å§‹åå¤„ç†åˆ†æç»“æœ...")
        
        # ç¡®ä¿åŸºæœ¬ç»“æ„å­˜åœ¨
        processed = {
            "drawing_basic_info": analyzed_result.get("drawing_basic_info", {}),
            "component_list": analyzed_result.get("component_list", []),
            "global_notes": analyzed_result.get("global_notes", []),
            "text_regions_analyzed": analyzed_result.get("text_regions_analyzed", []),
            "analysis_summary": analyzed_result.get("analysis_summary", {})
        }
        
        # éªŒè¯å’Œæ¸…ç†æ•°æ®
        if isinstance(processed["component_list"], list):
            # ç¡®ä¿æ¯ä¸ªæ„ä»¶æœ‰åŸºæœ¬å­—æ®µ
            for component in processed["component_list"]:
                if not isinstance(component, dict):
                    continue
                    
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                required_fields = ["component_id", "component_type", "material"]
                for field in required_fields:
                    if field not in component:
                        component[field] = ""
        
        # ä¿ç•™è‡ªç„¶è¯­è¨€æ‘˜è¦å­—æ®µ
        if "natural_language_summary" in analyzed_result:
            processed["natural_language_summary"] = analyzed_result["natural_language_summary"]
            
        logger.info("âœ… åˆ†æç»“æœåå¤„ç†å®Œæˆ")
        return processed
    
    def _calculate_processing_stats(self, 
                                  original_result: Dict[str, Any], 
                                  analyzed_result: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        original_texts = len(original_result.get("merged_result", {}).get("all_text_regions", []))
        analyzed_components = len(analyzed_result.get("component_list", []))
        analyzed_notes = len(analyzed_result.get("global_notes", []))
        
        return {
            "original_text_count": original_texts,
            "extracted_components": analyzed_components,
            "extracted_notes": analyzed_notes,
            "processing_method": "gpt_analysis_only",
            "summary": f"æå–äº†{analyzed_components}ä¸ªæ„ä»¶å’Œ{analyzed_notes}æ¡è¯´æ˜"
        }
    
    def _extract_stats(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ç»“æœç»Ÿè®¡ä¿¡æ¯"""
        text_regions = result.get("merged_result", {}).get("all_text_regions", [])
        return {
            "total_text_regions": len(text_regions),
            "total_characters": sum(len(region.get("text", "")) for region in text_regions),
            "avg_confidence": sum(region.get("confidence", 0.0) for region in text_regions) / len(text_regions) if text_regions else 0.0
        } 