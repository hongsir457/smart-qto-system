#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æä¼˜åŒ–å·¥å…·æ¨¡å—
åŒ…å«ï¼šOCRç¼“å­˜ç®¡ç†å™¨ã€åæ ‡è½¬æ¢æœåŠ¡ã€GPTå“åº”è§£æå™¨ã€åˆ†ææ—¥å¿—è®°å½•å™¨ç­‰
"""

import os
import json
import time
import logging
import re
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

@dataclass
class AnalysisMetadata:
    """ç»Ÿä¸€çš„åˆ†æå…ƒæ•°æ®ç»“æ„"""
    analysis_method: str
    batch_id: int
    slice_count: int
    success: bool
    ocr_cache_used: bool = False
    processing_time: float = 0.0
    confidence_score: float = 0.0
    retry_count: int = 0
    error_message: str = ""

@dataclass
class CoordinatePoint:
    """åæ ‡ç‚¹æ•°æ®ç»“æ„"""
    x: float
    y: float
    slice_id: str = ""
    global_x: float = 0.0
    global_y: float = 0.0

class AnalysisSettings:
    """åˆ†æé…ç½®è®¾ç½®"""
    MAX_SLICES_PER_BATCH = 8
    OCR_CACHE_TTL = 3600  # 1å°æ—¶
    COORDINATE_TRANSFORM_PRECISION = 2
    GPT_RESPONSE_MAX_RETRIES = 3
    OCR_CACHE_PRIORITY = ["global_cache", "shared_slice", "single_slice"]

class OCRCacheManager:
    """ç»Ÿä¸€çš„OCRç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.global_cache: Dict[str, Any] = {}
        self.shared_slice_cache: Dict[str, Any] = {}
        self.single_slice_cache: Dict[str, Any] = {}
        self.cache_timestamps: Dict[str, float] = {}
    
    def get_cached_ocr(self, slice_key: str, cache_type: str = "auto") -> Optional[Any]:
        """è·å–ç¼“å­˜çš„OCRç»“æœï¼ˆget_ocr_resultçš„åˆ«åï¼‰"""
        return self.get_ocr_result(slice_key, cache_type)
    
    def get_ocr_result(self, slice_key: str, cache_type: str = "auto") -> Optional[Any]:
        """æ ¹æ®ä¼˜å…ˆçº§è·å–OCRç»“æœ"""
        if cache_type == "auto":
            # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥
            for priority in AnalysisSettings.OCR_CACHE_PRIORITY:
                result = self._get_from_cache(slice_key, priority)
                if result:
                    AnalysisLogger.log_ocr_reuse(slice_key, len(result), priority)
                    return result
        else:
            return self._get_from_cache(slice_key, cache_type)
        return None
    
    def _get_from_cache(self, slice_key: str, cache_type: str) -> Optional[Any]:
        """ä»æŒ‡å®šç¼“å­˜ç±»å‹è·å–ç»“æœ"""
        cache_map = {
            "global_cache": self.global_cache,
            "shared_slice": self.shared_slice_cache,
            "single_slice": self.single_slice_cache
        }
        
        cache = cache_map.get(cache_type)
        if not cache:
            return None
            
        # æ£€æŸ¥ç¼“å­˜è¿‡æœŸ
        timestamp = self.cache_timestamps.get(f"{cache_type}_{slice_key}", 0)
        if time.time() - timestamp > AnalysisSettings.OCR_CACHE_TTL:
            self._remove_from_cache(slice_key, cache_type)
            return None
            
        return cache.get(slice_key)
    
    def set_ocr_result(self, slice_key: str, result: Any, cache_type: str = "global_cache"):
        """è®¾ç½®OCRç»“æœåˆ°æŒ‡å®šç¼“å­˜"""
        cache_map = {
            "global_cache": self.global_cache,
            "shared_slice": self.shared_slice_cache,
            "single_slice": self.single_slice_cache
        }
        
        cache = cache_map.get(cache_type)
        if cache is not None:
            cache[slice_key] = result
            self.cache_timestamps[f"{cache_type}_{slice_key}"] = time.time()
    
    def _remove_from_cache(self, slice_key: str, cache_type: str):
        """ä»ç¼“å­˜ä¸­ç§»é™¤è¿‡æœŸæ•°æ®"""
        cache_map = {
            "global_cache": self.global_cache,
            "shared_slice": self.shared_slice_cache,
            "single_slice": self.single_slice_cache
        }
        
        cache = cache_map.get(cache_type)
        if cache and slice_key in cache:
            del cache[slice_key]
        
        timestamp_key = f"{cache_type}_{slice_key}"
        if timestamp_key in self.cache_timestamps:
            del self.cache_timestamps[timestamp_key]
    
    def clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = []
        
        for key, timestamp in self.cache_timestamps.items():
            if current_time - timestamp > AnalysisSettings.OCR_CACHE_TTL:
                expired_keys.append(key)
        
        for key in expired_keys:
            cache_type, slice_key = key.split("_", 1)
            self._remove_from_cache(slice_key, cache_type)
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "global_cache_size": len(self.global_cache),
            "shared_slice_cache_size": len(self.shared_slice_cache),
            "single_slice_cache_size": len(self.single_slice_cache),
            "total_timestamps": len(self.cache_timestamps)
        }

class CoordinateTransformService:
    """ç»Ÿä¸€çš„åæ ‡è½¬æ¢æœåŠ¡"""
    
    def __init__(self, slice_coordinate_map: Dict[str, Any], original_image_info: Dict[str, Any]):
        self.slice_map = slice_coordinate_map
        self.original_info = original_image_info
        self.precision = AnalysisSettings.COORDINATE_TRANSFORM_PRECISION
    
    def transform_to_global(self, slice_coord: CoordinatePoint, slice_id: str) -> CoordinatePoint:
        """å°†åˆ‡ç‰‡åæ ‡è½¬æ¢ä¸ºå…¨å›¾åæ ‡"""
        slice_info = self.slice_map.get(slice_id)
        if not slice_info:
            logger.warning(f"æœªæ‰¾åˆ°åˆ‡ç‰‡ {slice_id} çš„åæ ‡æ˜ å°„ä¿¡æ¯")
            return slice_coord
        
        # è®¡ç®—å…¨å›¾åæ ‡
        global_x = slice_coord.x + slice_info.get('x_offset', 0)
        global_y = slice_coord.y + slice_info.get('y_offset', 0)
        
        # è¾¹ç•Œæ£€æŸ¥
        original_width = self.original_info.get('width', 0)
        original_height = self.original_info.get('height', 0)
        
        global_x = max(0, min(global_x, original_width))
        global_y = max(0, min(global_y, original_height))
        
        # ç²¾åº¦æ§åˆ¶
        global_x = round(global_x, self.precision)
        global_y = round(global_y, self.precision)
        
        return CoordinatePoint(
            x=slice_coord.x,
            y=slice_coord.y,
            slice_id=slice_id,
            global_x=global_x,
            global_y=global_y
        )
    
    def transform_bbox_to_global(self, bbox: Dict[str, float], slice_id: str) -> Dict[str, float]:
        """è½¬æ¢è¾¹ç•Œæ¡†åˆ°å…¨å›¾åæ ‡"""
        top_left = CoordinatePoint(x=bbox['x'], y=bbox['y'])
        bottom_right = CoordinatePoint(x=bbox['x'] + bbox['width'], y=bbox['y'] + bbox['height'])
        
        global_top_left = self.transform_to_global(top_left, slice_id)
        global_bottom_right = self.transform_to_global(bottom_right, slice_id)
        
        return {
            'x': global_top_left.global_x,
            'y': global_top_left.global_y,
            'width': global_bottom_right.global_x - global_top_left.global_x,
            'height': global_bottom_right.global_y - global_top_left.global_y
        }
    
    def batch_transform_coordinates(self, coordinates: List[Tuple[CoordinatePoint, str]]) -> List[CoordinatePoint]:
        """æ‰¹é‡è½¬æ¢åæ ‡"""
        results = []
        for coord, slice_id in coordinates:
            transformed = self.transform_to_global(coord, slice_id)
            results.append(transformed)
        return results

class GPTResponseParser:
    """ç»Ÿä¸€çš„GPTå“åº”è§£æå™¨"""
    
    @staticmethod
    def extract_json_from_response(response_text: str) -> Dict[str, Any]:
        """ä»GPTå“åº”ä¸­æå–JSONæ•°æ®"""
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = response_text.strip()
            
            # å¦‚æœå“åº”åŒ…å«```jsonæ ‡è®°ï¼Œæå–å…¶ä¸­çš„JSON
            json_match = re.search(r'```json\s*(.*?)\s*```', cleaned_response, re.DOTALL)
            if json_match:
                cleaned_response = json_match.group(1).strip()
            
            # å¦‚æœå“åº”ä»¥```å¼€å¤´ä½†æ²¡æœ‰jsonæ ‡è®°ï¼Œå»é™¤```
            elif cleaned_response.startswith('```'):
                lines = cleaned_response.split('\n')
                if len(lines) > 1:
                    cleaned_response = '\n'.join(lines[1:-1]) if lines[-1].strip() == '```' else '\n'.join(lines[1:])
            
            # å°è¯•è§£æJSON
            return json.loads(cleaned_response)
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ GPTå“åº”JSONè§£æå¤±è´¥: {e}")
            return GPTResponseParser._create_fallback_response()
    
    @staticmethod
    def _create_fallback_response() -> Dict[str, Any]:
        """åˆ›å»ºé™çº§å“åº”"""
        return {
            "drawing_info": {
                "drawing_title": "æœªè¯†åˆ«",
                "drawing_number": "æœªè¯†åˆ«",
                "scale": "æœªè¯†åˆ«",
                "project_name": "æœªè¯†åˆ«",
                "drawing_type": "ç»“æ„å›¾çº¸"
            },
            "component_ids": [],
            "component_types": [],
            "material_grades": [],
            "axis_lines": [],
            "summary": {
                "total_components": 0,
                "main_structure_type": "æœªçŸ¥",
                "complexity_level": "ä¸­ç­‰"
            }
        }
    
    @staticmethod
    def validate_json_structure(data: Dict[str, Any], required_fields: List[str]) -> bool:
        """éªŒè¯JSONç»“æ„æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ"""
        for field in required_fields:
            if field not in data:
                return False
        return True

class AnalysisLogger:
    """ç»Ÿä¸€çš„åˆ†ææ—¥å¿—è®°å½•å™¨"""
    
    @staticmethod
    def log_step(step_name: str, details: str = "", step_number: int = None, total_steps: int = None, 
                 status: str = "info", task_id: str = ""):
        """è®°å½•åˆ†ææ­¥éª¤ï¼Œæ”¯æŒå¤šç§å‚æ•°æ ¼å¼"""
        # æ™ºèƒ½emojié€‰æ‹©
        emoji_map = {
            "info": "ğŸš€",
            "success": "âœ…", 
            "warning": "âš ï¸",
            "error": "âŒ",
            "ocr_extraction_optimized": "ğŸ”",
            "ocr_extraction_completed": "ğŸ“‹",
            "global_ocr_overview": "ğŸŒ",
            "ocr_error": "ğŸ’¥"
        }
        
        emoji = emoji_map.get(status, "ğŸ“Œ")
        
        # æ„å»ºæ¶ˆæ¯
        message_parts = [emoji, "Step:", step_name]
        
        if step_number and total_steps:
            message_parts.extend([f"({step_number}/{total_steps})"])
        
        if details:
            message_parts.extend(["-", details])
        
        if task_id:
            message_parts.append(f"[Task: {task_id[:8]}...]")
        
        message = " ".join(message_parts)
        
        # æ ¹æ®çŠ¶æ€é€‰æ‹©æ—¥å¿—çº§åˆ«
        if status == "error":
            logger.error(message)
        elif status == "warning":
            logger.warning(message)
        else:
            logger.info(message)
    
    @staticmethod
    def log_ocr_reuse(slice_key: str, count: int, source: str):
        """è®°å½•OCRå¤ç”¨æƒ…å†µ"""
        logger.info(f"â™»ï¸ OCRå¤ç”¨: {slice_key} - {count}é¡¹ (æ¥æº: {source})")
    
    @staticmethod
    def log_batch_processing(batch_id: int, total_batches: int, slice_count: int):
        """è®°å½•æ‰¹æ¬¡å¤„ç†æƒ…å†µ"""
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤„ç†: {batch_id}/{total_batches} - {slice_count}ä¸ªåˆ‡ç‰‡")
    
    @staticmethod
    def log_coordinate_transform(transformed_count: int, total_count: int):
        """è®°å½•åæ ‡è½¬æ¢æƒ…å†µ"""
        logger.info(f"ğŸ“ åæ ‡è½¬æ¢: {transformed_count}/{total_count}ä¸ªåæ ‡ç‚¹")
    
    @staticmethod
    def log_cache_stats(stats: Dict[str, int]):
        """è®°å½•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        logger.info(f"ğŸ’¾ ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    @staticmethod
    def log_analysis_metadata(metadata: AnalysisMetadata):
        """è®°å½•åˆ†æå…ƒæ•°æ®"""
        logger.info(f"ğŸ“Š åˆ†æå…ƒæ•°æ®: {metadata.analysis_method} - æˆåŠŸ:{metadata.success}")
    
    @staticmethod
    def log_performance_metrics(operation_name: str, start_time: float, 
                              item_count: int = 0, success_count: int = 0):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        duration = time.time() - start_time
        if item_count > 0:
            rate = item_count / duration if duration > 0 else 0
            success_rate = success_count / item_count if item_count > 0 else 0
            logger.info(f"âš¡ {operation_name}: {duration:.2f}s, {rate:.1f}é¡¹/ç§’, æˆåŠŸç‡{success_rate:.1%}")
        else:
            logger.info(f"âš¡ {operation_name}: {duration:.2f}s")
    
    @staticmethod
    def log_error_with_context(error_msg: str, context: Dict[str, Any] = None, 
                              exception: Exception = None):
        """è®°å½•å¸¦ä¸Šä¸‹æ–‡çš„é”™è¯¯ä¿¡æ¯"""
        message = f"âŒ é”™è¯¯: {error_msg}"
        if context:
            message += f" | ä¸Šä¸‹æ–‡: {context}"
        if exception:
            message += f" | å¼‚å¸¸: {type(exception).__name__}: {exception}"
        logger.error(message)

class AnalyzerInstanceManager:
    """åˆ†æå™¨å®ä¾‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.instances: Dict[str, Any] = {}
        self.creation_times: Dict[str, float] = {}
        self.usage_counts: Dict[str, int] = {}
    
    def get_analyzer(self, analyzer_class):
        """è·å–åˆ†æå™¨å®ä¾‹ï¼ˆå¤ç”¨æˆ–åˆ›å»ºæ–°å®ä¾‹ï¼‰"""
        class_name = analyzer_class.__name__
        
        if class_name not in self.instances:
            self.instances[class_name] = self._create_new_analyzer(analyzer_class)
            self.creation_times[class_name] = time.time()
            self.usage_counts[class_name] = 0
        
        self.usage_counts[class_name] += 1
        AnalysisLogger.log_step(f"analyzer_reuse", f"{class_name} (ä½¿ç”¨æ¬¡æ•°: {self.usage_counts[class_name]})")
        
        return self.instances[class_name]
    
    def _create_new_analyzer(self, analyzer_class):
        """åˆ›å»ºæ–°çš„åˆ†æå™¨å®ä¾‹"""
        return analyzer_class()
    
    def reset_for_new_batch(self):
        """ä¸ºæ–°æ‰¹æ¬¡é‡ç½®å®ä¾‹ç®¡ç†å™¨"""
        self.instances.clear()
        self.creation_times.clear()
        # ä¿ç•™usage_countsç”¨äºç»Ÿè®¡
    
    def get_instance_stats(self) -> Dict[str, Any]:
        """è·å–å®ä¾‹ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "active_instances": len(self.instances),
            "usage_counts": self.usage_counts.copy(),
            "total_usages": sum(self.usage_counts.values())
        }

# å…¨å±€å®ä¾‹
ocr_cache_manager = OCRCacheManager()
analyzer_instance_manager = AnalyzerInstanceManager() 